{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fa76f98",
   "metadata": {
    "papermill": {
     "duration": 0.039463,
     "end_time": "2024-03-01T13:13:02.305677",
     "exception": false,
     "start_time": "2024-03-01T13:13:02.266214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"color: #7b6b59; font-size: 30px; text-align: center;\">Diverse Approaches to Document Classification</div>\n",
    "\n",
    "# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#7b6b59;overflow:hidden\">Introduction</div>\n",
    "\n",
    "**Text classification** consists in categorizing a text passage into several predefined labels. It is one of the most useful natural language processing (NLP) techniques and typical use cases include email routing, sentiment analysis of customer reviews, spam filtering, toxicity detection, etc. In practice, the circumstances may vastly differ from one use case to another. In particular:\n",
    "\n",
    "1. **Labeled data** may be abundant, scarce, or simply inexistent;\n",
    "1. **The vocabulary** used in the texts and the targeted labels may be **common or very specific to the context of a particular organization**;\n",
    "1. **The organization** implementing the use case may have **limited or extensive NLP expertise**.\n",
    "\n",
    "In this notebook, we’ll present several powerful text classification techniques to fit all these situations.\n",
    "\n",
    "<img width=\"1163\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/d3ab4069-8cf9-45c8-836b-858610432158\">\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">What is Transfer Learning?</span>\n",
    "\n",
    "A neural network is trained on a data. This network gains knowledge from this data, which is compiled as “weights” of the network. These weights can be extracted and then transferred to any other neural network. Instead of training the other neural network from scratch, we “transfer” the learned features.\n",
    "\n",
    "- **Transfer learning** involves using a pre-trained model on a specific task and applying its learned knowledge to a different but related task. The basic ideology of the feature is that the features learned by the pre-trained model on a large dataset can be generalized and useful for other tasks, even if the new task has a different dataset. The process typically involves taking a pre-trained model, removing its last layers, and replacing them with new layers. The initial layers of the pre-trained model are fine-tuned with a small learning rate to preserve the learned representations. They help in capturing the general features. The newly added layers are then trained using the new dataset specific to the target task.\n",
    "\n",
    "- **Fine-tuning** refers to the process of taking a pre-trained model and further training it on a new dataset. Fine-tuning pre-trained AI models is a powerful technique that has revolutionized the field of machine learning. It involves taking a model that has already been trained on a large dataset and then further training, or “fine-tuning,” it on a smaller, specific dataset. This process allows the model to adapt to new tasks with less data than training from scratch. **What is Fine-Tuning?** In the context of machine learning, fine-tuning is a process that takes a pre-trained model (a model trained on a large-scale dataset) and “tunes” this model for a different but related task. Pre-trained models are attractive for many reasons, but primarily because they’ve already learned patterns in their original training set. So, leveraging these learned patterns can lead to improved performance and faster training times on related tasks.\n",
    "\n",
    "To illustrate the difference between not fine-tuned and fine-tuned models, let’s consider an example in the field of natural language processing (NLP). Suppose we have a pre-trained model on a large corpus of English text, and we want to adapt this model to perform sentiment analysis (i.e., determining whether a piece of text is positive, negative, or neutral).\n",
    "\n",
    "- **Not Fine-Tuned:** If we use our pre-trained model without any fine-tuning, it might not perform well on the sentiment analysis task. This is because the original task it was trained on likely didn’t involve any form of sentiment classification, so the model might not have learned the necessary features for this task.\n",
    "\n",
    "- **Fine-Tuned:** If we fine-tune our pre-trained model on a smaller dataset specifically labeled for sentiment analysis, it can learn from the pre-existing knowledge of the English language it gained during pre-training and combine this with the specific examples in our smaller dataset. This can lead to significantly improved performance on the sentiment analysis task.\n",
    "\n",
    "The intuition behind transfer learning for image classification is that if a model is trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. You can then take advantage of these learned feature maps without having to start from scratch by training a large model on a large dataset.\n",
    "\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">What is a Pre-trained Model?</span>\n",
    "\n",
    "Simply put, a pre-trained model is a model created by some one else to solve a similar problem. Instead of building a model from scratch to solve a similar problem, you use the model trained on other problem as a starting point. A pre-trained model is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task. You either use the pretrained model as is or use transfer learning to customize this model to a given task.\n",
    "\n",
    "For example, if you want to build a self learning car. You can spend years to build a decent image recognition algorithm from scratch or you can take inception model (a pre-trained model) from Google which was built on ImageNet data to identify images in those pictures.\n",
    "\n",
    "A pre-trained model may not be 100% accurate in your application, but it saves huge efforts required to re-invent the wheel. Let me show this to you with a recent example.\n",
    "\n",
    "*The only change that I made to the VGG16 existing architecture is changing the softmax layer with 1000 outputs to 16 categories suitable for our problem and re-training the dense layer. This architecture gave me an accuracy of 70% much better than MLP and CNN. Also, the biggest benefit of using the VGG16 pre-trained model was almost negligible time to train the dense layer with greater accuracy. So, I moved forward with this approach of using a pre-trained model and the next step was to fine tune my VGG16 model to suit this problem.*\n",
    "\n",
    "What is our objective when we train a neural network? We wish to identify the correct weights for the network by multiple forward and backward iterations. By using pre-trained models which have been previously trained on large datasets, we can directly use the weights and architecture obtained and apply the learning on our problem statement. This is known as transfer learning. We “transfer the learning” of the pre-trained model to our specific problem statement.\n",
    "\n",
    "You should be very careful while choosing what pre-trained model you should use in your case. If the problem statement we have at hand is very different from the one on which the pre-trained model was trained – the prediction we would get would be very inaccurate. For example, a model previously trained for speech recognition would work horribly if we try to use it to identify objects using it.\n",
    "\n",
    "We make modifications in the pre-existing model by fine-tuning the model. Since we assume that the pre-trained network has been trained quite well, we would not want to modify the weights too soon and too much. While modifying we generally use a learning rate smaller than the one used for initially training the model.\n",
    "\n",
    "**Why use pre-trained models?**\n",
    "\n",
    "Pre-trained models are neural networks that have been trained on large amounts of data, usually for a general NLP task like language modeling, sentiment analysis, or question answering. They can capture complex patterns and features of natural language, and transfer them to other related tasks. Using pre-trained models can help you achieve better results, faster, and with less data than training a model from scratch. However, pre-trained models are not magic bullets. They might not fit your specific problem or domain, and they might have biases or limitations that affect their performance.\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Ways to Transfer Learning</span>\n",
    "\n",
    "- **Two Main Approaches:** There are two main approaches to transfer learning in NLP: \n",
    "    - **feature-based transfer learning** and \n",
    "    - **fine-tuning.**\n",
    "\n",
    "\n",
    "1. **Feature extraction** – We can use a pre-trained model as a feature extraction mechanism. What we can do is that we can remove the output layer( the one which gives the probabilities for being in each of the 1000 classes) and then use the entire network as a fixed feature extractor for the new data set. Use the representations learned by a previous network to extract meaningful features from new samples. You simply add a new classifier, which will be trained from scratch, on top of the pretrained model so that you can repurpose the feature maps learned previously for the dataset. You do not need to (re)train the entire model. The base convolutional network already contains features that are generically useful for classifying pictures. However, the final, classification part of the pretrained model is specific to the original classification task, and subsequently specific to the set of classes on which the model was trained. When working with a small dataset, it is a common practice to take advantage of features learned by a model trained on a larger dataset in the same domain. This is done by instantiating the pre-trained model and adding a fully-connected classifier on top. The pre-trained model is \"frozen\" and only the weights of the classifier get updated during training. In this case, the convolutional base extracted all the features associated with each image and you just trained a classifier that determines the image class given that set of extracted features.\n",
    "\n",
    "1. **Train some layers while freeze others** – Another way to use a pre-trained model is to train is partially. What we can do is we keep the weights of initial layers of the model frozen while we retrain only the higher layers. We can try and test as to how many layers to be frozen and how many to be trained. Unfreeze a few of the top layers of a frozen model base and jointly train both the newly-added classifier layers and the last layers of the base model. This allows us to \"fine-tune\" the higher-order feature representations in the base model in order to make them more relevant for the specific task. To further improve performance, one might want to repurpose the top-level layers of the pre-trained models to the new dataset via fine-tuning. In this case, you tuned your weights such that your model learned high-level features specific to the dataset. This technique is usually recommended when the training dataset is large and very similar to the original dataset that the pre-trained model was trained on. One way to increase performance even further is to train (or \"fine-tune\") the weights of the top layers of the pre-trained model alongside the training of the classifier you added. The training process will force the weights to be tuned from generic feature maps to features associated specifically with the dataset. Also, you should try to fine-tune a small number of top layers rather than the whole MobileNet model. In most convolutional networks, the higher up a layer is, the more specialized it is. The first few layers learn very simple and generic features that generalize to almost all types of images. As you go higher up, the features are increasingly more specific to the dataset on which the model was trained. The goal of fine-tuning is to adapt these specialized features to work with the new dataset, rather than overwrite the generic learning.\n",
    "\n",
    "1. **Use the Architecture of the pre-trained model** – What we can do is that we use architecture of the model while we initialize all the weights randomly and train the model according to our dataset again.\n",
    "\n",
    "The below diagram should help you decide on how to proceed on using the pre trained model in your case.\n",
    "\n",
    "<img width=\"772\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/578731ef-1025-486e-928b-217e9fc9b900\">\n",
    "\n",
    "- **Scenario 1:** Size of the Data set is small while the Data similarity is very high – In this case, since the data similarity is very high, we do not need to retrain the model. All we need to do is to customize and modify the output layers according to our problem statement. We use the pretrained model as a feature extractor. Suppose we decide to use models trained on Imagenet to identify if the new set of images have cats or dogs. Here the images we need to identify would be similar to imagenet, however we just need two categories as my output – cats or dogs. In this case all we do is just modify the dense layers and the final softmax layer to output 2 categories instead of a 1000.\n",
    "\n",
    "- **Scenario 2:** Size of the data is small as well as data similarity is very low – In this case we can freeze the initial (let’s say k) layers of the pretrained model and train just the remaining(n-k) layers again. The top layers would then be customized to the new data set. Since the new data set has low similarity it is significant to retrain and customize the higher layers according to the new dataset.  The small size of the data set is compensated by the fact that the initial layers are kept pretrained(which have been trained on a large dataset previously) and the weights for those layers are frozen.\n",
    "\n",
    "- **Scenario 3:** Size of the data set is large however the Data similarity is very low – In this case, since we have a large dataset, our neural network training would be effective. However, since the data we have is very different as compared to the data used for training our pretrained models. The predictions made using pretrained models would not be effective. Hence, its best to train the neural network from scratch according to your data.\n",
    "\n",
    "- **Scenario 4:** Size of the data is large as well as there is high data similarity – This is the ideal situation. In this case the pretrained model should be most effective. The best way to use the model is to retain the architecture of the model and the initial weights of the model. Then we can retrain this model using the weights as initialized in the pre-trained model.\n",
    "\n",
    "*Let’s now try to use a pretrained model for a simple problem. There are various architectures that have been trained on the imageNet data set. You can go through various architectures here. I have used vgg16 as pretrained model architecture and have tried to identify handwritten digits using it. Let’s see in which of the above scenarios would this problem fall into. We have around 60,000 training images of handwritten digits. This data set is definitely small. So the situation would either fall into scenario 1 or scenario 2. We shall try to solve the problem using both these scenarios.*\n",
    "\n",
    "1. Retrain the output dense layers only – Here we use vgg16 as a feature extractor. We then use these features and send them to dense layers which are trained according to our data set. The output layer is also replaced with our new softmax layer relevant to our problem. The output layer in a vgg16 is a softmax activation with 1000 categories. We remove this layer and replace it with a softmax layer of 10 categories. We just train the weights of these layers and try to identify the digits.\n",
    "\n",
    "2. Freeze the weights of first few layers – Here what we do is we freeze the weights of the first 8 layers of the vgg16 network, while we retrain the subsequent layers. This is because the first few layers capture universal features like curves and edges that are also relevant to our new problem. We want to keep those weights intact and we will get the network to focus on learning dataset-specific features in the subsequent layers.\n",
    "\n",
    "Be sure that the pre-trained model you have selected has been trained on a similar data set as the one that you wish to use it on. There are various architectures people have tried on different types of data sets and I strongly encourage you to go through these architectures and apply them on your own problem statements. \n",
    "\n",
    "## <span style=\"color: #7b6b59;\">Technique 1: Fine-tuning pre-trained models</span>\n",
    "Fine-tuning is often necessary because pre-trained models are trained on general language understanding, and the fine-tuning process adapts the model to a specific task or domain. Fine-tuning involves training the model on task-specific data, where the model is initialized with the pre-trained weights, and the weights are updated during the fine-tuning process.\n",
    "\n",
    "Fine-tuning involves adjusting various hyperparameters, such as learning rate, batch size, and number of training epochs, to optimize the model’s performance on the specific task. Fine-tuning can be done on a single task or on multiple tasks, where the model is trained on multiple tasks sequentially or simultaneously.\n",
    "\n",
    "\n",
    "**Fine-tuning example: Fine-tuning BERT for text classification**\n",
    "\n",
    "In this example, we want to build a sentiment analysis model for movie reviews. Instead of creating a new language model and training it from scratch, we use a pre-trained model called BERT, which has already learned the structure and nuances of the English language from a large text dataset.\n",
    "\n",
    "We add a classification layer on top of the pre-trained BERT model and then fine-tune the entire model on our dataset of movie reviews. Fine-tuning means we continue the training process for a few more epochs with a smaller learning rate. This allows the BERT model to adjust its weights slightly to better understand the specific task of sentiment analysis while preserving the knowledge it has learned from the larger text dataset.\n",
    "\n",
    "This approach leverages the knowledge BERT has already learned from the large text dataset to achieve better performance on our sentiment analysis task with less data and training time.\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Where to Start?</span>\n",
    "\n",
    "\n",
    "Fine-tuning pre-trained models involves several steps:\n",
    "\n",
    "1. **Select a Pre-Trained Model:** Choose a pre-trained model that is well-suited to your specific task. Models like BERT, GPT-3, and RoBERTa have different strengths, so consider the architecture and pre-training data when making your selection. The first step is to choose an appropriate pre-trained model. The choice of model usually depends on your specific task. For instance, if you’re working on an NLP task, models like BERT or GPT-3 could be good choices. The first step is to choose a pre-trained NLP model that suits your use case. There are many models available, such as BERT, GPT-2, RoBERTa, XLNet, and T5, each with different architectures, sizes, and pre-training objectives. You should consider factors such as the type of task (e.g., classification, generation, summarization, etc.), the language and domain of the data, the computational resources and time available, and the expected performance and accuracy. You can find pre-trained models on platforms such as Hugging Face or TensorFlow Hub, or train your own model from scratch or using transfer learning. When selecting a pre-trained model for your portfolio project, there are several factors to consider, such as the task and data, performance and resources, and availability and accessibility. It is important to evaluate how similar your data is to the data that the pre-trained model was trained on, as well as how accurate and robust you want your model to be. Additionally, it is necessary to consider how much computational power and memory you have, as well as how easy it is to find and download the pre-trained model. By comparing different pre-trained models based on these criteria, you can determine which one best suits your needs and constraints.\n",
    "    - **How to load a pre-trained model?** Once you have chosen a pre-trained model, load it into your code. Depending on the model and the framework, there are different ways to do this. One of the most convenient options is to use libraries like Hugging Face Transformers or PyTorch Lightning, which provide easy access to a wide range of pre-trained models and their corresponding tokenizers, weights, and configurations. For example, using Hugging Face Transformers, you can load a pre-trained BERT model for sentiment analysis with just a few lines of code. You can also load custom or fine-tuned models from local files or online repositories, as long as they are compatible with the library. Initialize the pre-trained model with the pre-trained weights and train it on your task-specific dataset. Transfer learning helps the model leverage its prior knowledge, making fine-tuning more efficient.\n",
    "\n",
    "2. **Prepare Your Dataset:** You’ll need a labeled dataset for your specific task. This dataset will be used to fine-tune the pre-trained model. The second step is to prepare the data for fine-tuning. You need to have a labeled dataset that matches your target task and domain. For example, if you want to fine-tune a model for sentiment analysis, you need a dataset of texts with positive or negative labels. You also need to split the data into training, validation, and test sets, and preprocess the data according to the model requirements. For example, you may need to tokenize, truncate, pad, or mask the texts, or add special tokens such as [CLS] or [SEP]. You can use libraries such as transformers or nltk to help you with data preparation. Gather and preprocess your task-specific dataset. This involves tokenization, handling special characters, padding sequences, and encoding labels for supervised tasks.\n",
    "\n",
    "3. **Set the hyperparameters & Fine-Tune the Model:** Next, you’ll need to fine-tune your selected pre-trained model on your dataset. This usually involves setting up your training configuration and then training your model. The third step is to set the hyperparameters for fine-tuning. Hyperparameters are the variables that control the training process, such as the learning rate, the batch size, the number of epochs, the optimizer, the loss function, and the regularization methods. You should tune the hyperparameters to optimize the performance and avoid overfitting or underfitting. You can use methods such as grid search, random search, or Bayesian optimization to find the best hyperparameters for your model and data. You can also use tools such as Optuna or Ray Tune to automate the hyperparameter tuning process.\n",
    "    - **How to fine-tune a pre-trained model?** Loading a pre-trained model is not enough to make it work on your specific task and data. You need to fine-tune it, which means adjusting its parameters to optimize its performance on your target domain. Fine-tuning a pre-trained model involves several steps, such as preparing the data by splitting it into training, validation, and test sets and preprocessing them according to the model's requirements. Additionally, you need to set the hyperparameters such as the learning rate, batch size, number of epochs, optimizer, loss function, and other options that affect the training process and the model's behavior. Furthermore, you must train the model by feeding it data and updating its parameters based on feedback from the loss function and validation metrics. Finally, you must evaluate the model by testing it on unseen data and measuring its performance using appropriate metrics such as accuracy, precision, recall, or F1-score. You may also need to analyze errors, outputs, and attention weights of the model in order to compare it with other models or baselines. Depending on your task, you may need to add custom layers on top of the pre-trained model. For example, for text classification, you can add a fully connected layer with softmax activation for predicting class labels.\n",
    "\n",
    "4. **Evaluate Your Model:** After fine-tuning, it’s important to evaluate your model’s performance on a validation set to see how well it’s doing. The final step is to evaluate the results of fine-tuning. You should use the test set to measure the performance of your fine-tuned model on your target task and domain. You should use appropriate metrics that reflect your objectives and expectations, such as accuracy, precision, recall, F1-score, BLEU, ROUGE, etc. You should also compare the results with the baseline model or other models to assess the improvement and the trade-offs. You can use libraries such as scikit-learn or nltk to calculate the metrics and visualize the results.\n",
    "    - **How to showcase your fine-tuned model?** After fine-tuning your pre-trained model, you might want to showcase it in your portfolio project and demonstrate your NLP skills and knowledge. Writing a blog post or report is one way to do this, as it allows you to explain the methodology and results of your project, as well as highlight any challenges and contributions. Additionally, visualizations, code snippets, and references can be included to support arguments and findings. Creating a demo or an app is another option. Tools like Streamlit, Flask, or Dash can be used to build web-based demos or apps that can be hosted on platforms like Heroku, AWS, or Google Cloud. Lastly, you can share your code and model on platforms like GitHub, Colab, or Kaggle to make them accessible and reproducible for others. Documentation of the code and model should also be included using comments, README files, or notebooks.\n",
    "\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">How to Keep Improving?</span>\n",
    "\n",
    "Large language models have achieved remarkable results in natural language processing (NLP) tasks, but they are not always optimized for specific tasks. Fine-tuning a pre-trained language model on a task or domain-specific dataset can improve its performance on that task or domain respectively. We will explore several fine-tuning techniques that can be applied to large language models to boost their performance on specific tasks or domains.\n",
    "\n",
    "Fine-tuning is more of an art than a science, and there are several ways you can continue to improve your fine-tuned models:\n",
    "\n",
    "1. **Task-specific training data:** The first and most important technique for fine-tuning a large language model is to use task-specific training data. A pre-trained language model has learned a general representation of language that can be applied to many tasks, but it may not perform well on a specific task without additional training. To fine-tune the model for a specific task, we need a dataset that is annotated with task-specific labels. For example, if we want to fine-tune the model for sentiment analysis, we need a dataset of text documents labeled with positive or negative sentiment. Once we have the task-specific training data, we can fine-tune the pre-trained language model on this dataset. During fine-tuning, the model is trained in a supervised way, where the input is the task-specific text and the output is the corresponding label. The fine-tuning process adjusts the pre-trained parameters to better fit the task-specific data. The output of the fine-tuned model is a model that is optimized for the specific task. Hence, a task-specific dataset contains text that is specific to a particular task, such as sentiment analysis or machine translation, and the goal is to fine-tune the pre-trained language model to perform that task better. The model is trained on a large corpus of text that is annotated or labeled for that task, which helps it learn the patterns and relationships between words that are specific to the task.\n",
    "\n",
    "1. **Domain Specific training data:** Machine learning models are built to generalize, meaning they can learn patterns and relationships in the data they are trained on and apply them to new, unseen data. However, if the model is trained on data that is not representative of the target domain, it may not perform well in that domain. A domain-specific dataset contains text that is specific to a particular domain, such as legal or medical texts, and the goal is to fine-tune the pre-trained language model to better understand the language and jargon used in that domain. The model is trained on a large corpus of domain-specific text, which helps it learn domain-specific language patterns and terminology. If a model is trained on a general corpus of text, it may not perform well in a specific domain, such as legal or medical text. To improve the model’s performance in these domains, it is essential to fine-tune the model with domain-specific data. Hence, domain-specific datasets are used to fine-tune language models to better understand specific domains, while task-specific datasets are used to fine-tune language models to perform specific tasks better. Both approaches can be used in combination, depending on the use case.\n",
    "\n",
    "1. **Hyperparameter Tuning:** You can experiment with different hyperparameters during the fine-tuning process such as learning rate, batch size, number of training epochs etc. Experiment with hyperparameters like dropout rates, layer sizes, and optimizer choice to find the best configuration for your task.\n",
    "\n",
    "1. **Data Augmentation:** Techniques such as text augmentation for NLP tasks or image augmentation for computer vision tasks can provide your model with more varied examples and help improve its performance. Augment your training data to improve model generalization. For text data, this might include synonym replacement, paraphrasing, or back-translation.\n",
    "\n",
    "1. **Regularization Techniques:** Methods like dropout or weight decay can help prevent overfitting during the fine-tuning process. Implement techniques like dropout or weight decay to prevent overfitting during fine-tuning. Regularization techniques can help prevent overfitting during fine-tuning. Overfitting occurs when the model memorizes the training data instead of generalizing to new data. Regularization techniques add constraints to the optimization process to encourage the model to learn a more general representation of the data. Two popular regularization techniques are **dropout** and **weight decay**. Dropout randomly drops out some of the neurons in the model during training, which can help prevent co-adaptation of the neurons and encourage the model to learn more robust features. Weight decay adds a penalty to the loss function for large weights, which can help prevent the model from overemphasizing noisy features.\n",
    "\n",
    "1. **Early Stopping:** Implement early stopping by monitoring validation performance to avoid overfitting.\n",
    "\n",
    "1. **Learning Rate:** Experiment with different learning rates during fine-tuning. A common approach is to use a smaller learning rate for the pre-trained layers and a larger learning rate for the custom layers. This helps retain valuable pre-trained knowledge while adapting to the new task. The learning rate is a hyperparameter that controls the step size taken during gradient descent optimization. A high learning rate can cause the model to overshoot the minimum of the loss function, while a low learning rate can cause slow convergence or getting stuck in local minima. During fine-tuning, adjusting the learning rate can improve performance. Starting with a high learning rate and gradually decreasing it over time can help the model converge to better solutions. One popular learning rate scheduling technique is called the “learning rate warmup.” During the warmup phase, the learning rate is gradually increased from a small value to the desired maximum value. This helps the model start learning more quickly and avoid getting stuck in suboptimal solutions.\n",
    "\n",
    "1. **Gradient clipping:** Gradient clipping can prevent the gradients from becoming too large during fine-tuning. Large gradients can cause the optimization process to become unstable and lead to divergent or oscillating behavior. Gradient clipping sets a maximum threshold for the gradient norm, so that the gradients are scaled down if they exceed the threshold.\n",
    "\n",
    "1. **Batch Size:** Adjust the batch size based on the available GPU memory. Smaller batch sizes are often used during fine-tuning to ensure model stability.\n",
    "\n",
    "1. **Number of Epochs:** Fine-tuning does not typically require as many epochs as pre-training. Start with a small number of epochs, monitor performance, and increase if necessary.\n",
    "\n",
    "1. **Loss Function:** Choose a loss function suitable for your task. Common choices include cross-entropy loss for classification tasks and mean squared error for regression tasks.\n",
    "\n",
    "1. **Task-Specific Metrics:** Choose evaluation metrics that are specific to your task. For example, F1 score for classification, BLEU score for translation, or ROUGE score for text summarization.\n",
    "\n",
    "1. **Inference:** After fine-tuning, save the model and develop an inference pipeline for your specific task. This may involve additional pre-processing and post-processing steps.\n",
    "\n",
    "1. **Monitoring and Iteration:** Continually monitor the model's performance on a validation dataset and iterate on the fine-tuning process as needed. You may need to adjust hyperparameters or collect more data to improve results.\n",
    "\n",
    "1. **Multi-task learning:** Multi-task learning is a technique that involves fine-tuning a pre-trained model on multiple related tasks simultaneously. The model is trained on a combination of the task-specific datasets, with each dataset contributing to the loss function in proportion to its importance. Multi-task learning can improve the performance of the model on all tasks by encouraging the model to learn more general features that are useful across tasks.\n",
    "\n",
    "Fine-tuning a large language model can significantly improve its performance on specific tasks. By using task-specific training data, adjusting the learning rate, applying regularization techniques, using gradient clipping, and using multi-task learning, we can fine-tune a pre-trained model to better fit the task-specific data.\n",
    "\n",
    "It’s worth noting that the specific finetuning techniques used will depend on the task and the specific model being used. Different tasks may require different levels of regularization or learning rate schedules, and some models may benefit more from multi-task learning than others.\n",
    "\n",
    "Overall, fine-tuning a pre-trained language model is a powerful technique for improving the performance of the model on specific tasks. It allows us to leverage the power of pre-trained models while tailoring them to specific use cases. By experimenting with different finetuning techniques and hyperparameters, we can further optimize the performance of the model for our specific needs.\n",
    "\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">Technique 2: Feature-based Transfer Learning</span>\n",
    "\n",
    "1. Feature extraction – We can use a pre-trained model as a feature extraction mechanism. What we can do is that we can remove the output layer( the one which gives the probabilities for being in each of the 1000 classes) and then use the entire network as a fixed feature extractor for the new data set.\n",
    "\n",
    "Feature Based Transfer learning is a method in machine learning where a pre-trained model is used as a starting point for training a new model on a different but related task. The idea behind transfer learning is that the knowledge gained from one task can be reused and applied to another task, reducing the amount of training data and computational resources required to achieve good performance. Transfer learning is especially useful in deep learning, where training large models from scratch can be very expensive and time-consuming.\n",
    "\n",
    "Transfer learning example: Feature extraction\n",
    "\n",
    "In this example, we want to build an image classifier to distinguish between images of cats and dogs. Instead of creating a new model and training it from scratch, we use a pre-trained model called VGG16, which has already learned to identify thousands of object categories from a large dataset called ImageNet.\n",
    "\n",
    "We remove the last few layers of VGG16 (which are responsible for actual object classification) and use the remaining layers as a “feature extractor” for our cat and dog images. These layers can transform the input images into a compact representation that captures the essential information. We then use this compact representation as input for training a simpler classifier, like a Support Vector Machine (SVM) or logistic regression.\n",
    "\n",
    "This approach leverages the knowledge VGG16 has already learned from the ImageNet dataset to speed up our training process and achieve better performance with less data.\n",
    "\n",
    "How might an Enterprise use a pre-trained LLM Model in conjunction with feature based transfer learning ?\n",
    "\n",
    "- **Image and video recognition:** Enterprises can use pre-trained models, such as VGG16, ResNet50, or MobileNet, to extract features from images and videos. They can then fine-tune the pre-trained models on their specific tasks, such as detecting defects in manufacturing products or identifying security threats in surveillance videos.\n",
    "\n",
    "- **Natural language processing:** Enterprises can use pre-trained models, such as BERT, GPT-2, or RoBERTa, to extract features from text data. They can then fine-tune the pre-trained models on their specific tasks, such as sentiment analysis, question-answering, or document classification.\n",
    "\n",
    "- **Recommendation systems:** Enterprises can use pre-trained models, such as deep autoencoders or matrix factorization, to learn latent representations of user preferences and item features. They can then fine-tune the pre-trained models on their specific recommendation tasks, such as product recommendations or personalized content recommendations.\n",
    "\n",
    "- **Speech recognition:** Enterprises can use pre-trained models, such as DeepSpeech or Kaldi, to extract features from audio data. They can then fine-tune the pre-trained models on their specific speech recognition tasks, such as voice assistants or call center transcriptions.\n",
    "\n",
    "- **Anomaly detection:** Enterprises can use pre-trained models, such as autoencoders or GANs, to learn the normal patterns of their data. They can then fine-tune the pre-trained models on their specific anomaly detection tasks, such as fraud detection or predictive maintenance.\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">Technique 3: Training from Scratch</span>\n",
    "\n",
    "Use the Architecture of the pre-trained model – What we can do is that we use architecture of the model while we initialize all the weights randomly and train the model according to our dataset again.\n",
    "\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">Considerations</span>\n",
    "\n",
    "- **Transfer Learning vs Fine-Tuning:** While these terms are often used interchangeably, they have subtle differences. Transfer learning generally refers to using the pre-trained model as a fixed feature extractor, while fine-tuning involves updating the weights of the pre-trained model during training. Fine-tuning involves taking the pre-trained model and training it on a new task or domain with additional data. Fine-tuning is a form of transfer learning, but they are not exactly the same thing. Transfer learning is a more general concept, while fine-tuning is a specific technique used within transfer learning. Fine-tuning involves taking a pre-trained model and further training it on a specific task or domain.\n",
    "\n",
    "- **When to Fine-Tune:**  Not all tasks or datasets benefit from fine-tuning. Sometimes, using a pre-trained model as a fixed feature extractor might be enough.\n",
    "\n",
    "- **Fine-Tuning vs Training from Scratch:** It’s important to understand when it’s beneficial to fine-tune a pre-trained model versus training a model from scratch.\n",
    "\n",
    "- **Challenges in Fine-Tuning:** Fine-tuning is not without its challenges. Issues such as catastrophic forgetting (where the model forgets its previously learned knowledge) and domain shift (where the distribution of the new task data is different from the pre-training data) are important considerations.\n",
    "\n",
    "\n",
    "\n",
    "In conclusion, fine-tuning pre-trained AI models is a powerful technique that enables the tailoring of versatile models to meet specific needs across various domains. It empowers AI practitioners to harness the vast knowledge encapsulated in these models and adapt it for custom applications. By following the steps outlined above and embracing ongoing refinement, you can unlock the true potential of fine-tuned AI models, making them invaluable tools for addressing a wide range of challenges and opportunities in the world of artificial intelligence. Whether you’re building chatbots, image classifiers, or tackling other complex tasks, fine-tuning is the key to achieving exceptional results while navigating ethical, legal, and practical considerations in this rapidly evolving field\n",
    "\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "- Pre-trained models can save time and resources by allowing developers to start with a model that has already learned useful features from data. This can be especially useful when working with small datasets or when trying to solve complex problems.\n",
    "\n",
    "- Pre-trained models can provide a good starting point for further fine-tuning and customization. This can help developers achieve better results more quickly than if they were starting from scratch.\n",
    "\n",
    "- Pre-trained models can provide a level of performance that would be difficult to achieve with a model trained from scratch. This is because pre-trained models have been trained on large amounts of data and have learned to recognize many different patterns and relationships.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "- Pre-trained models may not be suitable for all tasks or domains. For example, a pre-trained model trained on text data may not perform well on image data.\n",
    "- Pre-trained models may require additional fine-tuning and customization to achieve the desired level of performance. This can require additional time and resources.\n",
    "- Pre-trained models may not always be available for the specific task or domain that a developer is working on. In this case, the developer may need to train their own model from scratch.\n",
    "\n",
    "To make the most of a fine-tuned large language model, an enterprise should consider the following steps:\n",
    "\n",
    "1. Identify the specific NLP tasks that would benefit from the LLM’s capabilities.\n",
    "1. Collect, clean, and label the necessary data for fine-tuning the LLM for the specific tasks.\n",
    "1. Fine-tune the pre-trained LLM using the collected data and evaluate its performance.\n",
    "1. Deploy the fine-tuned model in production environments, such as chatbots, document management systems, or analytics platforms, using tools like NVIDIA’s Triton Inference Server.\n",
    "\n",
    "Continuously monitor the model’s performance and update it with new data to ensure its accuracy and relevance to the tasks at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8336b397",
   "metadata": {
    "papermill": {
     "duration": 0.038468,
     "end_time": "2024-03-01T13:13:02.383909",
     "exception": false,
     "start_time": "2024-03-01T13:13:02.345441",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#7b6b59;overflow:hidden\">Import Python Libraries</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94593da6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:13:02.463081Z",
     "iopub.status.busy": "2024-03-01T13:13:02.462740Z",
     "iopub.status.idle": "2024-03-01T13:13:20.642523Z",
     "shell.execute_reply": "2024-03-01T13:13:20.641746Z"
    },
    "papermill": {
     "duration": 18.222459,
     "end_time": "2024-03-01T13:13:20.644909",
     "exception": false,
     "start_time": "2024-03-01T13:13:02.422450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "#from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "# Import necessary classes and constants from the logging module\n",
    "from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "from typing import Tuple\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ebc2e4",
   "metadata": {
    "papermill": {
     "duration": 0.038717,
     "end_time": "2024-03-01T13:13:20.767348",
     "exception": false,
     "start_time": "2024-03-01T13:13:20.728631",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#7b6b59;overflow:hidden\">Load the Dataset</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be6216cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:13:20.846715Z",
     "iopub.status.busy": "2024-03-01T13:13:20.845650Z",
     "iopub.status.idle": "2024-03-01T13:13:20.965674Z",
     "shell.execute_reply": "2024-03-01T13:13:20.964869Z"
    },
    "papermill": {
     "duration": 0.16213,
     "end_time": "2024-03-01T13:13:20.968027",
     "exception": false,
     "start_time": "2024-03-01T13:13:20.805897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_prompts = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/train_prompts.csv\")\n",
    "train_essays = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\")\n",
    "test_essays = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7254ac80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:13:21.048980Z",
     "iopub.status.busy": "2024-03-01T13:13:21.048320Z",
     "iopub.status.idle": "2024-03-01T13:13:21.064950Z",
     "shell.execute_reply": "2024-03-01T13:13:21.064082Z"
    },
    "papermill": {
     "duration": 0.058293,
     "end_time": "2024-03-01T13:13:21.066872",
     "exception": false,
     "start_time": "2024-03-01T13:13:21.008579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>instructions</th>\n",
       "      <th>source_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Car-free cities</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "      <td># In German Suburb, Life Goes On Without Cars ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Does the electoral college work?</td>\n",
       "      <td>Write a letter to your state senator in which ...</td>\n",
       "      <td># What Is the Electoral College? by the Office...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prompt_id                       prompt_name  \\\n",
       "0          0                   Car-free cities   \n",
       "1          1  Does the electoral college work?   \n",
       "\n",
       "                                        instructions  \\\n",
       "0  Write an explanatory essay to inform fellow ci...   \n",
       "1  Write a letter to your state senator in which ...   \n",
       "\n",
       "                                         source_text  \n",
       "0  # In German Suburb, Life Goes On Without Cars ...  \n",
       "1  # What Is the Electoral College? by the Office...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_prompts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee878eac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:13:21.147189Z",
     "iopub.status.busy": "2024-03-01T13:13:21.146545Z",
     "iopub.status.idle": "2024-03-01T13:13:21.156045Z",
     "shell.execute_reply": "2024-03-01T13:13:21.155220Z"
    },
    "papermill": {
     "duration": 0.052076,
     "end_time": "2024-03-01T13:13:21.158096",
     "exception": false,
     "start_time": "2024-03-01T13:13:21.106020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0059830c</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars. Cars have been around since they became ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005db917</td>\n",
       "      <td>0</td>\n",
       "      <td>Transportation is a large necessity in most co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>008f63e3</td>\n",
       "      <td>0</td>\n",
       "      <td>\"America's love affair with it's vehicles seem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00940276</td>\n",
       "      <td>0</td>\n",
       "      <td>How often do you ride in a car? Do you drive a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00c39458</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars are a wonderful thing. They are perhaps o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  prompt_id                                               text  \\\n",
       "0  0059830c          0  Cars. Cars have been around since they became ...   \n",
       "1  005db917          0  Transportation is a large necessity in most co...   \n",
       "2  008f63e3          0  \"America's love affair with it's vehicles seem...   \n",
       "3  00940276          0  How often do you ride in a car? Do you drive a...   \n",
       "4  00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n",
       "\n",
       "   generated  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_essays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d203bc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:13:21.240520Z",
     "iopub.status.busy": "2024-03-01T13:13:21.240084Z",
     "iopub.status.idle": "2024-03-01T13:13:21.245884Z",
     "shell.execute_reply": "2024-03-01T13:13:21.245015Z"
    },
    "papermill": {
     "duration": 0.048991,
     "end_time": "2024-03-01T13:13:21.248000",
     "exception": false,
     "start_time": "2024-03-01T13:13:21.199009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1378, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_essays.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9cfa1c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:13:21.328414Z",
     "iopub.status.busy": "2024-03-01T13:13:21.328094Z",
     "iopub.status.idle": "2024-03-01T13:13:21.333622Z",
     "shell.execute_reply": "2024-03-01T13:13:21.332771Z"
    },
    "papermill": {
     "duration": 0.047558,
     "end_time": "2024-03-01T13:13:21.335542",
     "exception": false,
     "start_time": "2024-03-01T13:13:21.287984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_essays.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "343adf54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:13:21.415974Z",
     "iopub.status.busy": "2024-03-01T13:13:21.415306Z",
     "iopub.status.idle": "2024-03-01T13:13:23.479545Z",
     "shell.execute_reply": "2024-03-01T13:13:23.478481Z"
    },
    "papermill": {
     "duration": 2.106994,
     "end_time": "2024-03-01T13:13:23.481917",
     "exception": false,
     "start_time": "2024-03-01T13:13:21.374923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\")\n",
    "train_df = df[df.prompt_name != \"Car-free cities\"].reset_index(drop=True)\n",
    "valid_df = df[df.prompt_name == \"Car-free cities\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97496e7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:13:23.565116Z",
     "iopub.status.busy": "2024-03-01T13:13:23.564785Z",
     "iopub.status.idle": "2024-03-01T13:13:23.570491Z",
     "shell.execute_reply": "2024-03-01T13:13:23.569684Z"
    },
    "papermill": {
     "duration": 0.048148,
     "end_time": "2024-03-01T13:13:23.572303",
     "exception": false,
     "start_time": "2024-03-01T13:13:23.524155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40151, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14d57bd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:13:23.652364Z",
     "iopub.status.busy": "2024-03-01T13:13:23.652030Z",
     "iopub.status.idle": "2024-03-01T13:13:23.662600Z",
     "shell.execute_reply": "2024-03-01T13:13:23.661757Z"
    },
    "papermill": {
     "duration": 0.05284,
     "end_time": "2024-03-01T13:13:23.664550",
     "exception": false,
     "start_time": "2024-03-01T13:13:23.611710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>source</th>\n",
       "      <th>RDizzl3_seven</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Phones\\n\\nModern humans today are always on th...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This essay will explain if drivers should or s...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Driving while the use of cellular devices\\n\\nT...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phones &amp; Driving\\n\\nDrivers should not be able...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cell Phone Operation While Driving\\n\\nThe abil...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  Phones\\n\\nModern humans today are always on th...      0   \n",
       "1  This essay will explain if drivers should or s...      0   \n",
       "2  Driving while the use of cellular devices\\n\\nT...      0   \n",
       "3  Phones & Driving\\n\\nDrivers should not be able...      0   \n",
       "4  Cell Phone Operation While Driving\\n\\nThe abil...      0   \n",
       "\n",
       "          prompt_name           source  RDizzl3_seven  \n",
       "0  Phones and driving  persuade_corpus          False  \n",
       "1  Phones and driving  persuade_corpus          False  \n",
       "2  Phones and driving  persuade_corpus          False  \n",
       "3  Phones and driving  persuade_corpus          False  \n",
       "4  Phones and driving  persuade_corpus          False  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d242b110",
   "metadata": {
    "papermill": {
     "duration": 0.039291,
     "end_time": "2024-03-01T13:13:23.743702",
     "exception": false,
     "start_time": "2024-03-01T13:13:23.704411",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#7b6b59;overflow:hidden\">Exploratory Data Analysis</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f9ba93",
   "metadata": {
    "papermill": {
     "duration": 0.039311,
     "end_time": "2024-03-01T13:13:23.822391",
     "exception": false,
     "start_time": "2024-03-01T13:13:23.783080",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color: #7b6b59;\">Labels Distribution in Essay Data</span>\n",
    "\n",
    "- `generated`: Whether the essay was written by a student (0) or generated by an LLM (1). This field is the target and is not present in test_essays.csv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "363270ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:13:23.903426Z",
     "iopub.status.busy": "2024-03-01T13:13:23.903076Z",
     "iopub.status.idle": "2024-03-01T13:13:23.917189Z",
     "shell.execute_reply": "2024-03-01T13:13:23.916335Z"
    },
    "papermill": {
     "duration": 0.057136,
     "end_time": "2024-03-01T13:13:23.919133",
     "exception": false,
     "start_time": "2024-03-01T13:13:23.861997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generated\n",
       "0    1375\n",
       "1       3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_essays['generated'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81fabb60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:13:24.004283Z",
     "iopub.status.busy": "2024-03-01T13:13:24.003944Z",
     "iopub.status.idle": "2024-03-01T13:13:24.012051Z",
     "shell.execute_reply": "2024-03-01T13:13:24.011205Z"
    },
    "papermill": {
     "duration": 0.055229,
     "end_time": "2024-03-01T13:13:24.014131",
     "exception": false,
     "start_time": "2024-03-01T13:13:23.958902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generated\n",
       "0    0.997823\n",
       "1    0.002177\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_essays['generated'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df94d444",
   "metadata": {
    "papermill": {
     "duration": 0.039922,
     "end_time": "2024-03-01T13:13:24.095890",
     "exception": false,
     "start_time": "2024-03-01T13:13:24.055968",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding:20px;color:white;margin:0;font-size:24px;text-align:left;display:fill;border-radius:5px;background-color:#7b6b59;overflow:hidden\">Standard Approaches: Vectorization and Classic Machine Learning (ML) Model</div>\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">Introduction</span>\n",
    "\n",
    "A simple approach for text classification is to convert text passages in vectors and then use standard ML algorithms such as logistic regression or tree-based models. The key question then becomes: How do you transform a text passage in a vector?\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Option 1: TF-IDF, Sparse Vectorization and Classic Machine Learning (ML) Model</span>\n",
    "\n",
    "TF-IDF (or **term frequency — inverse document frequency**) is one way to achieve this vectorization. It returns a vector with one dimension for each word in a given vocabulary. Each component of this vector reflects the frequency of the corresponding word in the input text compared to the entire collection of texts.\n",
    "\n",
    "**TF-IDF has several drawbacks. It does not consider the order of the words in the text and it ignores the semantic similarity between words.** It also does not distinguish between the various meanings of a polysemous word (e.g., “sound” as in “a loud sound,” “they sound correct,” or “a sound proposal”).\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Option 2: Embeddings obtained from a pre-trained deep learning model, Dense Vectorization and Classic ML Model</span>\n",
    "\n",
    "A more effective approach, in particular if the training dataset is relatively small, is to use the vector representations (or **sentence embeddings**) obtained from a pre-trained deep learning model such as BERT.\n",
    "\n",
    "<img width=\"921\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/901505fd-908a-4810-8fb5-66022a0cbe76\">\n",
    "\n",
    "***Sparse vectorization with TF-IDF (left), dense vectorization with sentence embeddings (right)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98356b9",
   "metadata": {
    "papermill": {
     "duration": 0.039947,
     "end_time": "2024-03-01T13:13:24.175520",
     "exception": false,
     "start_time": "2024-03-01T13:13:24.135573",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#7b6b59;overflow:hidden\">Approach 1: Sparse Vectorization and Classic Machine Learning (ML) Model</div>\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">Overview of vectorization options</span>\n",
    "\n",
    "**Vectors & Word Embeddings: TF-IDF vs Word2Vec vs Bag-of-words vs BERT:**\n",
    "\n",
    "As discussed above, TF-IDF can be used to vectorize text into a format more agreeable for ML & NLP techniques. However while it is a popular NLP algorithm it is not the only one out there.\n",
    "\n",
    "1. **Bag of Words:** Bag of Words (BoW) simply counts the frequency of words in a document. Thus the vector for a document has the frequency of each word in the corpus for that document.  The key difference between bag of words and TF-IDF is that the former does not incorporate any sort of inverse document frequency (IDF)  and is only a frequency count (TF).\n",
    "\n",
    "1. **Word2Vec:**  Word2Vec is an algorithm that uses shallow 2-layer, not deep, neural networks to ingest a corpus and produce sets of vectors. Some key differences between TF-IDF and word2vec is that TF-IDF is a statistical measure that we can apply to terms in a document and then use that to form a vector whereas word2vec will produce a vector for a term and then more work may need to be done to convert that set of vectors into a singular vector or other format. Additionally TF-IDF does not take into consideration the context of the words in the corpus whereas word2vec does.\n",
    "\n",
    "1. **BERT - Bidirectional Encoder Representations from Transformers:** BERT is an ML/NLP technique developed by Google that uses a transformer based ML model to  convert phrases, words, etc into vectors. Key differences between TF-IDF and BERT are as follows: TF-IDF does not take into account the semantic meaning or context of the words whereas BERT does. Also BERT uses deep neural networks as part of its architecture, meaning that it can be much more computationally expensive than TF-IDF which has no such requirements. \n",
    "\n",
    "**Feature Engineering with Bag-of-Words or TF-IDF:**\n",
    "\n",
    "Instead of using deep learning methods, you might utilize statistical methods for text representation like Bag-of-Words or TF-IDF, combined with machine learning algorithms.\n",
    "\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">TF-IDF</span>\n",
    "\n",
    "Most machine learning algorithms are fulfilled with mathematical things such as statistics, algebra, calculus and etc. They expect the data to be numerical such as a 2-dimensional array with rows as instances and columns as features. The problem with natural language is that the data is in the form of raw text, so that the text needs to be transformed into a vector. **The process of transforming text into a vector is commonly referred to as text vectorization.** It’s a fundamental process in natural language processing because none of the machine learning algorithms understand a text, not even computers. Text vectorization algorithm namely TF-IDF vectorizer, which is a very popular approach for traditional machine learning algorithms can help in transforming text into vectors. In order to process natural language, the text must be represented as a numerical feature. **The process of transforming text into a numerical feature is called text vectorization.** TF-IDF is one of the most popular text vectorizers, the calculation is very simple and easy to understand. It gives the rare term high weight and gives the common term low weight. TF-IDF stands for term frequency-inverse document frequency and it is a measure, used in the fields of information retrieval (IR) and machine learning, that can quantify the importance or relevance of string representations (words, phrases, lemmas, etc)  in a document amongst a collection of documents (also known as a corpus).\n",
    "\n",
    "**Term frequency-inverse document frequency** is a text vectorizer that transforms the text into a usable vector. It combines 2 concepts, Term Frequency (TF) and Document Frequency (DF). TF-IDF can be broken down into two parts **TF (term frequency)** and **IDF (inverse document frequency)**.\n",
    "\n",
    "\n",
    "- **The term frequency** is the number of occurrences of a specific term in a document. Term frequency indicates how important a specific term in a document. Term frequency represents every text from the data as a matrix whose rows are the number of documents and columns are the number of distinct terms throughout all documents. Term frequency works by looking at the frequency of a particular term you are concerned with relative to the document. There are multiple measures, or ways, of defining frequency:\n",
    "\n",
    "    - Number of times the word appears in a document (raw count).\n",
    "    - Term frequency adjusted for the length of the document (raw count of occurences divided by number of words in the document).\n",
    "    - Logarithmically scaled frequency (e.g. log(1 + raw count)).\n",
    "    - Boolean frequency (e.g. 1 if the term occurs, or 0 if the term does not occur, in the document).\n",
    "\n",
    "- **Document frequency** is the number of documents containing a specific term. Document frequency indicates how common the term is.\n",
    "\n",
    "- **Inverse document frequency (IDF)** is the weight of a term, it aims to reduce the weight of a term if the term’s occurrences are scattered throughout all the documents. IDF can be calculated as follow:\n",
    "\n",
    "    <img width=\"781\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/e321a50a-138a-438b-9ee4-9320d21a8aed\">\n",
    "    \n",
    "    Where idfᵢ is the IDF score for term i, dfᵢ is the number of documents containing term i, and n is the total number of documents. The higher the DF of a term, the lower the IDF for the term. When the number of DF is equal to n which means that the term appears in all documents, the IDF will be zero, since log(1) is zero, when in doubt just put this term in the stopword list because it doesn't provide much information. **What is IDF (inverse document frequency)?** Inverse document frequency looks at how common (or uncommon) a word is amongst the corpus. IDF is calculated as follows where t is the term (word) we are looking to measure the commonness of and N is the number of documents (d) in the corpus (D).. The denominator is simply the number of documents in which the term, t, appears in. The reason we need IDF is to help correct for words like “of”, “as”, “the”, etc. since they appear frequently in an English corpus. Thus by taking inverse document frequency, we can minimize the weighting of frequent terms while making infrequent terms have a higher impact. Finally IDFs can also be pulled from either a background corpus, which corrects for sampling bias, or the dataset being used in the experiment at hand.\n",
    "\n",
    "    <img width=\"706\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/6012bdc9-5847-4234-9e8c-987adfd2828e\">\n",
    "    \n",
    "    Note: It can be possible for a term to not appear in the corpus at all, which can result in a divide-by-zero error. One way to handle this is to take the existing count and add 1. Thus making the denominator (1 + count). An example of how the  popular library scikit-learn handles this can be seen below.\n",
    "    \n",
    "    <img width=\"739\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/68d7d8f0-84a7-4b3c-a811-4695eae291d9\">\n",
    "\n",
    "- The **TF-IDF score** as the name suggests is just a multiplication of the term frequency matrix with its IDF, it can be calculated as follow:\n",
    "    \n",
    "    <img width=\"692\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/3d3217ff-fccb-4012-a962-b70c8d40c379\">\n",
    "    \n",
    "    Where wᵢⱼ is TF-IDF score for term i in document j, tfᵢⱼ is term frequency for term i in document j, and idfᵢ is IDF score for term i. To summarize the key intuition motivating TF-IDF is the importance of a term is inversely related to its frequency across documents.TF gives us information on how often a term appears in a document and IDF gives us information about the relative rarity of a term in the collection of documents. By multiplying these values together we can get our final TF-IDF value.\n",
    "    \n",
    "    <img width=\"710\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/6ca35452-1cca-4f6b-b695-253cd13fe27a\">\n",
    "\n",
    "\n",
    "**The higher the TF-IDF score the more important or relevant the term is; as a term gets less relevant, its TF-IDF score will approach 0.**\n",
    "\n",
    "\n",
    "**Example:** Suppose we have 3 texts and we need to vectorize these texts using TF-IDF.\n",
    "\n",
    "<img width=\"614\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/a4e32193-47f9-4291-bfd4-5a1f6046f481\">\n",
    "\n",
    "1. **Step 1:** Create a term frequency matrix where rows are documents and columns are distinct terms throughout all documents. Count word occurrences in every text.\n",
    "    \n",
    "    <img width=\"830\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/77e48f3a-13cf-4c51-bd45-32010ff239d7\">\n",
    "\n",
    "1. **Step 2:** Compute inverse document frequency (IDF) using the previously explained formula. The term i and processing has 0 IDF score, as previously mentioned we can drop these terms, but for the sake of simplicity, we keep these terms here.\n",
    "    \n",
    "    <img width=\"839\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/0bc6eec5-5317-4694-9ebb-c84f9c1b9d88\">\n",
    "    \n",
    "1. **Step 3:** Multiply TF matrix with IDF respectively. That's it 😃! the text is now ready to feed into a machine learning algorithm.\n",
    "\n",
    "     <img width=\"833\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/fd090353-4915-4f01-a88c-ee3e381e6382\">\n",
    "\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Pros of using TF-IDF</span>\n",
    "\n",
    "The biggest advantages of TF-IDF come from how simple and easy to use it is. It is simple to calculate, it is computationally cheap, and it is a simple starting point for similarity calculations (via TF-IDF vectorization + cosine similarity).\n",
    "\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Limitations, Cons of using TF-IDF</span>\n",
    "\n",
    "1. It is only useful as a lexical level feature.\n",
    "\n",
    "1. Synonymities are neglected.\n",
    "\n",
    "1. It doesn't capture semantic. Something to be aware of is that TF-IDF cannot help carry semantic meaning. It considers the importance of the words due to how it weighs them, but it cannot necessarily derive the contexts of the words and understand importance that way.\n",
    "\n",
    "1. The highest TF-IDF score may not make sense with the topic of the document, since IDF gives high weight if the DF of a term is low.\n",
    "\n",
    "1. It neglects the sequence of the terms. Also as mentioned above, like BoW, TF-IDF ignores word order and thus compound nouns like “Queen of England” will not be considered as a “single unit”. This also extends to situations like negation with “not pay the bill” vs “pay the bill”, where the order makes a big difference. In both cases using NER tools and underscores, “queen_of_england” or “not_pay” are ways to handle treating the phrase as a single unit. No concept of word order: TF-IDF treats all words as equally important, regardless of their order or position in the document. This can be problematic for certain applications, such as sentiment analysis, where word order can be crucial for determining the sentiment of a document.\n",
    "\n",
    "1. Another disadvantage is that it can suffer from memory-inefficiency since TF-IDF can suffer from the curse of dimensionality. Recall that the length of TF-IDF vectors is equal to the size of the vocabulary. In some classification contexts this may not be an issue but in other contexts like clustering this can be unwieldy as the number of documents increases. Thus looking into some of the above named alternatives (BERT, Word2Vec) may be necessary. **Vocabulary size:** The vocabulary size can become very large when working with large datasets, which can lead to high-dimensional feature spaces and difficulty in interpreting the results.\n",
    "\n",
    "1. Assumes independence: TF-IDF assumes that the terms in a document are independent of each other. However, this is often not the case in natural language, where words are often related to each other in complex ways.\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Where to use TF-IDF</span>\n",
    "\n",
    "As we can see, TF-IDF can be a very handy metric for determining how important a term is in a document. But how is TF-IDF used? There are three main applications for TF-IDF. These are in machine learning, information retrieval, and text summarization/keyword extraction.\n",
    "\n",
    "\n",
    "1. **Using TF-IDF in machine learning & natural language processing:** Machine learning algorithms often use numerical data, so when dealing with textual data or any natural language processing (NLP) task, a sub-field of ML/AI dealing with text, that data first needs to be converted to a vector of numerical data by a process known as vectorization. TF-IDF vectorization involves calculating the TF-IDF score for every word in your corpus relative to that document and then putting that information into a vector (see images above). Thus each document in your corpus would have its own vector, and the vector would have a TF-IDF score for every single word in the entire collection of documents. ***Once you have these vectors you can apply them to various use cases such as seeing if two documents are similar by comparing their TF-IDF vector using cosine similarity.***\n",
    "\n",
    "1. **Using TF-IDF in information retrieval:** TF-IDF also has use cases in the field of information retrieval, with one common example being search engines. Since TF-IDF can tell you about the relevant importance of a term based upon a document, a search engine can use TF-IDF to help rank search results based on relevance, with results which are more relevant to the user having higher TF-IDF scores.\n",
    "\n",
    "1. **Using TF-IDF in text summarization & keyword extraction:** Since TF-IDF weights words based on relevance, one can use this technique to determine that the words with the highest relevance are the most important. This can be used to help summarize articles more efficiently or to simply determine keywords (or even tags) for a document. Measures relevance: TF-IDF measures the importance of a term in a document, based on the frequency of the term in the document and the inverse document frequency (IDF) of the term across the entire corpus. This helps to identify which terms are most relevant to a particular document.\n",
    "\n",
    "1. **Interpretable:** The scores generated by TF-IDF are easy to interpret and understand, as they represent the importance of a term in a document relative to its importance across the entire corpus.\n",
    "\n",
    "1. Works well with different languages: TF-IDF can be used with different languages and character encodings, making it a versatile technique for processing multilingual text data.\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Conclusion</span>\n",
    "\n",
    "TF-IDF (Term Frequency - Inverse Document Frequency) is a handy algorithm that uses the frequency of words to determine how relevant those words are to a given document. It’s a relatively simple but intuitive approach to weighting words, allowing it to act as a great jumping off point for a variety of tasks. This includes building search engines, summarizing documents, or other tasks in the information retrieval and machine learning domains.\n",
    "\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">How to implement TF-IDF with scikit-learn</span>\n",
    "\n",
    "1. Thanks to the `TfidfVectorizer` class, implementing TF-IDF with `scikit-learn` is a fairly straightforward process. The first step is importing `TfidfVectorizer` and creating a list of documents to analyze and convert into TF-IDF features.\n",
    "\n",
    "1. Next, create an instance of the `TfidfVectorizer` class with the desired customization options, such as tokenization patterns, stopword removal or IDF smoothing parameters.\n",
    "\n",
    "1. Then, to fit and transform the corpus, call the `fit_transform()` method on the vectorizer instance and pass in the corpus. This computes term frequencies and inverse document frequencies while transforming the text data into a matrix of TF-IDF features.\n",
    "\n",
    "1. Finally, call `get_feature_names()` to inspect feature names and their corresponding TF-IDF values, then convert the variable to an array using toarray():\n",
    "\n",
    "By following these steps, you can implement TF-IDF with scikit-learn and transform your raw text data into valuable numerical representations for further analysis or feeding into machine learning models.\n",
    "\n",
    "When using the `TfidfVectorizer` from `scikit-learn`, **you do not necessarily need to tokenize the text yourself before passing it to the vectorizer**. TfidfVectorizer has built-in capabilities to tokenize and preprocess the text. Here's how it works by default:\n",
    "\n",
    "1. **Default Tokenization in TfidfVectorizer:**\n",
    "\n",
    "    - **Tokenization:** By default, TfidfVectorizer tokenizes the text by extracting word tokens and ignores punctuation and whitespace. This is typically done using a regular expression that defines what constitutes a token (word). The default pattern is `r\"(?u)\\b\\w\\w+\\b\"`, which captures sequences of alphanumeric characters (words) that are at least two characters long. This pattern is specified in the token_pattern argument. So while spaces between words usually signify where one word ends and another begins (and thus often correspond to word boundaries), the regex isn't splitting text directly on spaces. Instead, it's looking for those alphanumeric sequences that are bounded by non-word characters or the edges of the string, which more robustly constitutes what we think of as whole, standalone words. This method is more reliable because:\n",
    "        - **It ignores punctuation:** For example, in \"end-of-sentence.\", the period is not part of the last word, and the pattern correctly excludes it from the token \"sentence\".\n",
    "        - **It handles complex word separations:** Not all words are neatly separated by spaces, especially in languages with different scripts or in cases with punctuation like hyphens, apostrophes, etc. The pattern correctly identifies words in many of these cases.\n",
    "        In summary, while spaces are a significant part of how the pattern determines where words begin and end, the actual process involves identifying sequences of word characters that are delineated by word boundaries, which provides a more nuanced and effective approach to word tokenization in varied text environments.\n",
    "        \n",
    "    - **Preprocessing:** It converts all characters to lowercase (unless you set lowercase=False) and performs normalization, such as accent stripping, if specified.\n",
    "\n",
    "1. **Customization Options:**\n",
    "    1. **Custom Tokenizer:** You can provide a custom tokenizer function to the `tokenizer` parameter. This function takes a string as input and returns a list of tokens. If you have specific tokenization needs (e.g., handling special cases, working with a non-standard text format), you might implement and use your custom tokenizer.\n",
    "\n",
    "    1. **Custom Preprocessor:** Similarly, you can provide a custom preprocessing function to the `preprocessor` parameter. This function also takes a string as input and returns the processed string. It's applied to the text before tokenization.\n",
    "\n",
    "\n",
    "***Should You Tokenize Beforehand?***\n",
    "\n",
    "- **Usually Unnecessary:** For standard text processing needs, the default behavior of TfidfVectorizer is often sufficient. It is designed to handle typical cases of text vectorization, including tokenization and case normalization.\n",
    "\n",
    "- **Custom Needs:** If your text data requires specialized handling, such as dealing with a particular language's nuances, handling mixed text types, or integrating with an existing text processing pipeline, you might perform tokenization (and other text preprocessing) before vectorization. In such cases, you could use the tokenizer and preprocessor parameters to integrate your custom functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8885634f",
   "metadata": {
    "papermill": {
     "duration": 0.039828,
     "end_time": "2024-03-01T13:13:24.255587",
     "exception": false,
     "start_time": "2024-03-01T13:13:24.215759",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. **`ngram_range=(1, 3)`:** This parameter defines the range of n-gram sizes to include in the token counts. (1, 3) means that it will consider unigrams (single words), bigrams (two consecutive words), and trigrams (three consecutive words) as individual features for vectorization. Essentially, it's looking at the individual words, pairs of consecutive words, and triplets of consecutive words when creating the vectors.\n",
    "\n",
    "1. **`sublinear_tf=True`:** This parameter applies sublinear tf scaling, i.e., it replaces term frequency (tf) with 1 + log(tf). The idea is to reduce the sensitivity of the vectorizer to terms that occur very frequently and therefore might skew the results disproportionately. It's a way to temper the effect of terms that appear very often and might dominate the feature set. By transforming the frequency to the logarithmic scale, increases in term frequency have a gradually smaller effect on the computation of TF-IDF.\n",
    "\n",
    "1. **`lowercase=False`:** This indicates that the text will not be automatically converted to lowercase before tokenizing. By default, TfidfVectorizer converts all characters to lowercase to ensure that the same words in different cases are counted as the same token.\n",
    "\n",
    "1. **`analyzer='word'`:** This parameter sets the unit of features to words. Other options might include 'char' or 'char_wb' for character n-grams. 'word' means it will consider tokens of words as the feature base.\n",
    "\n",
    "1. **`tokenizer=dummy`:** This specifies a custom tokenizer function. Typically, TfidfVectorizer tokenizes the string by extracting words of at least two letters. By setting tokenizer to 'dummy', you are replacing the default tokenizer with your own custom function named dummy. This function will be used to split the text into tokens.\n",
    "\n",
    "1. **`token_pattern=None`:** Normally, this parameter defines the regex pattern that the tokenizer uses to find tokens in the text string. By setting it to None, and providing a custom tokenizer, you're effectively ignoring the default regex pattern and relying entirely on the custom tokenizer you've provided.\n",
    "\n",
    "1. **`preprocessor=dummy`:** Similar to the tokenizer, this specifies a custom pre-processing function. The default preprocessor in TfidfVectorizer takes care of removing accents and performing other cleaning steps. By setting it to 'dummy', you are specifying that your own custom function named dummy should be used for preprocessing the text.\n",
    "\n",
    "1. **`strip_accents='unicode'`:** This is used to remove accents during the preprocessing step. 'unicode' is a method that works on any characters that have a direct Unicode equivalent. It's an effective way to standardize text by removing accents and diacritical marks that might lead to variations in how words are processed.\n",
    "\n",
    "\n",
    "\n",
    "1. **Fitting the Vectorizer:** Initially, when we fit TfidfVectorizer to our documents (e.g., using vectorizer.fit(texts)), it learns the vocabulary of the corpus, meaning it identifies all unique terms used across all documents, considering the constraints and specifications we've given it (like token patterns, n-grams, etc.).\n",
    "\n",
    "1. **Building the Vocabulary Dictionary:** After fitting, the vectorizer has a complete list of terms used in the documents. It then creates a mapping of these terms to specific indices. This mapping is stored in `vectorizer.vocabulary_`.\n",
    "    \n",
    "    - Keys: Each unique term or token found in the corpus.\n",
    "    - Values: A unique integer index corresponding to each term. This index is used when creating the sparse matrix representation of the documents where each term's TF-IDF score will be placed.\n",
    "    \n",
    "    ```python \n",
    "{\n",
    "    'galaxy': 123,\n",
    "    'black hole': 15,\n",
    "    'star cluster': 678,\n",
    "    'nebula': 321,\n",
    "    ...\n",
    "}\n",
    "```\n",
    "    In this hypothetical vocabulary:\n",
    "\n",
    "    - The term 'galaxy' is found at column index 123 in the TF-IDF matrix.\n",
    "    - The term 'black hole' is found at column index 15, and so on.\n",
    "\n",
    "1. When you **transform** your documents into their TF-IDF representation using the fitted vectorizer (via vectorizer.transform(texts)), each document is represented as a sparse vector with the length of the total vocabulary, where most values are zero except for the indices corresponding to the terms present in the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30c2563e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:13:24.336430Z",
     "iopub.status.busy": "2024-03-01T13:13:24.336073Z",
     "iopub.status.idle": "2024-03-01T13:14:50.969070Z",
     "shell.execute_reply": "2024-03-01T13:14:50.968190Z"
    },
    "papermill": {
     "duration": 86.676402,
     "end_time": "2024-03-01T13:14:50.971526",
     "exception": false,
     "start_time": "2024-03-01T13:13:24.295124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3),sublinear_tf=True)\n",
    "X = vectorizer.fit_transform(train_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "326af5b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:14:51.057607Z",
     "iopub.status.busy": "2024-03-01T13:14:51.056778Z",
     "iopub.status.idle": "2024-03-01T13:15:00.276806Z",
     "shell.execute_reply": "2024-03-01T13:15:00.275806Z"
    },
    "papermill": {
     "duration": 9.26605,
     "end_time": "2024-03-01T13:15:00.279661",
     "exception": false,
     "start_time": "2024-03-01T13:14:51.013611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '00 00' '00 00 and' ... '完全禁止使用手机应该是合法和道路安全的唯一选择'\n",
      " '完全禁止使用手机应该是合法和道路安全的唯一选择 保护所有道路使用者的安全'\n",
      " '完全禁止使用手机应该是合法和道路安全的唯一选择 保护所有道路使用者的安全 司机必须在驾驶时将全部注意力都集中在道路上']\n"
     ]
    }
   ],
   "source": [
    "# Inspect feature names and TF-IDF values \n",
    "print(vectorizer.get_feature_names_out()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1da20787",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:00.373307Z",
     "iopub.status.busy": "2024-03-01T13:15:00.372949Z",
     "iopub.status.idle": "2024-03-01T13:15:09.764270Z",
     "shell.execute_reply": "2024-03-01T13:15:09.763344Z"
    },
    "papermill": {
     "duration": 9.435624,
     "end_time": "2024-03-01T13:15:09.766668",
     "exception": false,
     "start_time": "2024-03-01T13:15:00.331044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>be in contact</th>\n",
       "      <td>0.067319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way how</th>\n",
       "      <td>0.059980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>always on their</th>\n",
       "      <td>0.059508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are always on</th>\n",
       "      <td>0.055284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in contact</th>\n",
       "      <td>0.051028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>further without</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>further with you</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>further with this</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>further with their</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>完全禁止使用手机应该是合法和道路安全的唯一选择 保护所有道路使用者的安全 司机必须在驾驶时将全部注意力都集中在道路上</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6207945 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       tfidf\n",
       "be in contact                                       0.067319\n",
       "way how                                             0.059980\n",
       "always on their                                     0.059508\n",
       "are always on                                       0.055284\n",
       "in contact                                          0.051028\n",
       "...                                                      ...\n",
       "further without                                     0.000000\n",
       "further with you                                    0.000000\n",
       "further with this                                   0.000000\n",
       "further with their                                  0.000000\n",
       "完全禁止使用手机应该是合法和道路安全的唯一选择 保护所有道路使用者的安全 司机必须在驾驶时将全...  0.000000\n",
       "\n",
       "[6207945 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the first vector out (for the first document) \n",
    "first_vector_tfidfvectorizer=X[0] \n",
    "\n",
    "# place tf-idf values in a pandas data frame \n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=vectorizer.get_feature_names_out(), columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0d82d4",
   "metadata": {
    "papermill": {
     "duration": 0.042681,
     "end_time": "2024-03-01T13:15:09.854564",
     "exception": false,
     "start_time": "2024-03-01T13:15:09.811883",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The `vocab = vectorizer.vocabulary_` line returns a Python dictionary from the fitted `TfidfVectorizer` object. The dictionary's keys are the terms (or tokens) found in the document corpus, and the values are the column indices of these terms in the resulting TF-IDF matrix.\n",
    "The term 'galaxy' is found at column index 123 in the TF-IDF matrix.\n",
    "The term 'black hole' is found at column index 15, and so on.  This vocabulary is crucial because it maintains a consistent mapping of terms to indices, ensuring that when you transform new documents into vectors, the terms align correctly with the learned model's features. It's essential for both understanding the feature space of your model and for preparing new text inputs for predictions or further analysis with the trained vectorizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60e63240",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:09.942089Z",
     "iopub.status.busy": "2024-03-01T13:15:09.941750Z",
     "iopub.status.idle": "2024-03-01T13:15:09.945903Z",
     "shell.execute_reply": "2024-03-01T13:15:09.944957Z"
    },
    "papermill": {
     "duration": 0.051148,
     "end_time": "2024-03-01T13:15:09.948045",
     "exception": false,
     "start_time": "2024-03-01T13:15:09.896897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Getting vocab\n",
    "vocab = vectorizer.vocabulary_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c699154",
   "metadata": {
    "papermill": {
     "duration": 0.043741,
     "end_time": "2024-03-01T13:15:10.036620",
     "exception": false,
     "start_time": "2024-03-01T13:15:09.992879",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding:20px;color:white;margin:0;font-size:24px;text-align:left;display:fill;border-radius:5px;background-color:#7b6b59;overflow:hidden\">Approach 2: Extracting embeddings from pre-trained models and Classic Machine Learning (ML) Model - Transfer Learning without Fine-Tuning</div>\n",
    "\n",
    "\n",
    "\n",
    "Instead of fine-tuning a pre-trained model, you could use it as a feature extractor. For instance, you can pass your documents through a pre-trained model (like BERT) to get embeddings and then train a simpler machine learning model (like Logistic Regression) on those features.\n",
    "\n",
    "**Extracting embeddings from pre-trained BERT| Huggingface Transformers**\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">Overview</span>\n",
    "\n",
    "\n",
    "The need for standardization in training models and using the language model, Hugging Face, was found.NLP is democratized by Hugging Face, where the constructed API allows easy access to pre-trained models, datasets, and tokens. This Hugging Face's transformers library generates embeddings, and we use the pre-trained BERT model to extract the embeddings.\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">How to use embeddings for feature extraction?</span>\n",
    "\n",
    "Now, let’s talk about how you can use BERT with your text: The BERT Model learns complex understandings of the English language, which can help you extract different aspects of text for various tasks. If you have a set of sentences with labels, you can train a regular classifier using the information produced by the BERT Model as input (the text). To obtain the features of a particular text using this model in TensorFlow see the code below.\n",
    "\n",
    "**How to use embeddings to extract information from text column?**\n",
    "\n",
    "We are going to take advantage of the incredible hugging face 🤗 framework to extract information from this feature.\n",
    "\n",
    "1. **Step 1:** First, we need to import the model and the tokenizer: There are different models that we can try, and you check them here: https://huggingface.co/models?pipeline_tag=feature-extraction It is important to use the model’s tokenizer so that it receives the data in a proper format and they are also useful since they already clean up the data for you. Each tokenizer will have different ways of dealing with the data, therefore it is important to read about them.\n",
    "\n",
    "1. **Step 2:** Second, we extract the hidden state associated to the token CLS which represents an entire sequence of text and rather than dealing with a 768 array for each token in a string, we just need to deal with one (the 768 dimension varies from model to model).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb50f8ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:10.123477Z",
     "iopub.status.busy": "2024-03-01T13:15:10.123109Z",
     "iopub.status.idle": "2024-03-01T13:15:13.058806Z",
     "shell.execute_reply": "2024-03-01T13:15:13.057736Z"
    },
    "papermill": {
     "duration": 2.983005,
     "end_time": "2024-03-01T13:15:13.061935",
     "exception": false,
     "start_time": "2024-03-01T13:15:10.078930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1bad7ce67ab43a6a5690eb1c2399a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a91f978931a48599f957a4541d00a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca17a09e44294c1aaccbecfc827374d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f839c26d6f40ebba832dafb9e68a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3c7867a3bd4b9abd3446337f58e8f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: We need to import the model and the tokenizer\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "#model = TFBertModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)\n",
    "\n",
    "\n",
    "#custom_text = \"You are welcome to utilize any text of your choice.\"\n",
    "#encoded_input = tokenizer(custom_text, return_tensors='tf')\n",
    "#output_embeddings = model(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2b9979d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:13.151633Z",
     "iopub.status.busy": "2024-03-01T13:15:13.151255Z",
     "iopub.status.idle": "2024-03-01T13:15:13.155384Z",
     "shell.execute_reply": "2024-03-01T13:15:13.154504Z"
    },
    "papermill": {
     "duration": 0.050353,
     "end_time": "2024-03-01T13:15:13.157343",
     "exception": false,
     "start_time": "2024-03-01T13:15:13.106990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2: we extract the hidden state associated to the token CLS\n",
    "#train_df[\"embeddings\"] = train_df[\"text\"].apply(lambda x: model(**tokenizer(x, return_tensors=\"pt\", truncation=True)).last_hidden_state[:,0,:].detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a563e890",
   "metadata": {
    "papermill": {
     "duration": 0.045411,
     "end_time": "2024-03-01T13:15:13.247888",
     "exception": false,
     "start_time": "2024-03-01T13:15:13.202477",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This piece of code is a straightforward example of how to use the BERT tokenizer and model from the Hugging Face `transformers `library for encoding text into embeddings. Here's a breakdown of what each part does:\n",
    "\n",
    "1. **Importing the Necessary Classes:**\n",
    "    - **BertTokenizer:** A tokenizer class for BERT. It handles the conversion from text to tokens that BERT understands.\n",
    "    - **TFBertModel:** The BERT model class compatible with TensorFlow.\n",
    "\n",
    "1. **Using the BERT Tokenizer and Model:**\n",
    "    \n",
    "    1. **Load Pre-trained Models:**\n",
    "        - `tokenizer = BertTokenizer.from_pretrained('bert-base-cased')`: Loads the BERT tokenizer for the 'bert-base-cased' version. This tokenizer is responsible for breaking the text down into tokens that BERT can understand.\n",
    "        - `model = TFBertModel.from_pretrained(\"bert-base-cased\")`: Loads the pre-trained BERT model. This model will generate embeddings for the input text.\n",
    "\n",
    "1. **Prepare Custom Text:**\n",
    "\n",
    "    - `custom_text = \"You are welcome to utilize any text of your choice.\"`: A sample text that you want to convert into embeddings.\n",
    "\n",
    "1. **Tokenize the Text:**\n",
    "    - `encoded_input = tokenizer(custom_text, return_tensors='tf')`: The tokenizer converts the text into a format suitable for the BERT model. The `return_tensors='tf'` argument tells the tokenizer to return TensorFlow tensors.\n",
    "\n",
    "1. **Generate Embeddings:**\n",
    "\n",
    "    - `output_embeddings = model(encoded_input)`: Passes the tokenized input to the BERT model. The model returns the embeddings, which are a rich, contextual representation of each token in the input text.\n",
    "\n",
    "1. **Understanding the Output:** The `output_embeddings` returned by the model is typically a complex structure containing several types of embeddings:\n",
    "\n",
    "    - **Last Hidden State:** The output corresponding to the last layer of the BERT model, which gives you the embeddings for each token in the input sequence.\n",
    "    - **Pooler Output:** A pooled output of the last hidden state, which represents the entire input sequence, often used in classification tasks.\n",
    "\n",
    "To print the dimensions of the output_embeddings, you would typically focus on these two parts. Here is how you can do it:\n",
    "\n",
    "In these lines of code:\n",
    "\n",
    "- `output_embeddings.last_hidden_state.shape` will give you the dimensions of the last hidden state, which is usually of the form `[batch_size, sequence_length, hidden_size]`.\n",
    "- `output_embeddings.pooler_output.shape` will give you the dimensions of the pooled output, typically `[batch_size, hidden_size]`.\n",
    "Understanding these dimensions:\n",
    "\n",
    "- **batch_size:** The number of sequences processed at a time (for your case, it will be 1 as you're processing a single sentence).\n",
    "- **sequence_length:** The length of the tokenized input (number of tokens).\n",
    "- **hidden_size:** The size of the hidden layers in the BERT model. For 'bert-base-cased', it is usually 768.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5390c1a",
   "metadata": {
    "papermill": {
     "duration": 0.043306,
     "end_time": "2024-03-01T13:15:13.335488",
     "exception": false,
     "start_time": "2024-03-01T13:15:13.292182",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# <div style=\"padding:20px;color:white;margin:0;font-size:24px;text-align:left;display:fill;border-radius:5px;background-color:#7b6b59;overflow:hidden\">Advanced Approaches: Fine-tune a pre-trained model</div>\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">Introduction</span>\n",
    "\n",
    "**What does fine-tuning a pre-trained model mean?** \n",
    "\n",
    "The fine-tuning technique is used to optimize a model’s performance on a new or different task. It is used to tailor a model to meet a specific need or domain, say cancer detection, in the field of healthcare. Pre-trained models are fine-tuned by training them on large amounts of labeled data for a certain task, such as Natural Language Processing (NLP) or image classification. Once trained, the model can be applied to similar new tasks or datasets with limited labeled data by fine-tuning the pre-trained model.\n",
    "\n",
    "The fine-tuning process is commonly used in transfer learning, where a pre-trained model is used as a starting point to train a new model for a contrasting but related task. A pre-trained model can significantly diminish the labeled data required to train a new model, making it an effective tool for tasks where labeled data is scarce or expensive.\n",
    "\n",
    "**How does fine-tuning pre-trained models work?**\n",
    "\n",
    "Fine-tuning a pre-trained model works by updating the parameters utilizing the available labeled data instead of starting the training process from the ground up. The following are the generic steps involved in fine-tuning:\n",
    "\n",
    "1. **Loading the pre-trained model:** The initial phase in the process is to select and load the right model, which has already been trained on a large amount of data, for a related task.\n",
    "\n",
    "1. **Modifying the model for the new task - Adjust the Architecture:** Once a pre-trained model is loaded, its top layers must be replaced or retrained to customize it for the new task. Adapting the pre-trained model to new data is necessary because the top layers are often task specific. After selecting the pre-trained model, you need to make modifications to the model’s architecture to fit the requirements of your specific task. This typically involves modifying the top layers of the model. For example, you may need to change the number of output neurons in the final layer to match the number of classes in your classification task.\n",
    "\n",
    "1. **Freezing particular layers:** The earlier layers facilitating low-level feature extraction are usually frozen in a pre-trained model. Since these layers have already learned general features that are useful for various tasks, freezing them may allow the model to preserve these features, avoiding overfitting the limited labeled data available in the new task. Depending on the complexity of your task and the size of your dataset, you can choose to freeze some layers in the pre-trained model. Freezing a layer means preventing it from updating its weights during the fine-tuning process. This can be beneficial if the lower layers of the pre-trained model have already learned general features that are useful for your task. On the other hand, unfreezing allows the corresponding layers to adapt to the new data during fine-tuning.\n",
    "\n",
    "1. **Training the new layers:** With the labeled data available for the new task, the newly created layers are then trained, all the while keeping the weights of the earlier layers constant. As a result, the model’s parameters can be adapted to the new task, and its feature representations can be refined. Once you have adjusted the architecture and decided which layers to freeze or unfreeze, it’s time to train the modified model on your task-specific dataset. During training, it’s advisable to use a smaller learning rate than what was used in the initial pre-training phase. This helps prevent drastic changes to the already learned representations while allowing the model to adapt to the new data.\n",
    "\n",
    "1. **Fine-tuning the model:** Once the new layers are trained, you can fine-tune the entire model on the new task using the available limited data. Every task and dataset is unique, and it may require further experimentation with hyperparameters, loss functions, and other training strategies. Fine-tuning is not a one-size-fits-all approach, and you may need to iterate and fine-tune your fine-tuning strategy to achieve optimal results.\n",
    "\n",
    "**Understanding fine-tuning with an example**\n",
    "\n",
    "Suppose you have a pre-trained model trained on a wide range of medical data or images that can detect abnormalities like tumors and want to adapt the model for a specific use case, say identifying a rare type of cancer, but you have a limited set of labeled data available. In such a case, you must fine-tune the model by adding new layers on top of the pre-trained model and training the newly added layers with the available data. Typically, the earlier layers of a pre-trained model, which extract low-level features, are frozen to prevent overfitting.\n",
    "\n",
    "**Best practices to follow when fine-tuning a pre-trained model**\n",
    "\n",
    "While fine-tuning a pre-trained model, several best practices can help ensure successful outcomes. Here are some key practices to follow:\n",
    "\n",
    "1. **Understand the pre-trained model:** Gain a comprehensive understanding of the pre-trained model architecture, its strengths, limitations, and the task it was initially trained on. This knowledge can enhance the fine-tuning process and help make appropriate modifications.\n",
    "\n",
    "1. **Select a relevant pre-trained model:** Choose a pre-trained model that aligns closely with the target task or domain. A model trained on similar data or a related task will provide a better starting point for fine-tuning.\n",
    "\n",
    "1. **Freeze early layers:** Typically, the lower layers of a pre-trained model capture generic features and patterns. Freeze these early layers during fine-tuning to preserve the learned representations. This practice helps prevent catastrophic forgetting and lets the model focus on task-specific fine-tuning.\n",
    "\n",
    "1. **Adjust learning rate**: Experiment with different learning rates during fine-tuning. It is typical to use a smaller learning rate compared to the initial pre-training phase. A lower learning rate allows the model to adapt more gradually and prevent drastic changes that could lead to overfitting.\n",
    "\n",
    "1. **Utilize transfer learning techniques:** Transfer learning methods can enhance fine-tuning performance. Techniques like feature extraction, where pre-trained layers are used as fixed feature extractors, or gradual unfreezing, where layers are unfrozen gradually during training, can help preserve and transfer valuable knowledge.\n",
    "\n",
    "1. **Regularize the model:** Apply regularization techniques, **such as dropout or weight decay,** during fine-tuning to prevent overfitting. Regularization helps the model generalize better and reduces the risk of memorizing specific training examples.\n",
    "\n",
    "1. **Monitor and evaluate performance:** Continuously monitor and evaluate the performance of the fine-tuned model on validation or holdout datasets. Use appropriate evaluation metrics to assess the model’s progress and make informed decisions on further fine-tuning adjustments.\n",
    "\n",
    "1. **Data augmentation:** Augment the training data by applying transformations, perturbations, or adding noise. Data augmentation can increase the diversity and generalizability of the training data, leading to better fine-tuning results.\n",
    "\n",
    "1. **Consider domain adaptation:** If the target task or domain significantly differs from the pre-training data, consider domain adaptation techniques. These methods aim to bridge the gap between the pre-training data and the target data, improving the model’s performance on the specific task.\n",
    "\n",
    "1. **Regularly backup and save checkpoints:** Save model checkpoints at regular intervals during fine-tuning to ensure progress is saved and prevent data loss. This practice allows for easy recovery and enables the exploration of different fine-tuning strategies.\n",
    "\n",
    "There are two ways to do it: Since we are looking to fine-tune the model for a downstream task like classification, we can directly use:\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">1. A simple way</span>\n",
    "\n",
    "**Fine-tuning pretrained NLP models with Huggingface’s Trainer:** *A simple way to fine-tune pretrained NLP models without native Pytorch or Tensorflow*\n",
    "\n",
    "While working on a data science competition, I was fine-tuning a pre-trained model and realised how tedious it was to fine-tune a model using native PyTorch or Tensorflow. I experimented with Huggingface’s **Trainer API** and was surprised by how easy it was.\n",
    "\n",
    "- **Train Our Classification Model:** Now that our input data is properly formatted, it’s time to fine tune the pre-trained model, for instance a BERT model.\n",
    "    - For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task.\n",
    "\n",
    "    - **Classification Head:** Finally, the output from the pooler is passed through the classification head, which simply involves projecting the pooled embedding into a space with dimensionality equal to the number of different classes. It is called a head because this component of the model can be swapped out to suit a particular task. This is in contrast to the backbone of BERT — responsible for creating the contextualized representations of the tokens in the sequence — that remains the same regardless of the task.\n",
    "    - Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task. [Here](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html) is the current list of classes provided for fine-tuning.\n",
    "    \n",
    "    - The `BertForSequenceClassification` class is the outermost class that we call to instantiate our BERT model. It houses both the base architecture (self.bert) and the classification head (self.classifier). The outputs are the logits for which there is one value for each class. Taking the maximum value of these logits will give us the predicted class. However, if it is desired to interpret the logits as probabilities the softmax function will need to be applied. `BertForSequenceClassification` performs fine-tuning of logistic regression layer on the output dimension of 768.\n",
    "   \n",
    "    - We’ll be using `BertForSequenceClassification`. This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
    "    \n",
    "    - So, in summary, we fine-tune the entire pre-trained BERT model, including the last layers specifically designed for our classification task. It adjusts the model to our document classification problem using the data we provide, but it doesn't train the model entirely from scratch. The distinction is that \"from scratch\" would mean initializing all the model's weights randomly and learning them solely from our data, which usually requires a much larger dataset and more computational resources. Here, we're leveraging the general understanding already embedded in the BERT model from its pre-training, which provides a significant head start for most NLP tasks.\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">2. Adding Custom Layers on Top of a Hugging Face Model</span>\n",
    "\n",
    "Alternatively, we can define a custom module, that created a bert model based on the pre-trained weights and adds layers on top of it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2764ee",
   "metadata": {
    "papermill": {
     "duration": 0.043303,
     "end_time": "2024-03-01T13:15:13.424096",
     "exception": false,
     "start_time": "2024-03-01T13:15:13.380793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#7b6b59;overflow:hidden\">Approach 3: Fine-tune a pre-trained model with 🤗 Transformers</div>\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">1. Introduction</span>\n",
    "\n",
    "There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, and allows you to use state-of-the-art models without having to train one from scratch. 🤗 Transformers provides access to thousands of pretrained models for a wide range of tasks. **When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning,** an incredibly powerful training technique. In this section, we will fine-tune a pretrained model with a deep learning framework of our choice:\n",
    "\n",
    "- Fine-tune a pretrained model with 🤗 Transformers PyTorch Trainer.\n",
    "- Fine-tune a pretrained model in TensorFlow with Keras.\n",
    "- Fine-tune a pretrained model in native PyTorch.\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">2. Create a dataset or Prepare the dataset</span>\n",
    "\n",
    "- **From in-memory data:** Eventually, it’s also possible to instantiate a datasets.Dataset directly from in-memory data, currently one or:\n",
    "    - a python dict, or\n",
    "    - a pandas dataframe.\n",
    "\n",
    "A `datasets.Dataset` instance is more precisely a table with rows and columns in which the columns are typed. Querying an example (a single row) will thus return a python dictionary with keys corresponding to columns names, and values corresponding to the example’s value for each column.\n",
    "\n",
    "You can get the number of rows and columns of the dataset with various standard attributes. \n",
    "\n",
    "Sometimes, you may need to create a dataset if you’re working with your own data. Creating a dataset with **🤗 Datasets confers all the advantages of the library to your dataset: fast loading and processing, stream enormous datasets, memory-mapping, and more.** You can easily and rapidly create a dataset with 🤗 Datasets low-code approaches, reducing the time it takes to start training a model. In many cases, it is as easy as dragging and dropping your data files into a dataset repository on the Hub.\n",
    "\n",
    "Creating a `Dataset` object from our dataset when fine-tuning a pre-trained model with Hugging Face Transformers is important for several reasons:\n",
    "\n",
    "1. **Efficiency:** The `Dataset` object is optimized for performance. It enables efficient data loading, preprocessing, and iteration, which is crucial when dealing with large datasets common in NLP tasks.\n",
    "\n",
    "1. **Easy Integration:** Hugging Face Transformers and Datasets libraries are designed to work together seamlessly. By using a `Dataset` object, we can directly apply transformations, tokenization, and batching, which are necessary for preparing our data for the model.\n",
    "\n",
    "1. **Consistency and Reproducibility:** Creating a `Dataset` object ensures that data processing steps are consistent. This is important for reproducibility of results, a key aspect of any scientific experiment. You can share your dataset with others, and they'll be able to achieve the same results using the same preprocessing steps.\n",
    "\n",
    "1. **Advanced Features:** The Dataset object comes with many advanced features like easy slicing, indexing, and even complex transformations. It supports operations like `map`, `filter`, and `shuffle`, which are essential for training neural networks.\n",
    "\n",
    "1. **Scalability:** Datasets in Hugging Face are designed to be scalable. They can handle datasets much larger than your system's RAM and facilitate distributed training by efficiently managing memory and processing.\n",
    "\n",
    "1. **Community Standards:** Using widely adopted standards like the Dataset object from Hugging Face ensures that your work is accessible and understandable by a broader community. It also makes it easier for you to use datasets and models shared by others.\n",
    "\n",
    "In essence, **using a `Dataset` object simplifies the data preprocessing pipeline, ensures efficient and reproducible training, and aligns your work with community practices.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6efc5e5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:13.512395Z",
     "iopub.status.busy": "2024-03-01T13:15:13.511999Z",
     "iopub.status.idle": "2024-03-01T13:15:13.805147Z",
     "shell.execute_reply": "2024-03-01T13:15:13.804256Z"
    },
    "papermill": {
     "duration": 0.339918,
     "end_time": "2024-03-01T13:15:13.807655",
     "exception": false,
     "start_time": "2024-03-01T13:15:13.467737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the train dataset is: (40151, 5)\n",
      "---------------------------------------------------\n",
      "The number of columns in the train dataset is: 5\n",
      "The column names are: ['text', 'label', 'prompt_name', 'source', 'RDizzl3_seven']\n",
      "The columns' detailed types are: {'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None), 'prompt_name': Value(dtype='string', id=None), 'source': Value(dtype='string', id=None), 'RDizzl3_seven': Value(dtype='bool', id=None)}\n",
      "---------------------------------------------------\n",
      "The number of rows in the train dataset is: 40151\n",
      "Or the length of the train dataset is: 40151\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "valid_dataset = Dataset.from_pandas(valid_df)\n",
    "\n",
    "print(f\"The shape of the train dataset is: {train_dataset.shape}\")\n",
    "print(\"---------------------------------------------------\")\n",
    "\n",
    "print(f\"The number of columns in the train dataset is: {train_dataset.num_columns}\")\n",
    "print(f\"The column names are: {train_dataset.column_names}\")\n",
    "print(f\"The columns' detailed types are: {train_dataset.features}\")\n",
    "\n",
    "print(\"---------------------------------------------------\")\n",
    "print(f\"The number of rows in the train dataset is: {train_dataset.num_rows}\")\n",
    "print(f\"Or the length of the train dataset is: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "807d096e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:13.896503Z",
     "iopub.status.busy": "2024-03-01T13:15:13.896139Z",
     "iopub.status.idle": "2024-03-01T13:15:13.900584Z",
     "shell.execute_reply": "2024-03-01T13:15:13.899737Z"
    },
    "papermill": {
     "duration": 0.051137,
     "end_time": "2024-03-01T13:15:13.902782",
     "exception": false,
     "start_time": "2024-03-01T13:15:13.851645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# While you can access a single row with the train_dataset[i] pattern, \n",
    "# you can also access several rows using slice notation or with a list of indices (or a numpy/torch/tf array of indices):\n",
    "#print(train_dataset[1])\n",
    "#print(\"--------------------------------\\n\")\n",
    "#print(train_dataset[:2])\n",
    "#print(\"--------------------------------\\n\")\n",
    "#print(train_dataset[\"text\"][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff65e8c",
   "metadata": {
    "papermill": {
     "duration": 0.044035,
     "end_time": "2024-03-01T13:15:13.991467",
     "exception": false,
     "start_time": "2024-03-01T13:15:13.947432",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color: #7b6b59;\">3. Initialise pre-trained model and tokenizer</span>\n",
    "\n",
    "Before we can fine-tune a pretrained model, we have to prepare it for training. As you now know, we need a tokenizer to process the text and include a **padding** and **truncation** strategy to handle any variable sequence lengths. To process our dataset in one step, use 🤗 Datasets `map` method to apply a preprocessing function over the entire dataset:\n",
    "\n",
    "To feed our text to deberta, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.\n",
    "\n",
    "The tokenization must be performed by the tokenizer included with deberta–the below cell will download this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cefb93f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:14.081689Z",
     "iopub.status.busy": "2024-03-01T13:15:14.081198Z",
     "iopub.status.idle": "2024-03-01T13:15:16.593270Z",
     "shell.execute_reply": "2024-03-01T13:15:16.592065Z"
    },
    "papermill": {
     "duration": 2.560221,
     "end_time": "2024-03-01T13:15:16.595578",
     "exception": false,
     "start_time": "2024-03-01T13:15:14.035357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea3c4eb9d67451e8c8efff754e5198a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42259d58d8d479d9416e4120ee82306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2f10dea84c43ce829f851228e77018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-xsmall\", use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957ee23d",
   "metadata": {
    "papermill": {
     "duration": 0.044226,
     "end_time": "2024-03-01T13:15:16.685122",
     "exception": false,
     "start_time": "2024-03-01T13:15:16.640896",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Since we are using a pretrained model, we need to ensure that the input data is in the same form as what the pretrained model was trained on. Thus, we would need to instantiate the tokenizer using the name of the model.\n",
    "\n",
    "Now that the model and tokenizer have been initialised, we can proceed to preprocess the data.\n",
    "\n",
    "**Preprocess text using pretrained tokenizer**\n",
    "\n",
    "Let us preprocess the text using the tokenizer intialised earlier.\n",
    "\n",
    "The input text that we are using for the tokenizer is a list of strings.\n",
    "\n",
    "We have set `padding=True`, `truncation=True`, `max_length=128` so that we can get same length inputs for the model- the long texts will be truncated to 128 tokens while the short texts will have extra tokens added to make it 128 tokens.\n",
    "\n",
    "128 tokens is used because this is the maximum token length that the pre-trained model can take.\n",
    "\n",
    "After tokenizing your text, you will get a python dictionary with 3 keys:\n",
    "\n",
    "- Input_ids\n",
    "- token_type_ids\n",
    "- attention_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b28693e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:16.777333Z",
     "iopub.status.busy": "2024-03-01T13:15:16.776980Z",
     "iopub.status.idle": "2024-03-01T13:15:16.782077Z",
     "shell.execute_reply": "2024-03-01T13:15:16.781052Z"
    },
    "papermill": {
     "duration": 0.053396,
     "end_time": "2024-03-01T13:15:16.784249",
     "exception": false,
     "start_time": "2024-03-01T13:15:16.730853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_function(samples):\n",
    "    return tokenizer(samples[\"text\"], max_length=128, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da920576",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:16.874847Z",
     "iopub.status.busy": "2024-03-01T13:15:16.873595Z",
     "iopub.status.idle": "2024-03-01T13:15:54.488334Z",
     "shell.execute_reply": "2024-03-01T13:15:54.487583Z"
    },
    "papermill": {
     "duration": 37.6615,
     "end_time": "2024-03-01T13:15:54.490444",
     "exception": false,
     "start_time": "2024-03-01T13:15:16.828944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd387ad0176481a9f95dea2e2063c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee26d3c63f04371be2d54e446624229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_valid_dataset = valid_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0186f1c",
   "metadata": {
    "papermill": {
     "duration": 0.043089,
     "end_time": "2024-03-01T13:15:54.580325",
     "exception": false,
     "start_time": "2024-03-01T13:15:54.537236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color: #7b6b59;\">4. Train with PyTorch Trainer</span>\n",
    "\n",
    "🤗 Transformers provides a Trainer class optimized for training 🤗 Transformers models, making it easier to start training without manually writing your own training loop. The Trainer API supports a wide range of training options and features such as logging, gradient accumulation, and mixed precision.\n",
    "\n",
    "1. **Start by loading your model and specify the number of expected labels.**: You will see a warning about some of the pretrained weights not being used and some weights being randomly initialized. Don’t worry, this is completely normal! The pretrained head of the BERT model is discarded, and replaced with a randomly initialized classification head. You will fine-tune this new model head on your sequence classification task, transferring the knowledge of the pretrained model to it.\n",
    "1. **Training hyperparameters:** Next, create a `TrainingArguments` class which contains all the hyperparameters you can tune as well as flags for activating different training options. For this tutorial you can start with the default training hyperparameters, but feel free to experiment with these to find your optimal settings.\n",
    "1. **Evaluate:** `Trainer` does not automatically evaluate model performance during training. You’ll need to pass Trainer a function to compute and report metrics. \n",
    "1. **Trainer:** Create a `Trainer` object with your model, training arguments, training and test datasets, and evaluation function. Then fine-tune your model by calling `train()`.\n",
    "\n",
    "For this task, we first want to modify the pre-trained Deberta model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task.\n",
    "\n",
    "Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained Deberta model, each has different top layers and output types designed to accomodate their specific NLP task. We’ll be using `AutoModelForSequenceClassification`. This is the normal Deberta model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task. Have also a look on [BertForSequenceClassification source code](https://huggingface.co/transformers/v3.0.2/_modules/transformers/modeling_bert.html#BertForSequenceClassification).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a667f3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:54.667972Z",
     "iopub.status.busy": "2024-03-01T13:15:54.667606Z",
     "iopub.status.idle": "2024-03-01T13:15:56.724362Z",
     "shell.execute_reply": "2024-03-01T13:15:56.723426Z"
    },
    "papermill": {
     "duration": 2.103128,
     "end_time": "2024-03-01T13:15:56.726635",
     "exception": false,
     "start_time": "2024-03-01T13:15:54.623507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0047ec3700cf47799b265b41dbc09830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/241M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-xsmall and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DebertaV2ForSequenceClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 384, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 384)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=384, out_features=2, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-xsmall\", num_labels=2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b0d8b28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:56.815806Z",
     "iopub.status.busy": "2024-03-01T13:15:56.815184Z",
     "iopub.status.idle": "2024-03-01T13:15:56.821684Z",
     "shell.execute_reply": "2024-03-01T13:15:56.820781Z"
    },
    "papermill": {
     "duration": 0.052642,
     "end_time": "2024-03-01T13:15:56.823590",
     "exception": false,
     "start_time": "2024-03-01T13:15:56.770948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2509"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_name = \"roc_auc\"\n",
    "train_batch_size = 4\n",
    "eval_batch_size = 32\n",
    "grad_acc = 4\n",
    "num_steps = len(train_df) // (train_batch_size * grad_acc)\n",
    "num_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caefd1d0",
   "metadata": {
    "papermill": {
     "duration": 0.043128,
     "end_time": "2024-03-01T13:15:56.910040",
     "exception": false,
     "start_time": "2024-03-01T13:15:56.866912",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Defining TrainingArguments and Trainer**\n",
    "\n",
    "Here is where the magic of the Trainer function is. We can define the training parameters in the TrainingArguments and Trainer class as well as train the model with a single command.\n",
    "\n",
    "We need to first define a function to calculate the metrics of the validation set. Since this is a binary classification problem, we can use accuracy, precision, recall and f1 score.\n",
    "\n",
    "Next, we specify some training parameters, set the pretrained model, train data and evaluation data in the TrainingArgs and Trainer class.\n",
    "\n",
    "After we have defined the parameters , simply run `trainer.train()` to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24905fbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:57.000225Z",
     "iopub.status.busy": "2024-03-01T13:15:56.999841Z",
     "iopub.status.idle": "2024-03-01T13:15:57.007660Z",
     "shell.execute_reply": "2024-03-01T13:15:57.006912Z"
    },
    "papermill": {
     "duration": 0.055908,
     "end_time": "2024-03-01T13:15:57.009581",
     "exception": false,
     "start_time": "2024-03-01T13:15:56.953673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"deberta-v3-xsmall_finetuned\",\n",
    "    evaluation_strategy=\"steps\", # If you’d like to monitor your evaluation metrics during fine-tuning, specify the evaluation_strategy parameter in your training arguments to report the evaluation metric at the end of each epoch\n",
    "    save_strategy = \"steps\",\n",
    "    eval_steps = num_steps // 3,\n",
    "    save_steps = num_steps // 3,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    gradient_accumulation_steps=grad_acc,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=metric_name,\n",
    "    report_to='none', # change to wandb after enabling internet access\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa8e0684",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:57.100167Z",
     "iopub.status.busy": "2024-03-01T13:15:57.099783Z",
     "iopub.status.idle": "2024-03-01T13:15:57.105645Z",
     "shell.execute_reply": "2024-03-01T13:15:57.104832Z"
    },
    "papermill": {
     "duration": 0.053398,
     "end_time": "2024-03-01T13:15:57.107470",
     "exception": false,
     "start_time": "2024-03-01T13:15:57.054072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n",
    "    auc = roc_auc_score(labels, probs[:,1], multi_class='ovr')\n",
    "    return {\"roc_auc\": auc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fbf3318",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:57.196514Z",
     "iopub.status.busy": "2024-03-01T13:15:57.195716Z",
     "iopub.status.idle": "2024-03-01T13:15:57.208685Z",
     "shell.execute_reply": "2024-03-01T13:15:57.207873Z"
    },
    "papermill": {
     "duration": 0.059472,
     "end_time": "2024-03-01T13:15:57.210671",
     "exception": false,
     "start_time": "2024-03-01T13:15:57.151199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c17771d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:57.301916Z",
     "iopub.status.busy": "2024-03-01T13:15:57.301068Z",
     "iopub.status.idle": "2024-03-01T13:15:57.305210Z",
     "shell.execute_reply": "2024-03-01T13:15:57.304342Z"
    },
    "papermill": {
     "duration": 0.052153,
     "end_time": "2024-03-01T13:15:57.307248",
     "exception": false,
     "start_time": "2024-03-01T13:15:57.255095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e190a0d2",
   "metadata": {
    "papermill": {
     "duration": 0.043889,
     "end_time": "2024-03-01T13:15:57.394729",
     "exception": false,
     "start_time": "2024-03-01T13:15:57.350840",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color: #7b6b59;\">5. Making prediction</span>\n",
    "\n",
    "After the model is trained, we repeat the same steps for the test data:\n",
    "\n",
    "1. Tokenize test data with pretrained tokenizer\n",
    "1. Create torch dataset\n",
    "1. Load trained model\n",
    "1. Define Trainer\n",
    "\n",
    "To load the trained model from the previous steps, set the model_path to the path containing the trained model weights.\n",
    "\n",
    "To make prediction, only a single command is needed as well `test_trainer.predict(test_dataset)` .\n",
    "\n",
    "After making a prediction, you will only get the raw prediction. Additional preprocessing steps will be needed to get it to a usable format.\n",
    "\n",
    "Since the task is just a simple sequence classification task, we can just obtain the argmax across axis 1. Note that other NLP tasks may require different ways to preprocess the raw predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67a0324f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:57.485098Z",
     "iopub.status.busy": "2024-03-01T13:15:57.484276Z",
     "iopub.status.idle": "2024-03-01T13:15:57.488967Z",
     "shell.execute_reply": "2024-03-01T13:15:57.488002Z"
    },
    "papermill": {
     "duration": 0.052373,
     "end_time": "2024-03-01T13:15:57.491068",
     "exception": false,
     "start_time": "2024-03-01T13:15:57.438695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\n",
    "# test_ds = Dataset.from_pandas(test)\n",
    "# test_ds_enc = test_ds.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a780d97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:57.586025Z",
     "iopub.status.busy": "2024-03-01T13:15:57.585673Z",
     "iopub.status.idle": "2024-03-01T13:15:57.589968Z",
     "shell.execute_reply": "2024-03-01T13:15:57.589007Z"
    },
    "papermill": {
     "duration": 0.052918,
     "end_time": "2024-03-01T13:15:57.592023",
     "exception": false,
     "start_time": "2024-03-01T13:15:57.539105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test_preds = trainer.predict(test_ds_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63cb0952",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:57.683787Z",
     "iopub.status.busy": "2024-03-01T13:15:57.683383Z",
     "iopub.status.idle": "2024-03-01T13:15:57.687771Z",
     "shell.execute_reply": "2024-03-01T13:15:57.686984Z"
    },
    "papermill": {
     "duration": 0.051828,
     "end_time": "2024-03-01T13:15:57.689773",
     "exception": false,
     "start_time": "2024-03-01T13:15:57.637945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# logits = test_preds.predictions\n",
    "# probs = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n",
    "# sub = pd.DataFrame()\n",
    "# sub['id'] = test['id']\n",
    "# sub['generated'] = probs[:,1]\n",
    "# sub.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07868be7",
   "metadata": {
    "papermill": {
     "duration": 0.043534,
     "end_time": "2024-03-01T13:15:57.777011",
     "exception": false,
     "start_time": "2024-03-01T13:15:57.733477",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#7b6b59;overflow:hidden\">Approach 4: Fine-tuning by Adding Custom Layers on Top of a Hugging Face Pre-trained Model</div>\n",
    "\n",
    "\n",
    "We are accustomed to the canonical way of fine-tuning: append just an additional output layer after Transformer for downstream tasks or back-end part of models which takes representations from the last layer of the pre-trained language models as the default input.\n",
    "\n",
    "However, due to the multi-layer structure of Transformers, different layers capture different levels of representations. They learn a rich hierarchy of linguistic information i.e. with surface features in lower layers, syntactic features in middle layers, and semantic features in higher layers.\n",
    "\n",
    "<img width=\"1023\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/6a64afa0-0276-4706-a05a-706153b06507\">\n",
    "\n",
    "The BERT authors tested word-embedding strategies by feeding different vector combinations as input features to a BiLSTM used on a named entity recognition task and observing the resulting F1 scores. Concatenation of the last four layers produced the best results.\n",
    "\n",
    "This is partially demonstrated by noting that the different layers of BERT encode very different kinds of information, so the appropriate pooling strategy will change depending on the application because different layers encode different kinds of information. This holds true for other variants as well.\n",
    "\n",
    "\n",
    "The notebook will show many different ways these outputs and hidden representations can be utilized to do much more than just adding an output layer. Below are the various techniques we will be implementing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5e11ed",
   "metadata": {
    "papermill": {
     "duration": 0.043829,
     "end_time": "2024-03-01T13:15:57.864204",
     "exception": false,
     "start_time": "2024-03-01T13:15:57.820375",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span style=\"color: #7b6b59;\">Step 4.1: Utils</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98d1d1d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:57.954605Z",
     "iopub.status.busy": "2024-03-01T13:15:57.953910Z",
     "iopub.status.idle": "2024-03-01T13:15:57.958535Z",
     "shell.execute_reply": "2024-03-01T13:15:57.957653Z"
    },
    "papermill": {
     "duration": 0.051893,
     "end_time": "2024-03-01T13:15:57.960634",
     "exception": false,
     "start_time": "2024-03-01T13:15:57.908741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"./\"\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cc4e5e",
   "metadata": {
    "papermill": {
     "duration": 0.043641,
     "end_time": "2024-03-01T13:15:58.048249",
     "exception": false,
     "start_time": "2024-03-01T13:15:58.004608",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The `seed_everything` function is designed to set a fixed seed for various random number generators across different libraries and configurations in Python, to ensure that the execution of the code is deterministic. This is particularly useful in machine learning experiments where reproducibility of results is important. Several operations in machine learning and data processing introduce randomness, which can lead to different outcomes in different runs if the random number generators (RNGs) are not controlled. Here are some common sources of randomness:\n",
    "\n",
    "1. **Data Shuffling:** In most machine learning workflows, datasets are shuffled before training to ensure that the model does not learn any unintended patterns from the order of the data. This shuffling process is random.\n",
    "\n",
    "1. **Weight Initialization:** Neural networks and many other models initialize their parameters (weights) randomly. This random initialization can lead to different starting points for the training process, affecting the final model.\n",
    "\n",
    "1. **Mini-batch Selection:** During training, especially with stochastic gradient descent (SGD) and its variants, data is often divided into mini-batches randomly for each training epoch. The selection of samples for each mini-batch introduces randomness.\n",
    "\n",
    "1. **Dropout:** Dropout is a regularization technique used in neural networks where a random subset of neurons is \"dropped\" (i.e., their output is temporarily set to zero) during each training iteration to prevent overfitting.\n",
    "\n",
    "1. **Ensemble Methods:** Some ensemble methods, like Random Forests or bagging techniques, rely on randomness to create diversity among the models they combine. For example, Random Forests use bootstrap sampling (sampling with replacement) and random feature selection for splitting nodes.\n",
    "\n",
    "1. **Random Seeds in Algorithms:** Some machine learning algorithms, such as k-means clustering or algorithms that involve stochastic optimization, use random seeds to initiate processes.\n",
    "\n",
    "1. **Data Augmentation:** In deep learning, particularly in computer vision tasks, data augmentation techniques (like random rotations, flipping, cropping, etc.) are used to artificially expand the training dataset by applying random transformations to the original images.\n",
    "\n",
    "1. **Exploration in Reinforcement Learning:** Reinforcement learning algorithms often include an exploration mechanism where actions are chosen randomly to explore the environment, as opposed to exploiting the currently known best strategy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e51e656b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:58.139608Z",
     "iopub.status.busy": "2024-03-01T13:15:58.138863Z",
     "iopub.status.idle": "2024-03-01T13:15:58.145201Z",
     "shell.execute_reply": "2024-03-01T13:15:58.144339Z"
    },
    "papermill": {
     "duration": 0.054312,
     "end_time": "2024-03-01T13:15:58.147245",
     "exception": false,
     "start_time": "2024-03-01T13:15:58.092933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    \"\"\"\n",
    "    Seed all possible random number generators to ensure reproducibility of results.\n",
    "\n",
    "    This function sets a fixed seed for the random number generators in the `random`, `numpy`, and `torch` libraries,\n",
    "    and also ensures deterministic behavior in CUDA operations if PyTorch is used with CUDA.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): The seed value to use for all random number generators. Defaults to 42.\n",
    "    \"\"\"\n",
    "    random.seed(seed)  # Seed Python's built-in random module.\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # Set PYTHONHASHSEED environment variable to ensure reproducibility when hashing objects in Python.\n",
    "    np.random.seed(seed)  # Seed NumPy's random number generator.\n",
    "    torch.manual_seed(seed)  # Seed PyTorch's random number generator for CPU operations.\n",
    "    torch.cuda.manual_seed(seed)  # Seed PyTorch's random number generator for CUDA (GPU) operations.\n",
    "    \n",
    "    # Ensure that CUDA operations are deterministic. This might impact performance.\n",
    "    torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8780169",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:58.237697Z",
     "iopub.status.busy": "2024-03-01T13:15:58.236969Z",
     "iopub.status.idle": "2024-03-01T13:15:58.241810Z",
     "shell.execute_reply": "2024-03-01T13:15:58.240825Z"
    },
    "papermill": {
     "duration": 0.05256,
     "end_time": "2024-03-01T13:15:58.244027",
     "exception": false,
     "start_time": "2024-03-01T13:15:58.191467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81857f91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:58.335416Z",
     "iopub.status.busy": "2024-03-01T13:15:58.334555Z",
     "iopub.status.idle": "2024-03-01T13:15:58.342330Z",
     "shell.execute_reply": "2024-03-01T13:15:58.341341Z"
    },
    "papermill": {
     "duration": 0.055581,
     "end_time": "2024-03-01T13:15:58.344616",
     "exception": false,
     "start_time": "2024-03-01T13:15:58.289035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_logger(filename=OUTPUT_DIR+'train'):\n",
    "    \"\"\"\n",
    "    Creates a logger that outputs log messages to both the console and a file.\n",
    "\n",
    "    This function configures a logger to write log messages with the INFO level\n",
    "    and above to both the standard output stream (console) and a specified log file.\n",
    "    The format of the log messages is set to display the message content only.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The base name of the log file. The '.log' extension will be appended\n",
    "                        to this base name. Defaults to OUTPUT_DIR+'train', where OUTPUT_DIR\n",
    "                        is assumed to be a predefined directory path.\n",
    "\n",
    "    Returns:\n",
    "        logging.Logger: A configured logger object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a logger with the name of the current module\n",
    "    logger = getLogger(__name__)\n",
    "    \n",
    "    # Set the logger's severity level to INFO\n",
    "    logger.setLevel(INFO)\n",
    "    \n",
    "    # Create a stream handler to output log messages to the console\n",
    "    handler1 = StreamHandler()\n",
    "    # Set the format for the stream handler to display only the message content\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    \n",
    "    # Create a file handler to output log messages to a file, appending '.log' to the filename\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    # Set the format for the file handler to display only the message content\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    \n",
    "    # Add the stream and file handlers to the logger\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    \n",
    "    # Return the configured logger\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dabf03eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:58.434843Z",
     "iopub.status.busy": "2024-03-01T13:15:58.434134Z",
     "iopub.status.idle": "2024-03-01T13:15:58.439506Z",
     "shell.execute_reply": "2024-03-01T13:15:58.438506Z"
    },
    "papermill": {
     "duration": 0.05298,
     "end_time": "2024-03-01T13:15:58.441641",
     "exception": false,
     "start_time": "2024-03-01T13:15:58.388661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the LOGGER by calling get_logger function without specifying a filename\n",
    "# It uses the default filename derived from OUTPUT_DIR+'train'\n",
    "LOGGER = get_logger()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90481bc",
   "metadata": {
    "papermill": {
     "duration": 0.045642,
     "end_time": "2024-03-01T13:15:58.533019",
     "exception": false,
     "start_time": "2024-03-01T13:15:58.487377",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span style=\"color: #7b6b59;\">Step 4.2: Data Loading</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f188e67f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:58.624135Z",
     "iopub.status.busy": "2024-03-01T13:15:58.623240Z",
     "iopub.status.idle": "2024-03-01T13:15:58.691707Z",
     "shell.execute_reply": "2024-03-01T13:15:58.690676Z"
    },
    "papermill": {
     "duration": 0.115987,
     "end_time": "2024-03-01T13:15:58.693967",
     "exception": false,
     "start_time": "2024-03-01T13:15:58.577980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (1378, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0059830c</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars. Cars have been around since they became ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005db917</td>\n",
       "      <td>0</td>\n",
       "      <td>Transportation is a large necessity in most co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>008f63e3</td>\n",
       "      <td>0</td>\n",
       "      <td>\"America's love affair with it's vehicles seem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00940276</td>\n",
       "      <td>0</td>\n",
       "      <td>How often do you ride in a car? Do you drive a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00c39458</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars are a wonderful thing. They are perhaps o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  prompt_id                                               text  \\\n",
       "0  0059830c          0  Cars. Cars have been around since they became ...   \n",
       "1  005db917          0  Transportation is a large necessity in most co...   \n",
       "2  008f63e3          0  \"America's love affair with it's vehicles seem...   \n",
       "3  00940276          0  How often do you ride in a car? Do you drive a...   \n",
       "4  00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.shape: (3, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>2</td>\n",
       "      <td>Aaa bbb ccc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1111bbbb</td>\n",
       "      <td>3</td>\n",
       "      <td>Bbb ccc ddd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2222cccc</td>\n",
       "      <td>4</td>\n",
       "      <td>CCC ddd eee.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  prompt_id          text\n",
       "0  0000aaaa          2  Aaa bbb ccc.\n",
       "1  1111bbbb          3  Bbb ccc ddd.\n",
       "2  2222cccc          4  CCC ddd eee."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission.shape: (3, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1111bbbb</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2222cccc</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  generated\n",
       "0  0000aaaa        0.1\n",
       "1  1111bbbb        0.9\n",
       "2  2222cccc        0.4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\")\n",
    "train.rename(columns={'generated': 'label'}, inplace=True)\n",
    "\n",
    "test = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\")\n",
    "submission = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv\")\n",
    "\n",
    "print(f\"train.shape: {train.shape}\")\n",
    "display(train.head())\n",
    "print(f\"test.shape: {test.shape}\")\n",
    "display(test.head())\n",
    "print(f\"submission.shape: {submission.shape}\")\n",
    "display(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60357eb9",
   "metadata": {
    "papermill": {
     "duration": 0.044103,
     "end_time": "2024-03-01T13:15:58.785336",
     "exception": false,
     "start_time": "2024-03-01T13:15:58.741233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span style=\"color: #7b6b59;\">Step 4.3: k-fold Cross-Validation</span>\n",
    "\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">Cross-Validation</span>\n",
    "\n",
    "In machine learning (ML), generalization usually refers to the ability of an algorithm to be effective across various inputs. It means that the ML model does not encounter performance degradation on the new inputs from the same distribution of the training data.\n",
    "\n",
    "For human beings generalization is the most natural thing possible. We can classify on the fly. For example, we would definitely recognize a dog even if we didn’t see this breed before. Nevertheless, it might be quite a challenge for an ML model. That’s why checking the algorithm’s ability to generalize is an important task that requires a lot of attention when building the model.\n",
    "\n",
    "To do that, we use Cross-Validation (CV). There is always a need to validate the stability of your machine learning model. I mean you just can’t fit the model to your training data and hope it would accurately work for the real data it has never seen before. You need some kind of assurance that your model has got most of the patterns from the data correct, and its not picking up too much on the noise, or in other words its low on bias and variance.\n",
    "\n",
    "Cross Validation is a very useful technique:\n",
    "\n",
    "- for **assessing the effectiveness of your model**, particularly in cases where you need to mitigate overfitting. \n",
    "- It is also of use in **determining the hyper parameters of your model, in the sense that which parameters will result in lowest test error.** \n",
    "\n",
    "This is all the basic you need to get started with cross validation. You can get started with all kinds of validation techniques using `Scikit-Learn`, that gets you up and running with just a few lines of code in python.\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">What is cross-validation?</span>\n",
    "\n",
    "This process of deciding whether the numerical results quantifying hypothesized relationships between variables, are acceptable as descriptions of the data, is known as validation. Generally, an error estimation for the model is made after training, better known as evaluation of residuals. In this process, a numerical estimate of the difference in predicted and original responses is done, also called the training error. However, this only gives us an idea about how well our model does on data used to train it. **Now its possible that the model is underfitting or overfitting the data. So, the problem with this evaluation technique is that it does not give an indication of how well the learner will generalize to an independent/ unseen data set. Getting this idea about our model is known as Cross Validation.**\n",
    "\n",
    "- **Cross-validation is a technique for evaluating a machine learning model and testing its performance.** CV is commonly used in applied ML tasks. It helps to compare and select an appropriate model for the specific predictive modeling problem.\n",
    "\n",
    "CV is easy to understand, easy to implement, and it tends to have a lower bias than other methods used to count the model’s efficiency scores. All this makes cross-validation a powerful tool for selecting the best model for the specific task.\n",
    "\n",
    "There are a lot of different techniques that may be used to cross-validate a model. Still, all of them have a similar algorithm:\n",
    "\n",
    "1. **Divide** the dataset into two parts: one for training, other for testing\n",
    "1. **Train** the model on the training set\n",
    "1. **Validate** the model on the test set\n",
    "1. **Repeat** 1-3 steps a couple of times. This number depends on the CV method that you are using\n",
    "\n",
    "As you may know, there are plenty of CV techniques. Some of them are commonly used, others work only in theory. \n",
    "\n",
    "1. **Hold-out**\n",
    "1. **K-folds**\n",
    "1. **Stratified K-folds**\n",
    "1. **Repeated K-folds**\n",
    "1. **Nested K-folds**\n",
    "1. **Time series CV**\n",
    "\n",
    "Above listed validation techniques are also referred to as **Non-exhaustive cross validation methods**. These do not compute all ways of splitting the original sample, i.e. you just have to decide how many subsets need to be made. Also, these are approximations of method listed below, also called **Exhaustive Methods,** that computes all possible ways the data can be split into training and test sets.\n",
    "\n",
    "1. **Leave-one-out**\n",
    "1. **Leave-p-out**\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Cross-Validation Techniques</span>\n",
    "\n",
    "1. **Hold-out cross-validation:** Hold-out cross-validation is the simplest and most common technique. You might not know that it is a hold-out method but you certainly use it every day. *We usually use the hold-out method on large datasets as it requires training the model only once.* It is really easy to implement hold-out. The error estimation then tells how our model is doing on unseen data or the validation set. For example, you may do it using `sklearn.model_selection.train_test_split`. The algorithm of hold-out technique:\n",
    "\n",
    "    - Divide the dataset into two parts: the **training set** and the **test set**. Usually, 80% of the dataset goes to the training set and 20% to the test set but you may choose any splitting that suits you better\n",
    "    - Train the model on the training set\n",
    "    - Validate on the test set\n",
    "    - Save the result of the validation\n",
    "    \n",
    "    1. Disadvantages:\n",
    "        - For example, a dataset that is not completely even distribution-wise. If so we may end up in a rough spot after the split. For example, the training set will not represent the test set. Both training and test sets may differ a lot, one of them might be easier or harder.  This is a simple kind of cross validation technique, also known as the holdout method. Although this method doesn’t take any overhead to compute and is better than traditional validation, it still suffers from issues of high variance. This is because it is not certain which data points will end up in the validation set and the result might be entirely different for different sets.\n",
    "        - Moreover, the fact that we test our model only once might be a bottleneck for this method. Due to the reasons mentioned before, the result obtained by the hold-out technique may be considered inaccurate. \n",
    "\n",
    "1. **k-Fold cross-validation:** k-Fold cross-validation is a technique that minimizes the disadvantages of the hold-out method. k-Fold introduces a new way of splitting the dataset which helps to overcome the “test only once bottleneck”. As there is never enough data to train your model, removing a part of it for validation poses a problem of underfitting. **By reducing the training data, we risk losing important patterns/ trends in data set, which in turn increases error induced by bias.** So, what we require is a method that provides ample data for training the model and also leaves ample data for validation. K Fold cross validation does exactly that. In K Fold cross validation, the data is divided into k subsets. Now the holdout method is repeated k times, such that each time, one of the k subsets is used as the test set/ validation set and the other k-1 subsets are put together to form a training set. The error estimation is averaged over all k trials to get total effectiveness of our model. As can be seen, every data point gets to be in a validation set exactly once, and gets to be in a training set k-1 times. This significantly reduces bias as we are using most of the data for fitting, and also significantly reduces variance as most of the data is also being used in validation set. Interchanging the training and test sets also adds to the effectiveness of this method. **As a general rule and empirical evidence, K = 5 or 10 is generally preferred, but nothing’s fixed and it can take any value.**\n",
    "\n",
    "    The algorithm of the k-Fold technique:\n",
    "    \n",
    "    <img width=\"500\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/376f6016-01a3-499d-816f-a677f544bf2e\">\n",
    "    \n",
    "    - Pick a number of folds – k. Usually, k is 5 or 10 but you can choose any number which is less than the dataset’s length.\n",
    "    - Split the dataset into k equal (if possible) parts (they are called folds)\n",
    "    - Choose k – 1 folds as the training set. The remaining fold will be the test set\n",
    "    - Train the model on the training set. On each iteration of cross-validation, you must train a new model independently of the model trained on the previous iteration\n",
    "    - Validate on the test set\n",
    "    - Save the result of the validation\n",
    "    - Repeat steps 3 – 6 k times. Each time use the remaining  fold as the test set. \n",
    "    - In the end, you should have validated the model on every fold that you have. To get the final score average the results that you got on step 6.\n",
    "    \n",
    "    To perform k-Fold cross-validation you can use `sklearn.model_selection.KFold`. In general, it is always better to use k-Fold technique instead of hold-out. In a head to head, comparison k-Fold gives a more stable and trustworthy result since training and testing is performed on several different parts of the dataset. We can make the overall score even more robust if we increase the number of folds to test the model on many different sub-datasets.\n",
    "    \n",
    "    1. Disadvantages:\n",
    "        - Still, k-Fold method has a disadvantage. Increasing k results in training more models and the training process might be really expensive and time-consuming.\n",
    "\n",
    "1. **Stratified k-Fold cross-validation:** Sometimes we may face a large imbalance of the target value in the dataset. For example, in a dataset concerning wristwatch prices, there might be a larger number of wristwatch having a high price. In the case of classification, in cats and dogs dataset there might be a large shift towards the dog class. **Stratified k-Fold is a variation of the standard k-Fold CV technique which is designed to be effective in such cases of target imbalance.** In some cases, there may be a large imbalance in the response variables. For example, in dataset concerning price of houses, there might be large number of houses having high price. Or in case of classification, there might be several times more negative samples than positive samples. For such problems, a slight variation in the K Fold cross validation technique is made, such that each fold contains approximately the same percentage of samples of each target class as the complete set, or in case of prediction problems, the mean response value is approximately equal in all the folds. This variation is also known as Stratified K Fold.\n",
    "\n",
    "    It works as follows. Stratified k-Fold splits the dataset on k folds such that each fold contains approximately the same percentage of samples of each target class as the complete set. In the case of regression, Stratified k-Fold makes sure that the mean target value is approximately equal in all the folds. The algorithm of Stratified k-Fold technique:\n",
    "\n",
    "    - Pick a number of folds – k\n",
    "    - Split the dataset into k folds. Each fold must contain approximately the same percentage of samples of each target class as the complete set \n",
    "    - Choose k – 1 folds which will be the training set. The remaining fold will be the test set\n",
    "    - Train the model on the training set. On each iteration a new model must be trained\n",
    "    - Validate on the test set\n",
    "    - Save the result of the validation\n",
    "    - Repeat steps 3 – 6 k times. Each time use the remaining  fold as the test set. In the end, you should have validated the model on every fold that you have.\n",
    "    - To get the final score average the results that you got on step 6.\n",
    "\n",
    "    As you may have noticed, the algorithm for Stratified k-Fold technique is similar to the standard k-Folds. You don’t need to code something additionally as the method will do everything necessary for you. Stratified k-Fold also has a built-in method in sklearn – `sklearn.model_selection.StratifiedKFold`. All mentioned above about k-Fold CV is true for Stratified k-Fold technique. When choosing between different CV methods, make sure you are using the proper one. For example, you might think that your model performs badly simply because you are using k-Fold CV to validate the model which was trained on the dataset with a class imbalance. To avoid that you should always do a proper exploratory data analysis on your data.\n",
    "\n",
    "1. **Repeated k-Fold cross-validation:** Repeated k-Fold cross-validation or Repeated random sub-sampling CV is probably the most robust of all CV techniques in this paper. It is a variation of k-Fold but in the case of Repeated k-Folds k is not the number of folds. It is the number of times we will train the model. The general idea is that on every iteration we will randomly select samples all over the dataset as our test set. For example, if we decide that 20% of the dataset will be our test set, 20% of samples will be randomly selected and the rest 80% will become the training set. The algorithm of Repeated k-Fold technique:\n",
    "\n",
    "    - Pick k – number of times the model will be trained\n",
    "    - Pick a number of samples which will be the test set\n",
    "    - Split the dataset\n",
    "    - Train on the training set. On each iteration of cross-validation, a new model must be trained\n",
    "    - Validate on the test set\n",
    "    - Save the result of the validation\n",
    "    - Repeat steps 3-6 k times\n",
    "    - To get the final score average the results that you got on step 6.\n",
    "    \n",
    "    Repeated k-Fold has clear advantages over standard k-Fold CV. Firstly, the proportion of train/test split is not dependent on the number of iterations. Secondly, we can even set unique proportions for every iteration. Thirdly, random selection of samples from the dataset makes Repeated k-Fold even more robust to selection bias. Still, there are some disadvantages. k-Fold CV guarantees that the model will be tested on all samples, whereas Repeated k-Fold is based on randomization which means that some samples may never be selected to be in the test set at all. At the same time, some samples might be selected multiple times. Thus making it a bad choice for imbalanced datasets. Sklearn will help you to implement a Repeated k-Fold CV. Just use `sklearn.model_selection.RepeatedKFold`. In sklearn implementation of this technique you must set the number of folds that you want to have (n_splits) and the number of times the split will be performed (n_repeats). It guarantees that you will have different folds on each iteration.\n",
    "\n",
    "1. **Leave-one-out cross-validation:** Leave-one-out сross-validation (LOOCV) is an extreme case of k-Fold CV. Imagine if k is equal to n where n is the number of samples in the dataset. Such k-Fold case is equivalent to Leave-one-out technique. The algorithm of LOOCV technique:\n",
    "\n",
    "    - Choose one sample from the dataset which will be the test set\n",
    "    - The remaining n – 1 samples will be the training set\n",
    "    - Train the model on the training set. On each iteration, a new model must be trained\n",
    "    - Validate on the test set\n",
    "    - Save the result of the validation\n",
    "    - Repeat steps 1 – 5 n times as for n samples we have n different training and test sets\n",
    "    - To get the final score average the results that you got on step 5.\n",
    "    \n",
    "    For LOOCV sklearn also has a built-in method. It can be found in the model_selection library – sklearn.model_selection.LeaveOneOut.\n",
    "    \n",
    "    1. Advantages:\n",
    "        - The greatest advantage of Leave-one-out cross-validation is that it doesn’t waste much data. We use only one sample from the whole dataset as a test set, whereas the rest is the training set. \n",
    "        \n",
    "    1. Disadvantages:\n",
    "        - But when compared with k-Fold CV, LOOCV requires building n models instead of k models, when we know that n which stands for the number of samples in the dataset is much higher than k. It means LOOCV is more computationally expensive than k-Fold, it may take plenty of time to cross-validate the model using LOOCV. **Thus, the Data Science community has a general rule based on empirical evidence and different researches, which suggests that 5- or 10-fold cross-validation should be preferred over LOOCV.**\n",
    "\n",
    "1. **Leave-p-out cross-validation:** Leave-p-out cross-validation (LpOC) is similar to Leave-one-out CV as it creates all the possible training and test sets by using p samples as the test set. All mentioned about LOOCV is true and for LpOC. This approach leaves p data points out of training data, i.e. if there are n data points in the original sample then, n-p samples are used to train the model and p points are used as the validation set. This is repeated for all combinations in which original sample can be separated this way, and then the error is averaged for all trials, to give overall effectiveness. **This method is exhaustive in the sense that it needs to train and validate the model for all possible combinations, and for moderately large p, it can become computationally infeasible.** Still, it is worth mentioning that unlike LOOCV and k-Fold test sets will overlap for LpOC if p is higher than 1. The algorithm of LpOC technique:\n",
    "\n",
    "    - Choose p samples from the dataset which will be the test set\n",
    "    - The remaining n – p samples will be the training set\n",
    "    - Train the model on the training set. On each iteration, a new model must be trained\n",
    "    - Validate on the test set\n",
    "    - Save the result of the validation\n",
    "    - Repeat steps 2 – 5 Cpn times \n",
    "    - To get the final score average the results that you got on step 5\n",
    "    \n",
    "    You can perform Leave-p-out CV using sklearn – `sklearn.model_selection.LeavePOut`. LpOC has all the disadvantages of the LOOCV, but, nevertheless, it’s as robust as LOOCV. A particular case of this method is when p = 1. This is known as Leave one out cross validation. **This method is generally preferred over the previous one because it does not suffer from the intensive computation, as number of possible combinations is equal to number of data points in original sample or n.**\n",
    "\n",
    "1. **Nested cross-validation:** In the case of k-fold and stratified k-fold cross-validation, we get a poor estimate of the error in training and test data. Hyperparameter tuning is done separately in the earlier methods. When cross-validation is used simultaneously for tuning the hyperparameters and generalizing the error estimate, nested cross-validation is required. Nested Cross Validation can be applicable in both k-fold and stratified k-fold variants. Nested k-fold cross-validation is an advanced form of cross-validation used primarily for model selection and hyperparameter tuning, particularly when the dataset is not very large. It's a way to more reliably estimate the performance of a model on unseen data. Two Layers of k-Fold Cross-Validation:\n",
    "\n",
    "    - **Outer Loop:** This is for model evaluation. The data is split into k 'folds'. In each iteration, one fold is used as the test set, and the remaining k-1 folds are used for training (and further split in the inner loop).\n",
    "    - **Inner Loop:** Within each iteration of the outer loop, the training data is again split into k 'folds' for hyperparameter tuning. The model is trained on k-1 of these folds and validated on the remaining fold. This process is repeated for each combination of hyperparameters to find the best set.\n",
    "   \n",
    "   **Purpose:**\n",
    "\n",
    "    - **Model Selection and Hyperparameter Tuning:** The inner loop is used to select the best model and hyperparameters. The outer loop evaluates the performance of this best model.\n",
    "    - **Avoiding Data Leakage:** By having a separate test set in the outer loop that is never used for training or hyperparameter tuning, nested k-fold cross-validation avoids leaking test data into the model training proces\n",
    "\n",
    "   **When to Use Nested k-Fold Cross-Validation:**\n",
    "    - **Small Datasets:** When your dataset is not large enough to afford a separate, dedicated hold-out set for final model evaluation.\n",
    "\n",
    "    - **Reliable Performance Estimation:** When you need a more reliable and unbiased estimate of the model's performance on unseen data.\n",
    "\n",
    "    - **Hyperparameter Tuning:** It is particularly useful when the process of hyperparameter tuning is critical to the performance of the model.\n",
    "\n",
    "    - **Model Selection:** When comparing multiple models or configurations and you need a rigorous method to assess which model is likely to perform best on unseen data.\n",
    "\n",
    "    Suppose you have a dataset and want to use a support vector machine (SVM) model. You're not sure what value of the regularization parameter C to use. Nested k-fold cross-validation would allow you to test different values of C in the inner loop to find the best one, while the outer loop would give you a reliable estimate of how well the SVM with this C value performs on unseen data. Nested k-fold cross-validation is a thorough and rigorous approach to model evaluation and selection, particularly useful in scenarios where every data point is valuable, and an unbiased estimate of the model's performance is crucial. It's more computationally intensive than simple k-fold cross-validation but provides a more accurate estimate of a model's generalization capabilities.\n",
    "\n",
    "1. **Time-series cross-validation:** Traditional cross-validation techniques don’t work on sequential data such as time-series because we cannot choose random data points and assign them to either the test set or the train set as it makes no sense to use the values from the future to forecast values in the past. The above cross-validation methods may not be suitable for evaluating time series models because the order of the data is very important in time series data. That’s why time series cross-validation was introduced. There are mainly two ways to go about this:\n",
    "\n",
    "    - **Rolling cross-validation:** Cross-validation is done on a rolling basis i.e. starting with a small subset of data for training purposes, predicting the future values, and then checking the accuracy on the forecasted data points. The following image can help you get the intuition behind this approach. In time series cross-validation, folds are created in a forward-chaining fashion. We start with a small subset of data as the training fold and a much smaller subset as the validation fold. The validation fold gets shifted in time and the previous validation fold gets added to the training fold in the next iteration. We can use the Scikit-learn TimeSeriesSplit() function to perform time series cross-validation. The number of splits is specified in the n_splits hyperparameter. For the time-series dataset, the split of data into train and validation is according to the time also referred to as forward chaining method or rolling cross-validation. For a particular iteration, the next instance of train data can be treated as validation data. As mentioned in the above diagram, for the 1st iteration, 1st 3 rows are considered as train data and the next instance T4 is validation data. The chance of choice of train and validation data is forwarded for further iterations.\n",
    "    \n",
    "        <img width=\"844\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/343151b4-cbf7-4707-a628-ab312a53b8e5\">\n",
    "        <img width=\"887\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/fed37a32-056a-42cd-ab85-b81eed1e4b9c\">\n",
    "        \n",
    "    - **Blocked cross-validation:** The first technique may introduce leakage from future data to the model. The model will observe future patterns to forecast and try to memorize them. That’s why blocked cross-validation was introduced. It works by adding margins at two positions. The first is between the training and validation folds in order to prevent the model from observing lag values which are used twice, once as a regressor and another as a response. The second is between the folds used at each iteration in order to prevent the model from memorizing patterns from one iteration to the next.\n",
    "   \n",
    "        <img width=\"773\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/193a7093-dde0-4d03-a7c1-bfd66bda2b12\">\n",
    "\n",
    "\n",
    "\n",
    "**Best practices and tips**\n",
    "\n",
    "It’s worth mentioning that sometimes performing cross-validation might be a little tricky. \n",
    "\n",
    "For example, it’s quite easy to make a logical mistake when splitting the dataset which may lead to an untrustworthy CV result. \n",
    "\n",
    "You may find some tips that you need to keep in mind when cross-validating a model below:\n",
    "\n",
    "1. Be logical when splitting the data (does the splitting method make sense)\n",
    "1. Use the proper CV method (is this method viable for my use-case)\n",
    "1. When working with time series don’t validate on the past (see the first tip)\n",
    "1. When working with medical or financial data remember to split by person. Avoid having data for one person both in the training and the test set as it may be considered as data leak\n",
    "1. When cropping patches from larger images remember to split by the large image Id\n",
    "\n",
    "Of course, tips differ from task to task and it’s almost impossible to cover all of them. That’s why performing a solid exploratory data analysis before starting to cross-validate a model is always the best practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a3c3b5b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:58.879266Z",
     "iopub.status.busy": "2024-03-01T13:15:58.878123Z",
     "iopub.status.idle": "2024-03-01T13:15:58.897154Z",
     "shell.execute_reply": "2024-03-01T13:15:58.896271Z"
    },
    "papermill": {
     "duration": 0.067036,
     "end_time": "2024-03-01T13:15:58.899048",
     "exception": false,
     "start_time": "2024-03-01T13:15:58.832012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    345\n",
       "1    345\n",
       "2    344\n",
       "3    344\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# CV split\n",
    "# ====================================================\n",
    "\n",
    "n_folds = 4\n",
    "train_folds = [0, 1, 2, 3]\n",
    "\n",
    "stratified_k_fold = StratifiedKFold(\n",
    "    n_splits=n_folds,\n",
    "    shuffle=True, \n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "# Iterating Over Each Fold\n",
    "# The enumerate function is used to iterate over the fold splits. \n",
    "# It provides two pieces of information for each iteration:\n",
    "# fold: The current fold number (starting from 0).\n",
    "# (train_index, val_index): Two arrays containing indices of the training and validation data for the current fold\n",
    "for fold, (train_index, val_index) in enumerate(stratified_k_fold.split(train, train[\"label\"])): # It generates indices for training and validation sets for each fold.\n",
    "    \n",
    "    # Inside the loop, for each fold, the validation indices (val_index) are used \n",
    "    # to assign the fold number to the corresponding rows in train.\n",
    "    # This line does the assignment. It sets the 'fold' column of the DataFrame for rows in val_index to the current fold number.\n",
    "    # This effectively tags each data point with the fold number it will be a part of in the validation set.\n",
    "    # The purpose of this assignment is to keep track of which data points should be in the validation set for each fold.\n",
    "    # When you actually train the model, you can easily filter the DataFrame to get the appropriate training and validation sets based on these fold numbers.\n",
    "\n",
    "    train.loc[val_index, 'fold'] = int(fold)\n",
    "\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0854d8dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:58.988182Z",
     "iopub.status.busy": "2024-03-01T13:15:58.987846Z",
     "iopub.status.idle": "2024-03-01T13:15:59.001906Z",
     "shell.execute_reply": "2024-03-01T13:15:59.000709Z"
    },
    "papermill": {
     "duration": 0.061137,
     "end_time": "2024-03-01T13:15:59.003983",
     "exception": false,
     "start_time": "2024-03-01T13:15:58.942846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(345, 5)\n",
      "(345, 5)\n",
      "(344, 5)\n",
      "(344, 5)\n"
     ]
    }
   ],
   "source": [
    "for fold in range(n_folds):\n",
    "    \n",
    "    train_folds = train[train[\"fold\"] != fold].reset_index(drop=True)\n",
    "    valid_folds = train[train[\"fold\"] == fold].reset_index(drop=True)\n",
    "    print(valid_folds.shape)\n",
    "    valid_labels = valid_folds[\"label\"].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92f9827",
   "metadata": {
    "papermill": {
     "duration": 0.044057,
     "end_time": "2024-03-01T13:15:59.094093",
     "exception": false,
     "start_time": "2024-03-01T13:15:59.050036",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span style=\"color: #7b6b59;\">Step 4.4: Prepare the Training Data - Tokenization and DataLoader</span>\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">Tokenization with Hugging Face 🤗 Transformers</span>\n",
    "\n",
    "\n",
    "A tokenizer is in charge of preparing the inputs for a model. The Hugging Face library contains tokenizers for all the models. \n",
    "\n",
    "Before you can train a model on a dataset, it needs to be preprocessed into the expected model input format. Whether your data is text, images, or audio, they need to be converted and assembled into batches of tensors. 🤗 Transformers provides a set of preprocessing classes to help prepare your data for the model. Text, use a **Tokenizer** to convert text into a sequence of tokens, create a numerical representation of the tokens, and assemble them into tensors. The main tool for preprocessing textual data is a [tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer). \n",
    "\n",
    "1. A tokenizer splits text into tokens according to a set of rules. \n",
    "1. The tokens are converted into numbers and then tensors, which become the model inputs. \n",
    "1. Any additional inputs required by the model are added by the tokenizer.\n",
    "\n",
    "***Tip:*** If you plan on using a pretrained model, it’s important to use the associated pretrained tokenizer. This ensures the text is split the same way as the pretraining corpus, and uses the same corresponding tokens-to-index (usually referred to as the vocab) during pretraining.\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Step 1.1: Loading a pretrained tokenizer</span>\n",
    "\n",
    "\n",
    "Get started by loading a pretrained tokenizer with the AutoTokenizer.from_pretrained() method. This downloads the vocab a model was pretrained with:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3c4346c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:15:59.182728Z",
     "iopub.status.busy": "2024-03-01T13:15:59.181955Z",
     "iopub.status.idle": "2024-03-01T13:16:01.457934Z",
     "shell.execute_reply": "2024-03-01T13:16:01.456933Z"
    },
    "papermill": {
     "duration": 2.322504,
     "end_time": "2024-03-01T13:16:01.460059",
     "exception": false,
     "start_time": "2024-03-01T13:15:59.137555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6455a400a0147c79025730b7f054d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c361d03ebf94a8cad966dd933cfb5c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30951d3e06f4dec97724765e98c2ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./tokenizer/tokenizer_config.json',\n",
       " './tokenizer/special_tokens_map.json',\n",
       " './tokenizer/spm.model',\n",
       " './tokenizer/added_tokens.json',\n",
       " './tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "OUTPUT_DIR = \"./\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "tokenizer.save_pretrained(OUTPUT_DIR + \"tokenizer/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be1380",
   "metadata": {
    "papermill": {
     "duration": 0.049691,
     "end_time": "2024-03-01T13:16:01.555818",
     "exception": false,
     "start_time": "2024-03-01T13:16:01.506127",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color: #7b6b59;\">Step 1.2: Then pass your text to the tokenizer</span>\n",
    "\n",
    "\n",
    "The tokenizer returns a dictionary with three important items:\n",
    "\n",
    "1. **input_ids** are the indices corresponding to each token in the sentence.\n",
    "1. **attention_mask** indicates whether a token should be attended to or not.\n",
    "1. **token_type_ids** identifies which sequence a token belongs to when there is more than one sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a787c95e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:01.648664Z",
     "iopub.status.busy": "2024-03-01T13:16:01.647710Z",
     "iopub.status.idle": "2024-03-01T13:16:01.653207Z",
     "shell.execute_reply": "2024-03-01T13:16:01.652337Z"
    },
    "papermill": {
     "duration": 0.053099,
     "end_time": "2024-03-01T13:16:01.655084",
     "exception": false,
     "start_time": "2024-03-01T13:16:01.601985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 771, 298, 57249, 267, 262, 6303, 265, 41267, 261, 270, 306, 281, 6245, 263, 1538, 264, 5693, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(\"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\")\n",
    "print(encoded_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4726c9",
   "metadata": {
    "papermill": {
     "duration": 0.044335,
     "end_time": "2024-03-01T13:16:01.744329",
     "exception": false,
     "start_time": "2024-03-01T13:16:01.699994",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Return your input by decoding the input_ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f93d13c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:01.834989Z",
     "iopub.status.busy": "2024-03-01T13:16:01.834648Z",
     "iopub.status.idle": "2024-03-01T13:16:01.840928Z",
     "shell.execute_reply": "2024-03-01T13:16:01.840035Z"
    },
    "papermill": {
     "duration": 0.05414,
     "end_time": "2024-03-01T13:16:01.842890",
     "exception": false,
     "start_time": "2024-03-01T13:16:01.788750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger.[SEP]'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1354d86c",
   "metadata": {
    "papermill": {
     "duration": 0.044492,
     "end_time": "2024-03-01T13:16:01.933042",
     "exception": false,
     "start_time": "2024-03-01T13:16:01.888550",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As you can see, the tokenizer added two special tokens - `CLS` and `SEP` (classifier and separator) - to the sentence. Not all models need special tokens, but if they do, the tokenizer automatically adds them for you.\n",
    "\n",
    "If there are several sentences you want to preprocess, pass them as a list to the tokenizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb338611",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:02.026653Z",
     "iopub.status.busy": "2024-03-01T13:16:02.026013Z",
     "iopub.status.idle": "2024-03-01T13:16:02.032151Z",
     "shell.execute_reply": "2024-03-01T13:16:02.031166Z"
    },
    "papermill": {
     "duration": 0.055113,
     "end_time": "2024-03-01T13:16:02.034117",
     "exception": false,
     "start_time": "2024-03-01T13:16:01.979004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[1, 420, 339, 314, 567, 2962, 302, 2], [1, 1310, 280, 297, 428, 313, 2212, 314, 567, 2962, 261, 31663, 260, 2], [1, 458, 314, 11583, 268, 3933, 302, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "batch_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "]\n",
    "encoded_inputs = tokenizer(batch_sentences)\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900a49bc",
   "metadata": {
    "papermill": {
     "duration": 0.045934,
     "end_time": "2024-03-01T13:16:02.128471",
     "exception": false,
     "start_time": "2024-03-01T13:16:02.082537",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- **Pad:** Sentences aren’t always the same length which can be an issue because tensors, the model inputs, need to have a uniform shape. Padding is a strategy for ensuring tensors are rectangular by adding a special padding token to shorter sentences.\n",
    "    - Set the `padding` parameter to `True` to pad the shorter sequences in the batch to match the longest sequence.\n",
    "- **Truncation:** On the other end of the spectrum, sometimes a sequence may be too long for a model to handle. In this case, you’ll need to truncate the sequence to a shorter length.\n",
    "    - Set the `truncation` parameter to `True` to truncate a sequence to the maximum length accepted by the model.\n",
    "- **Build tensors:** Finally, you want the tokenizer to return the actual tensors that get fed to the model.\n",
    "    - Set the `return_tensors` parameter to either `pt` for PyTorch, or `tf` for TensorFlow:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb4b4c62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:02.221690Z",
     "iopub.status.busy": "2024-03-01T13:16:02.220822Z",
     "iopub.status.idle": "2024-03-01T13:16:02.229469Z",
     "shell.execute_reply": "2024-03-01T13:16:02.228357Z"
    },
    "papermill": {
     "duration": 0.057323,
     "end_time": "2024-03-01T13:16:02.231410",
     "exception": false,
     "start_time": "2024-03-01T13:16:02.174087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,   420,   339,   314,   567,  2962,   302,     2,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [    1,  1310,   280,   297,   428,   313,  2212,   314,   567,  2962,\n",
      "           261, 31663,   260,     2],\n",
      "        [    1,   458,   314, 11583,   268,  3933,   302,     2,     0,     0,\n",
      "             0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "163df734",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:02.323837Z",
     "iopub.status.busy": "2024-03-01T13:16:02.323506Z",
     "iopub.status.idle": "2024-03-01T13:16:02.329059Z",
     "shell.execute_reply": "2024-03-01T13:16:02.328343Z"
    },
    "papermill": {
     "duration": 0.054063,
     "end_time": "2024-03-01T13:16:02.330896",
     "exception": false,
     "start_time": "2024-03-01T13:16:02.276833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\"\n",
    "inputs = tokenizer.encode_plus(\n",
    "    text, \n",
    "    return_tensors=\"pt\", \n",
    "    add_special_tokens=True, \n",
    "    padding=\"max_length\",\n",
    "    max_length=512,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ff986435",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:02.423588Z",
     "iopub.status.busy": "2024-03-01T13:16:02.423217Z",
     "iopub.status.idle": "2024-03-01T13:16:02.430188Z",
     "shell.execute_reply": "2024-03-01T13:16:02.429338Z"
    },
    "papermill": {
     "duration": 0.055323,
     "end_time": "2024-03-01T13:16:02.432024",
     "exception": false,
     "start_time": "2024-03-01T13:16:02.376701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 771, 298, 57249, 267, 262, 6303, 265, 41267, 261, 270, 306, 281, 6245, 263, 1538, 264, 5693, 260, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\n",
    "    \"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\", return_tensors=None, \n",
    "    add_special_tokens=True, \n",
    "    padding=\"max_length\",\n",
    "    max_length=512,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f7024",
   "metadata": {
    "papermill": {
     "duration": 0.045763,
     "end_time": "2024-03-01T13:16:02.524491",
     "exception": false,
     "start_time": "2024-03-01T13:16:02.478728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I typically use the `tokenizer.encode_plus()` function to tokenize my input, but there is another function that can be used to tokenize input, and this `tokenizer.encode()`. The main difference between `tokenizer.encode_plus()` and `tokenizer.encode()` is that `tokenizer.encode_plus()` returns more information. Specifically, it returns the actual input ids, the attention masks, and the token type ids, and it returns all of these in a dictionary. `tokenizer.encode()` **only returns the input ids**, and it returns this either as a list or a tensor depending on the parameter, `return_tensors = “pt”`.\n",
    "\n",
    "In Hugging Face's Transformers library, the difference between `tokenizer(input)` and `tokenizer.encode_plus(...)` lies in their functionality and the level of control they offer. In summary, `tokenizer(input)` is a simpler method for basic tokenization, while `tokenizer.encode_plus(...)` provides more options and is suitable for scenarios where you need to customize the tokenization process to fit specific model requirements. When you call `tokenizer(inputs)` in Hugging Face's Transformers library, it essentially acts as a high-level wrapper that internally calls methods like `encode_plus` or similar functionalities, depending on the specific tokenizer implementation. The `encode_plus` method is one of the comprehensive methods for encoding text, handling various tasks like tokenization, conversion to token IDs, adding special tokens, creating attention masks, and managing sequence length (truncation and padding).\n",
    "\n",
    "So, in the background, when you use `tokenizer(inputs)`, it's likely invoking `encode_plus `or a functionally equivalent method, carrying out a series of steps to prepare the input text for processing by the model. The exact methods called can vary between different tokenizer classes, but they generally perform similar tasks to prepare and format the input data appropriately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b34c07d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:02.620315Z",
     "iopub.status.busy": "2024-03-01T13:16:02.619945Z",
     "iopub.status.idle": "2024-03-01T13:16:02.624587Z",
     "shell.execute_reply": "2024-03-01T13:16:02.623747Z"
    },
    "papermill": {
     "duration": 0.053883,
     "end_time": "2024-03-01T13:16:02.626505",
     "exception": false,
     "start_time": "2024-03-01T13:16:02.572622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_text = train_df['text'][:16].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "72c63b45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:02.720924Z",
     "iopub.status.busy": "2024-03-01T13:16:02.720103Z",
     "iopub.status.idle": "2024-03-01T13:16:02.750339Z",
     "shell.execute_reply": "2024-03-01T13:16:02.749418Z"
    },
    "papermill": {
     "duration": 0.079551,
     "end_time": "2024-03-01T13:16:02.752354",
     "exception": false,
     "start_time": "2024-03-01T13:16:02.672803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = tokenizer.batch_encode_plus(\n",
    "    train_text, \n",
    "    return_tensors=\"pt\", \n",
    "    add_special_tokens=True, \n",
    "    padding=\"max_length\",\n",
    "    max_length=512,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3f8b6b66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:02.846674Z",
     "iopub.status.busy": "2024-03-01T13:16:02.845992Z",
     "iopub.status.idle": "2024-03-01T13:16:02.851999Z",
     "shell.execute_reply": "2024-03-01T13:16:02.851125Z"
    },
    "papermill": {
     "duration": 0.054772,
     "end_time": "2024-03-01T13:16:02.853847",
     "exception": false,
     "start_time": "2024-03-01T13:16:02.799075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(features[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "09aa303c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:02.956119Z",
     "iopub.status.busy": "2024-03-01T13:16:02.955763Z",
     "iopub.status.idle": "2024-03-01T13:16:02.972035Z",
     "shell.execute_reply": "2024-03-01T13:16:02.970774Z"
    },
    "papermill": {
     "duration": 0.074202,
     "end_time": "2024-03-01T13:16:02.974451",
     "exception": false,
     "start_time": "2024-03-01T13:16:02.900249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>source</th>\n",
       "      <th>RDizzl3_seven</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Phones\\n\\nModern humans today are always on th...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This essay will explain if drivers should or s...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Driving while the use of cellular devices\\n\\nT...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phones &amp; Driving\\n\\nDrivers should not be able...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cell Phone Operation While Driving\\n\\nThe abil...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  Phones\\n\\nModern humans today are always on th...      0   \n",
       "1  This essay will explain if drivers should or s...      0   \n",
       "2  Driving while the use of cellular devices\\n\\nT...      0   \n",
       "3  Phones & Driving\\n\\nDrivers should not be able...      0   \n",
       "4  Cell Phone Operation While Driving\\n\\nThe abil...      0   \n",
       "\n",
       "          prompt_name           source  RDizzl3_seven  \n",
       "0  Phones and driving  persuade_corpus          False  \n",
       "1  Phones and driving  persuade_corpus          False  \n",
       "2  Phones and driving  persuade_corpus          False  \n",
       "3  Phones and driving  persuade_corpus          False  \n",
       "4  Phones and driving  persuade_corpus          False  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "54b6e12c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:03.089372Z",
     "iopub.status.busy": "2024-03-01T13:16:03.088575Z",
     "iopub.status.idle": "2024-03-01T13:16:03.095106Z",
     "shell.execute_reply": "2024-03-01T13:16:03.094239Z"
    },
    "papermill": {
     "duration": 0.060663,
     "end_time": "2024-03-01T13:16:03.097616",
     "exception": false,
     "start_time": "2024-03-01T13:16:03.036953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[\"input_ids\"].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "672cd8fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:03.200064Z",
     "iopub.status.busy": "2024-03-01T13:16:03.199072Z",
     "iopub.status.idle": "2024-03-01T13:16:03.205186Z",
     "shell.execute_reply": "2024-03-01T13:16:03.204225Z"
    },
    "papermill": {
     "duration": 0.057924,
     "end_time": "2024-03-01T13:16:03.207315",
     "exception": false,
     "start_time": "2024-03-01T13:16:03.149391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_input(text):\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        add_special_tokens=True, \n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    )\n",
    "  \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "52fe29a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:03.309155Z",
     "iopub.status.busy": "2024-03-01T13:16:03.308214Z",
     "iopub.status.idle": "2024-03-01T13:16:04.738970Z",
     "shell.execute_reply": "2024-03-01T13:16:04.738035Z"
    },
    "papermill": {
     "duration": 1.486177,
     "end_time": "2024-03-01T13:16:04.741202",
     "exception": false,
     "start_time": "2024-03-01T13:16:03.255025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./tokenizer/tokenizer_config.json',\n",
       " './tokenizer/special_tokens_map.json',\n",
       " './tokenizer/spm.model',\n",
       " './tokenizer/added_tokens.json',\n",
       " './tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# tokenizer\n",
    "# ====================================================\n",
    "OUTPUT_DIR = \"./\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "tokenizer.save_pretrained(OUTPUT_DIR + \"tokenizer/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708c409e",
   "metadata": {
    "papermill": {
     "duration": 0.069014,
     "end_time": "2024-03-01T13:16:04.860531",
     "exception": false,
     "start_time": "2024-03-01T13:16:04.791517",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### <span style=\"color: #7b6b59;\">Introduction to PyTorch Dataset and DataLoader</span>\n",
    "\n",
    "Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` that allow you to use pre-loaded datasets as well as your own data. **Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.**\n",
    "\n",
    "Your training pipeline should be as modular as possible in order to aid quick prototyping and maintaining usability. Using a poorly-written data loader / not using a data loader (using a Python generator or a function) can affect the parallelization ability of your code. \n",
    "\n",
    "Dataset processing is a highly important part of any training pipeline and should be kept separate from modeling. \n",
    "\n",
    "***How to use `Datasets` and `DataLoader` in PyTorch for custom text data***\n",
    "\n",
    "In this section, we'll go through the PyTorch data primitives, namely `torch.utils.data.DataLoader` and `torch.utils.data.Dataset`, and understand how to create our own DataLoader and Datasets by subclassing these modules. \n",
    "\n",
    "We will learn how to make a custom Dataset and manage it with DataLoader in PyTorch. Creating a PyTorch `Dataset` and managing it with `Dataloader` keeps your data manageable and helps to simplify your machine learning pipeline. **A Dataset stores all your data, and Dataloader is can be used to iterate through the data, manage batches, transform the data, and much more.**\n",
    "\n",
    "\n",
    "- **Pandas** is not essential to create a Dataset object. However, it’s a powerful tool for managing data so i’m going to use it.\n",
    "\n",
    "- **`torch.utils.data`** imports the required functions we need to create and use Dataset and DataLoader.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b2401745",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:04.990415Z",
     "iopub.status.busy": "2024-03-01T13:16:04.989878Z",
     "iopub.status.idle": "2024-03-01T13:16:04.995995Z",
     "shell.execute_reply": "2024-03-01T13:16:04.994875Z"
    },
    "papermill": {
     "duration": 0.070475,
     "end_time": "2024-03-01T13:16:04.998021",
     "exception": false,
     "start_time": "2024-03-01T13:16:04.927546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf6adf5",
   "metadata": {
    "papermill": {
     "duration": 0.057638,
     "end_time": "2024-03-01T13:16:05.104035",
     "exception": false,
     "start_time": "2024-03-01T13:16:05.046397",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color: #7b6b59;\">Implementing A Custom Dataset In PyTorch</span>\n",
    "\n",
    "A dataset is an abstract class in PyTorch that represents a collection of data. It is responsible for loading and preprocessing data from a source and returning it in the form of a PyTorch tensor.\n",
    "\n",
    "\n",
    "Now, for most purposes, you will need to write your own implementation of a `Dataset`. So let's see how you can write a custom dataset by subclassing `torch.utils.data.Dataset`.\n",
    "\n",
    "You'll need to implement 3 functions. The Dataset class provides 3 main methods:\n",
    "\n",
    "1. **`__init__`**: This function is called when instancing the object. It's typically used to store some essential locations like file paths and image transforms. `class TextDataset(Dataset)`: Create a class called ‘TextDataset’, this can be called whatever you want. Passed in to the class is the dataset module which we imported earlier. `def __init__(self, text, labels)`: When you initialise the class you need to import two variables. In this case, the variables are called ‘text’ and ‘labels’ to match the data which will be added.\n",
    "\n",
    "1. **`__len__`**: This function returns the length of the dataset. `self.labels = labels` & `self.text = text`: The imported variables can now be used in functions within the class by using self.text or self.labels. `def __len__(self)`: This function just returns the length of the labels when called. E.g., if you had a dataset with 5 labels, then the integer 5 would be returned.\n",
    "\n",
    "1. **`__getitem__`**: This is the big kahuna 🏅. This function is responsible for returning a sample from the dataset based on the index provided. returns a single data point from the dataset at a given index. The getitem method is where the actual data loading and preprocessing takes place. It takes an index as input and returns a data point, which can be a tensor or a dictionary of tensors. This method is used by the DataLoader class to load and preprocess the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d8f124",
   "metadata": {
    "papermill": {
     "duration": 0.04664,
     "end_time": "2024-03-01T13:16:05.198115",
     "exception": false,
     "start_time": "2024-03-01T13:16:05.151475",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color: #7b6b59;\">PyTorch DataLoader: A Complete Guide</span>\n",
    "\n",
    "PyTorch provides an intuitive and incredibly versatile tool, the DataLoader class, to load data in meaningful ways. Because data preparation is a critical step to any type of data work, being able to work with, and understand, DataLoaders is an important step in your deep learning journey. The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval.\n",
    "\n",
    "**DataLoader is an iterable that abstracts this complexity for us in an easy API.**\n",
    "\n",
    "The PyTorch DataLoader class is built on top of the PyTorch Dataset class, which provides a standard interface for accessing data. The DataLoader class takes in a Dataset object and provides a way to iterate over the data in batches. This allows for efficient processing of large datasets by allowing parallelization of data loading and preprocessing.\n",
    "\n",
    "\n",
    "\n",
    "**What Does a PyTorch DataLoader Do?**\n",
    "\n",
    "The PyTorch DataLoader class is an important tool to help you prepare, manage, and serve your data to your deep learning networks. Because many of the pre-processing steps you will need to do before beginning training a model, finding ways to standardize these processes is critical for the readability and maintainability of your code.\n",
    "\n",
    "The PyTorch DataLoader allows you to:\n",
    "\n",
    "- **Define a dataset to work with:** identifying where the data is coming from and how it should be accessed.\n",
    "- **Batch the data:** define how many training or testing samples to use in a single iteration. Because data are often split across training and testing sets of large sizes, being able to work with batches of data can allow your training and testing processes to be more manageable.\n",
    "- **Shuffle the data:** PyTorch can handle shuffling data for you as it loads data into batches. This can increase representativeness in your dataset and prevent accidental skewness.\n",
    "- **Support multi-processing:** PyTorch is optimized to run multiple processes at once in order to make better use of modern CPUs and GPUs and to save time in training and testing your data. The DataLoader class lets you define how many workers should go at once.\n",
    "- **Merge datasets together:** optionally, PyTorch also allows you to merge multiple datasets together. While this may not be a common task, having it available to you is an a great feature.\n",
    "- **Load data directly on CUDA tensors:** because PyTorch can run on the GPU, you can load the data directly onto the CUDA before they’re returned.\n",
    "\n",
    "The DataLoader is a PyTorch utility class that provides a way to iterate over a Dataset object in batches. It is designed to handle large datasets efficiently and can be configured to load data in parallel, preprocess data on the fly, and shuffle data for each epoch.\n",
    "\n",
    "The DataLoader takes in a Dataset object and provides a number of configuration options, including batch size, shuffling, and number of worker processes for parallel data loading. The DataLoader class is responsible for batching the data and returning it in a format that can be consumed by the model\n",
    "\n",
    "\n",
    "`DataLoader` class has a lot of different parameters available. Of course, one of the most important parameters is the actual dataset. Generally, you’ll be working with at least a training and a testing dataset. **Because of this, it’s a convention that you’ll have at least two DataLoaders, to be able to load data for both your training and testing data.**\n",
    "\n",
    "PyTorch lets you define many different parameters to influence how data are loaded. This can have a big impact on the speed at which your model can train, how well it can train, and ensuring that data are sampled appropriately.\n",
    "\n",
    "We have loaded that dataset into the DataLoader and can iterate through the dataset as needed. Each iteration below returns a batch of train_features and train_labels (containing `batch_size=8` features and labels respectively). Because we specified `shuffle=True`, after we iterate over all batches the data is shuffled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "296d0aec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:05.293130Z",
     "iopub.status.busy": "2024-03-01T13:16:05.292777Z",
     "iopub.status.idle": "2024-03-01T13:16:05.296908Z",
     "shell.execute_reply": "2024-03-01T13:16:05.296060Z"
    },
    "papermill": {
     "duration": 0.053622,
     "end_time": "2024-03-01T13:16:05.298767",
     "exception": false,
     "start_time": "2024-03-01T13:16:05.245145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_len = 512\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b0e367da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:05.399001Z",
     "iopub.status.busy": "2024-03-01T13:16:05.398052Z",
     "iopub.status.idle": "2024-03-01T13:16:05.405711Z",
     "shell.execute_reply": "2024-03-01T13:16:05.404984Z"
    },
    "papermill": {
     "duration": 0.058643,
     "end_time": "2024-03-01T13:16:05.407684",
     "exception": false,
     "start_time": "2024-03-01T13:16:05.349041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DAIGTDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.texts = df[\"text\"].values\n",
    "        self.labels = df[\"label\"].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        tokenized = self.tokenizer.encode_plus(\n",
    "            text=text,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "        return tokenized['input_ids'].squeeze(), tokenized['attention_mask'].squeeze(), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "86c64f59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:05.506796Z",
     "iopub.status.busy": "2024-03-01T13:16:05.506432Z",
     "iopub.status.idle": "2024-03-01T13:16:05.513063Z",
     "shell.execute_reply": "2024-03-01T13:16:05.512068Z"
    },
    "papermill": {
     "duration": 0.058809,
     "end_time": "2024-03-01T13:16:05.515174",
     "exception": false,
     "start_time": "2024-03-01T13:16:05.456365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_dataset = DAIGTDataset(train[0:20], tokenizer, max_len)\n",
    "sample_loader = DataLoader(\n",
    "    dataset=sample_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "94ea37ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:05.616558Z",
     "iopub.status.busy": "2024-03-01T13:16:05.615782Z",
     "iopub.status.idle": "2024-03-01T13:16:05.622032Z",
     "shell.execute_reply": "2024-03-01T13:16:05.621079Z"
    },
    "papermill": {
     "duration": 0.05877,
     "end_time": "2024-03-01T13:16:05.624029",
     "exception": false,
     "start_time": "2024-03-01T13:16:05.565259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate(input_ids, attention_mask):\n",
    "    mask_len = attention_mask.sum(axis=1).max()\n",
    "    return input_ids[:,:mask_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "917a4dd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:05.724923Z",
     "iopub.status.busy": "2024-03-01T13:16:05.724149Z",
     "iopub.status.idle": "2024-03-01T13:16:06.192930Z",
     "shell.execute_reply": "2024-03-01T13:16:06.191537Z"
    },
    "papermill": {
     "duration": 0.521432,
     "end_time": "2024-03-01T13:16:06.195249",
     "exception": false,
     "start_time": "2024-03-01T13:16:05.673817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512])\n",
      "tensor(512)\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "tensor(512)\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "tensor(512)\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "tensor(512)\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "tensor(512)\n",
      "torch.Size([4, 512])\n"
     ]
    }
   ],
   "source": [
    "for step, (input_ids, attention_mask, labels) in enumerate(sample_loader):\n",
    "    print(attention_mask.shape)\n",
    "    print(attention_mask.sum(axis=1).max())\n",
    "    inputs_v2 = collate(input_ids, attention_mask)\n",
    "    print(inputs_v2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ca52f8a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:06.294551Z",
     "iopub.status.busy": "2024-03-01T13:16:06.293789Z",
     "iopub.status.idle": "2024-03-01T13:16:06.300779Z",
     "shell.execute_reply": "2024-03-01T13:16:06.299817Z"
    },
    "papermill": {
     "duration": 0.057039,
     "end_time": "2024-03-01T13:16:06.302771",
     "exception": false,
     "start_time": "2024-03-01T13:16:06.245732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 771, 298, 57249, 267, 262, 6303, 265, 41267, 261, 270, 306, 281, 6245, 263, 1538, 264, 5693, 260, 273, 280, 358, 110355, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(\"Do not meddle in the affairs of wizards, for they are subtle and quick to anger. I'm happ\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b50bb8",
   "metadata": {
    "papermill": {
     "duration": 0.046486,
     "end_time": "2024-03-01T13:16:06.396413",
     "exception": false,
     "start_time": "2024-03-01T13:16:06.349927",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span style=\"color: #7b6b59;\">Step 4.5: Modelling with Hugging Face 🤗 Transformers</span>\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Introduction</span>\n",
    "\n",
    "\n",
    "In Hugging Face Transformers there are 2 main outputs and 3 if configured; that we receive after giving input_ids and attention_mask as input.\n",
    "\n",
    "- **pooler output (batch size, hidden size)**: Last layer hidden-state of the first token of the sequence\n",
    "- **last hidden state (batch size, seq Len, hidden size)**: which is the sequence of hidden states at the output of the last layer.\n",
    "- **hidden states (n layers, batch size, seq Len, hidden size)**: Hidden states for all layers and for all ids.\n",
    "\n",
    "In this notebook, we will show many different ways these outputs and hidden representations can be utilized to do much more than just adding an output layer. Below are the various techniques we will be implementing.\n",
    "\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Last Hidden State Output</span>\n",
    "\n",
    "\n",
    "<img width=\"894\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/923fb8e9-58b0-4984-a4e3-d097afde3b88\">\n",
    "\n",
    "**This is the first and default output from models.**\n",
    "\n",
    "Last Hidden State output is the sequence of hidden-states at the output of the last layer of the model. The output is usually `[batch, maxlen, hidden_state]`, it can be narrowed down to `[batch, 1, hidden_state]` for `[CLS]` token, as the `[CLS]` token is 1st token in the sequence. Here , `[batch, 1, hidden_state]` can be equivalently considered as `[batch, hidden_state]`.\n",
    "\n",
    "#### <span style=\"color: #7b6b59;\">Implementation Details</span>\n",
    "\n",
    "\n",
    "All models have outputs that are instances of subclasses of [`ModelOutput`](https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/output#transformers.utils.ModelOutput). Those are data structures containing all the information returned by the model, but that can also be used as tuples or dictionaries.\n",
    "\n",
    "```python\n",
    "\n",
    "outputs = model(**inputs, labels=labels)\n",
    "\n",
    "```\n",
    "When considering our outputs object as tuple, it only considers the attributes that don’t have None values. For instance, it has two elements, loss then logits, so will return the `tuple (outputs.loss, outputs.logits)`. `outputs[:2]`\n",
    "\n",
    "\n",
    "In Hugging Face Transformers, when you use a model from the `AutoModel` class with `AutoModel.from_pretrained`, the specific subclass of `ModelOutput` that the model returns depends on the type of model you are using (e.g., BERT, GPT-2, T5, etc.) and the nature of the task (e.g., sequence classification, token classification, language modeling, etc.).\n",
    "\n",
    "To determine which subclass of `ModelOutput` is returned, you should consider the following:\n",
    "\n",
    "1. **Model Type:** Different models are designed for different kinds of tasks. For instance, BERT-like models might return `BaseModelOutput` or `SequenceClassifierOutput`, while GPT-like models might return `CausalLMOutput`.\n",
    "\n",
    "1. **Task:** The nature of the task also influences the output type. For example:\n",
    "\n",
    "    - For sequence classification tasks, models often return SequenceClassifierOutput.\n",
    "    - For token classification tasks (like Named Entity Recognition), models might return TokenClassifierOutput.\n",
    "    - For language modeling tasks, models could return CausalLMOutput or MaskedLMOutput.\n",
    "\n",
    "1. **Documentation::** The best way to know for sure is to refer to the Hugging Face documentation for the specific model you are using. The documentation usually specifies the output format for each model.\n",
    "\n",
    "1. **Inspecting the Output:** You can programmatically inspect the output to determine its type. For example, after running `outputs = self.model(**inputs)`, you can check `type(outputs)` to see the class of the output.\n",
    "\n",
    "1. **Common Attributes:** Most ModelOutput subclasses have common attributes like `loss`, `logits`, `hidden_states`, and `attentions`, but the presence and relevance of these attributes can vary. The exact composition of the output object will align with the requirements of the model's intended task.\n",
    "\n",
    "1. **Configuration:** Sometimes, the configuration of the model (self.config) can give you hints about the expected output type, especially if it contains task-specific configurations.\n",
    "\n",
    "Remember that Hugging Face's design philosophy with `ModelOutput` is to provide flexibility and convenience, allowing outputs to be used like tuples, dictionaries, or objects with named attributes. This makes it easier to access the information you need for your specific application.\n",
    "\n",
    "For instance, if we have a look on the [documentation for the Deberta Model](https://huggingface.co/docs/transformers/model_doc/deberta#transformers.DebertaModel.forward) in the `forward` method  \n",
    "we will see that \"**Returns `transformers.modeling_outputs.BaseModelOutput` or `tuple(torch.FloatTensor)`**\". Now if we jump to the [BaseModelOutput documentation](https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput) we'll get \n",
    "\n",
    "<img width=\"1044\" alt=\"image\" src=\"https://github.com/microsoft/DeBERTa/assets/28102493/dbe927a5-4f60-4673-bf28-29a8a96e05aa\">\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Mean Pooling</span>\n",
    "\n",
    "#### <span style=\"color: #7b6b59;\">Introduction</span>\n",
    "\n",
    "Since Transformers are contextual model, the idea is `[CLS]` token would have captured the entire context and would be sufficient for simple downstream tasks such as classification. Hence, for tasks such as classification using sentence representations, you can use `[batch, hidden_state]`.\n",
    "\n",
    "We can also consider the last hidden state `[batch, maxlen, hidden_state]`, the average across maxlen dimensions to get averaged/mean embeddings.\n",
    "\n",
    "There are multiple different ways to do this. We can simply take `torch.mean(last_hidden_state, 1)` but rather we will be implementing something different. We will make use of attention masks as well so that we can ignore padding tokens which is a better way of implementing average embeddings.\n",
    "\n",
    "***What is pooling in Transformer models?***\n",
    "\n",
    "In the context of transformers, pooling refers to the process of summarizing the outputs of the transformer layers into a fixed-size vector, often used for downstream tasks such as classification.\n",
    "\n",
    "In a transformer architecture, the input sequence is processed by a series of self-attention and feedforward layers. Each layer produces a sequence of output vectors, which encode the input sequence in a higher-level representation. Pooling involves taking the output vectors from one or more of these layers and aggregating them into a single vector.\n",
    "\n",
    "There are different types of pooling mechanisms used in transformer architectures, including:\n",
    "\n",
    "\n",
    "1. **Max Pooling:** where the maximum value across the sequence of output vectors is selected as the summary representation.\n",
    "\n",
    "1. **Mean Pooling:** where the average of the output vectors is taken as the summary representation.\n",
    "\n",
    "1. **Last Hidden State:** where the final output vector of the transformer is used as the summary representation.\n",
    "\n",
    "1. **Self-Attention Pooling:** where a weighted sum of the output vectors is computed, with the weights determined by a learned attention mechanism.\n",
    "\n",
    "#### <span style=\"color: #7b6b59;\">Neural Networks: Pooling Layers</span>\n",
    "\n",
    "In this section, we’ll walk through **pooling**, a machine-learning technique ***widely used that reduces the size of the input and, thus the complexity of deep learning models while preserving important features and relationships in the input data***. In particular, we’ll introduce pooling, explain its usage, highlight its importance, and give brief examples of how it works.\n",
    "\n",
    "***What Are Pooling Layers?***\n",
    "\n",
    "In machine learning and neural networks, the dimensions of the input data and the parameters of the neural network play a crucial role. So this number can be controlled by the stacking of one or more pooling layers. Depending on the type of the pooling layer, an operation is performed on each channel of the input data independently to summarize its values into a single one and thus keep the most important features. These values are driven as input to the next layer of the model and so on. The pooling process may be repeated several times, and each iteration reduces the spatial dimensions. The value aggregation can be performed by using different techniques.\n",
    "\n",
    "***Types of Pooling Layers***\n",
    "\n",
    "There are many pooling operations and different extensions that have been developed to address specific challenges in different applications.\n",
    "\n",
    "\n",
    "1. **Max Pooling:** Max pooling is a convolution technique that chooses the maximum value from the patch of the input data and summarizes these values into a feature map: This method maintains the most significant features of the input by reducing its dimensions.\n",
    "    <img width=\"660\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/4e844e1d-8d59-4f97-beb8-00845cc7e45a\">\n",
    "    \n",
    "1. **Average Pooling:** Average pooling calculates the average value from a patch of input data and summarizes these values into a feature map: This method is preferable in cases in which smoothing the input data is necessary as it helps to identify the presence of outliers. \n",
    "     <img width=\"625\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/af4b21e0-10f8-4518-9a5c-dbdf960f48fa\">\n",
    "\n",
    "1. **Global Pooling:** Global pooling summarizes the values of all neurons for each patch of the input data into a feature map, regardless of their spatial location. This technique is also used to reduce the dimensionality of the input and can be performed either by using the maximum or average pooling operation. \n",
    "\n",
    "1. **Stochastic Pooling:** Stochastic pooling is a deterministic pooling operation that introduces randomness into the max pooling process. This technique helps in improving the robustness of the model to small variations in the input data.\n",
    "\n",
    "***Advantages and Disadvantages***\n",
    "\n",
    "In machine learning, pooling layers offer several advantages and disadvantages as well.\n",
    "\n",
    "First of all, pooling layers help in keeping the most important characteristics of the input data. Furthermore, the addition of pooling layers in the neural network offers translation invariance, which means that the model can generate the same outputs regardless of small changes in the input. Moreover, these techniques help in reducing the impact of outliers.\n",
    "\n",
    "On the other hand, the pooling processes may lead to information loss, increased training complexity, and limited model interpretability.\n",
    "\n",
    "***Usages of Pooling Layers in Machine Learning***\n",
    "\n",
    "Pooling layers play a critical role in the size and complexity of the model and are widely used in several machine-learning tasks. They are usually employed after the convolutional layers in the convolutional neural network’s structure and are mainly used for downsampling the output.\n",
    "\n",
    "These techniques are commonly used in convolutional neural networks and deep learning models of computer vision, speech recognition, and natural language processing.\n",
    "\n",
    "\n",
    "***In conclusion, pooling layers play a critical role in reducing the size and complexity of deep learning models while preserving important features and relationships in the input data.***\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9df59602",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:06.498417Z",
     "iopub.status.busy": "2024-03-01T13:16:06.497655Z",
     "iopub.status.idle": "2024-03-01T13:16:06.502479Z",
     "shell.execute_reply": "2024-03-01T13:16:06.501519Z"
    },
    "papermill": {
     "duration": 0.060673,
     "end_time": "2024-03-01T13:16:06.504525",
     "exception": false,
     "start_time": "2024-03-01T13:16:06.443852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # or \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ee206cd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:06.605606Z",
     "iopub.status.busy": "2024-03-01T13:16:06.604847Z",
     "iopub.status.idle": "2024-03-01T13:16:08.996870Z",
     "shell.execute_reply": "2024-03-01T13:16:08.995986Z"
    },
    "papermill": {
     "duration": 2.444264,
     "end_time": "2024-03-01T13:16:08.999353",
     "exception": false,
     "start_time": "2024-03-01T13:16:06.555089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b9f8df07b74fccbcbb70bb648a782e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"microsoft/deberta-v3-base\", output_hidden_states=True)\n",
    "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-base\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5425e482",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:09.103475Z",
     "iopub.status.busy": "2024-03-01T13:16:09.103045Z",
     "iopub.status.idle": "2024-03-01T13:16:15.663535Z",
     "shell.execute_reply": "2024-03-01T13:16:15.662218Z"
    },
    "papermill": {
     "duration": 6.61457,
     "end_time": "2024-03-01T13:16:15.666097",
     "exception": false,
     "start_time": "2024-03-01T13:16:09.051527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for step, (input_ids, attention_mask, labels) in enumerate(sample_loader):\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cb39acf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:15.766213Z",
     "iopub.status.busy": "2024-03-01T13:16:15.765838Z",
     "iopub.status.idle": "2024-03-01T13:16:15.772102Z",
     "shell.execute_reply": "2024-03-01T13:16:15.771330Z"
    },
    "papermill": {
     "duration": 0.057731,
     "end_time": "2024-03-01T13:16:15.774089",
     "exception": false,
     "start_time": "2024-03-01T13:16:15.716358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutput"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6500f95d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:15.875839Z",
     "iopub.status.busy": "2024-03-01T13:16:15.875335Z",
     "iopub.status.idle": "2024-03-01T13:16:15.885954Z",
     "shell.execute_reply": "2024-03-01T13:16:15.885069Z"
    },
    "papermill": {
     "duration": 0.065269,
     "end_time": "2024-03-01T13:16:15.887894",
     "exception": false,
     "start_time": "2024-03-01T13:16:15.822625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0244e-02,  6.7374e-02, -2.3058e-02,  ..., -7.2960e-02,\n",
       "           6.6919e-02,  3.3520e-02],\n",
       "         [ 6.1867e-01, -2.4755e-01,  1.2476e-01,  ...,  1.2333e+00,\n",
       "          -1.8161e-01, -9.5691e-02],\n",
       "         [ 2.7716e-01,  3.4418e-01, -3.9991e-01,  ...,  6.6889e-01,\n",
       "          -7.3343e-03,  4.5186e-01],\n",
       "         ...,\n",
       "         [-2.8558e-01,  5.3847e-01,  5.7045e-02,  ..., -9.0902e-01,\n",
       "           3.6311e-01,  2.4040e-01],\n",
       "         [-8.3092e-01,  6.3363e-01,  2.4380e-01,  ...,  6.6695e-01,\n",
       "          -2.1120e-01,  1.0565e+00],\n",
       "         [ 2.6948e-02,  6.5354e-02, -8.3646e-04,  ..., -7.3275e-02,\n",
       "           7.2130e-02,  2.9461e-02]],\n",
       "\n",
       "        [[ 2.3808e-02,  6.5923e-02,  1.0164e-02,  ..., -6.0424e-02,\n",
       "           6.2456e-02,  2.7879e-02],\n",
       "         [ 2.8578e-01,  4.9128e-01,  9.5755e-02,  ..., -1.1389e-01,\n",
       "           1.4006e-01, -3.4079e-01],\n",
       "         [ 1.8625e-01, -7.0034e-01,  1.8400e-02,  ..., -1.1186e+00,\n",
       "           4.3184e-01,  5.4246e-02],\n",
       "         ...,\n",
       "         [ 1.7070e-01, -2.0535e-01,  2.2603e-01,  ..., -6.2682e-02,\n",
       "          -9.4599e-01, -2.5370e-01],\n",
       "         [ 6.0976e-02,  4.8790e-01,  2.8918e-01,  ..., -4.1985e-02,\n",
       "          -3.0933e-01,  8.7965e-02],\n",
       "         [ 3.2609e-02,  6.9364e-02,  2.6946e-02,  ..., -7.2223e-02,\n",
       "           6.6579e-02,  2.3047e-02]],\n",
       "\n",
       "        [[-2.5951e-02,  6.5802e-02,  3.1365e-02,  ..., -5.2777e-02,\n",
       "           6.3712e-02,  1.6016e-02],\n",
       "         [ 1.3156e-01,  7.9103e-01,  8.0079e-02,  ..., -5.4470e-01,\n",
       "          -4.8703e-01,  2.6877e-01],\n",
       "         [-3.7666e-01,  8.7165e-01, -1.9666e-01,  ..., -6.9959e-01,\n",
       "          -5.8610e-01,  9.4435e-01],\n",
       "         ...,\n",
       "         [-1.5172e-01,  1.9281e-01, -1.7749e+00,  ...,  1.4924e-01,\n",
       "          -2.3734e-01,  1.1913e+00],\n",
       "         [-1.5172e-01,  1.9281e-01, -1.7749e+00,  ...,  1.4924e-01,\n",
       "          -2.3734e-01,  1.1913e+00],\n",
       "         [-1.5172e-01,  1.9281e-01, -1.7749e+00,  ...,  1.4924e-01,\n",
       "          -2.3734e-01,  1.1913e+00]],\n",
       "\n",
       "        [[ 2.4869e-02,  7.2912e-02, -4.8464e-02,  ..., -4.6648e-02,\n",
       "           5.6330e-02,  6.4202e-02],\n",
       "         [ 7.9655e-01, -1.7365e-02,  4.3846e-02,  ...,  2.5031e-01,\n",
       "          -5.2120e-01,  2.7548e-01],\n",
       "         [ 5.5798e-02,  8.8427e-01,  1.0985e+00,  ..., -1.2937e+00,\n",
       "          -5.5454e-01,  2.4294e-01],\n",
       "         ...,\n",
       "         [ 1.7731e-01, -4.3949e-01, -2.8049e-02,  ..., -2.0617e+00,\n",
       "          -2.0684e-01, -7.0143e-01],\n",
       "         [-9.1070e-02,  5.9005e-01,  5.6786e-01,  ..., -3.8289e-01,\n",
       "          -9.9353e-01,  1.6355e-01],\n",
       "         [ 3.3759e-02,  6.8299e-02, -3.5936e-02,  ..., -5.1224e-02,\n",
       "           5.1328e-02,  6.3355e-02]]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]\n",
    "# OR outputs[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "76f5ed15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:15.986759Z",
     "iopub.status.busy": "2024-03-01T13:16:15.986045Z",
     "iopub.status.idle": "2024-03-01T13:16:15.992662Z",
     "shell.execute_reply": "2024-03-01T13:16:15.991628Z"
    },
    "papermill": {
     "duration": 0.059738,
     "end_time": "2024-03-01T13:16:15.996228",
     "exception": false,
     "start_time": "2024-03-01T13:16:15.936490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 768])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state = outputs[0]\n",
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6014f0",
   "metadata": {
    "papermill": {
     "duration": 0.04996,
     "end_time": "2024-03-01T13:16:16.109604",
     "exception": false,
     "start_time": "2024-03-01T13:16:16.059644",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- **Step 1 - Attention Mask Expansion:** Expand Attention Mask from `[batch_size, max_len]` to `[batch_size, max_len, hidden_size]`. `attention_mask` is used to identify the actual tokens and padding tokens in the input sequences. It has 1s for real tokens and 0s for padding.\n",
    "    - `.unsqueeze(-1)` adds an extra dimension at the end of the `attention_mask`, making it compatible for element-wise multiplication with `last_hidden_state`. `attention_mask` is a 2D tensor where each row corresponds to a sequence in the batch, and each element in a row is either 0 (for padding tokens) or 1 (for actual tokens). For example, if you have a batch size of 2 and sequence length of 4, it might look like this:\n",
    "    ```python\n",
    "\n",
    "        [\n",
    "            [1, 1, 0, 0], # First sequence with 2 actual tokens and 2 paddings\n",
    "            [1, 1, 1, 0]  # Second sequence with 3 actual tokens and 1 padding\n",
    "        ]  \n",
    "    ```\n",
    "    - Unsqueezing operation adds an extra dimension at the end, making it a 3D tensor. For the example above, after unsqueezing, it would look like:\n",
    "    ```python\n",
    "        [\n",
    "            [\n",
    "                [1], [1], [0], [0] # First sequence with an added dimension\n",
    "            ],   \n",
    "            [\n",
    "                [1], [1], [1], [0] # Second sequence with an added dimension\n",
    "            ]\n",
    "        ]\n",
    "    ```\n",
    "    - Expansion (`expand(last_hidden_state.size())`): `.expand(last_hidden_state.size())` adjusts the size of the mask to match the dimensions of `last_hidden_state`, ensuring that each embedding vector in the sequence has a corresponding mask value. The `last_hidden_state tensor` has a shape similar to `[batch_size, seq_length, hidden_size]` where `hidden_size` is the size of the embedding vectors. The expansion makes the attention mask match this shape by repeating its values across the new dimension. After expansion, using the example above and assuming hidden_size is 3, the expanded mask would conceptually look like:\n",
    "    ```python\n",
    "        [\n",
    "            [              # First sequence\n",
    "                [1, 1, 1], # First token (actual token)\n",
    "                [1, 1, 1], # Second token (actual token)\n",
    "                [0, 0, 0], # Third token (padding)\n",
    "                [0, 0, 0]  # Fourth token (padding)\n",
    "            ],\n",
    "            [              # Second sequence\n",
    "                [1, 1, 1], # First token (actual token)\n",
    "                [1, 1, 1], # Second token (actual token)\n",
    "                [1, 1, 1], # Third token (actual token)\n",
    "                [0, 0, 0]  # Fourth token (padding)\n",
    "            ]           \n",
    "        ]\n",
    "    ```\n",
    "    - In this tensor, each `[1, 1, 1]` or `[0, 0, 0]` corresponds to a token in the sequence, replicated across the hidden_size dimension. This expanded attention mask can now be element-wise multiplied with the last_hidden_state tensor, effectively zeroing out the embeddings of padding tokens and leaving the embeddings of actual tokens unchanged. This is a crucial step before summing the embeddings for mean pooling, as it ensures that only meaningful token embeddings are considered.\n",
    "    - `.float()` converts the mask to float type for subsequent arithmetic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4566fa41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:16.207318Z",
     "iopub.status.busy": "2024-03-01T13:16:16.206613Z",
     "iopub.status.idle": "2024-03-01T13:16:16.215511Z",
     "shell.execute_reply": "2024-03-01T13:16:16.214345Z"
    },
    "papermill": {
     "duration": 0.059898,
     "end_time": "2024-03-01T13:16:16.217459",
     "exception": false,
     "start_time": "2024-03-01T13:16:16.157561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512, 1])\n",
      "tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1]]])\n"
     ]
    }
   ],
   "source": [
    "print(attention_mask.shape)\n",
    "print(attention_mask.unsqueeze(-1).shape)\n",
    "#print(attention_mask)\n",
    "#print(attention_mask.unsqueeze(-1))\n",
    "print(attention_mask.unsqueeze(-1).expand(last_hidden_state.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3cae8bae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:16.315022Z",
     "iopub.status.busy": "2024-03-01T13:16:16.314716Z",
     "iopub.status.idle": "2024-03-01T13:16:16.321575Z",
     "shell.execute_reply": "2024-03-01T13:16:16.320785Z"
    },
    "papermill": {
     "duration": 0.057845,
     "end_time": "2024-03-01T13:16:16.323496",
     "exception": false,
     "start_time": "2024-03-01T13:16:16.265651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2dbe70",
   "metadata": {
    "papermill": {
     "duration": 0.046881,
     "end_time": "2024-03-01T13:16:16.417402",
     "exception": false,
     "start_time": "2024-03-01T13:16:16.370521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- **Step 2 - Embeddings Summation:** Sum Embeddings along max_len axis so now we have `[batch_size, hidden_size]`.\n",
    "    - The code multiplies `last_hidden_state` with `input_mask_expanded` to zero out embeddings corresponding to padding tokens.\n",
    "    - `torch.sum(..., 1)` sums up the embeddings across the sequence length dimension (tokens), resulting in a single vector for each sequence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "63a64dc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:16.512616Z",
     "iopub.status.busy": "2024-03-01T13:16:16.512275Z",
     "iopub.status.idle": "2024-03-01T13:16:16.518112Z",
     "shell.execute_reply": "2024-03-01T13:16:16.517405Z"
    },
    "papermill": {
     "duration": 0.055766,
     "end_time": "2024-03-01T13:16:16.520095",
     "exception": false,
     "start_time": "2024-03-01T13:16:16.464329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "72925f7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:16.616883Z",
     "iopub.status.busy": "2024-03-01T13:16:16.616601Z",
     "iopub.status.idle": "2024-03-01T13:16:16.622857Z",
     "shell.execute_reply": "2024-03-01T13:16:16.621800Z"
    },
    "papermill": {
     "duration": 0.057446,
     "end_time": "2024-03-01T13:16:16.624702",
     "exception": false,
     "start_time": "2024-03-01T13:16:16.567256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512, 768])\n",
      "torch.Size([4, 512, 768])\n",
      "torch.Size([4, 512, 768])\n",
      "torch.Size([4, 768])\n"
     ]
    }
   ],
   "source": [
    "print(last_hidden_state.shape)\n",
    "print(input_mask_expanded.shape)\n",
    "element_wise = last_hidden_state * input_mask_expanded\n",
    "print(element_wise.shape)\n",
    "print(sum_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbff48e6",
   "metadata": {
    "papermill": {
     "duration": 0.047573,
     "end_time": "2024-03-01T13:16:16.719184",
     "exception": false,
     "start_time": "2024-03-01T13:16:16.671611",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- **Step 3 - Summation of Mask Values:** Sum Mask along `max_len axis`. This is done so that we can ignore padding tokens.\n",
    "    - Summing up the `input_mask_expanded `across the sequence length gives the number of actual tokens (not padding) in each sequence. In this step, the code is summing up the values of the expanded attention mask (`input_mask_expanded`) across the sequence length dimension. The purpose of this operation is to count the number of actual tokens (ignoring padding tokens) in each sequence of the batch. This count is important for the mean pooling operation, as it tells us by what number we should divide the sum of embeddings to get the average embedding per sequence. `input_mask_expanded.sum(1)` computes the sum along dimension 1, which is the sequence **length dimension**. This effectively adds up the mask values for each token in a sequence, giving us a total count of actual tokens per sequence. Since `input_mask_expanded` was expanded to match the `last_hidden_state` size and had 1s for actual tokens and 0s for padding, summing it up over the sequence length dimension will count the number of 1s, i.e., the number of actual tokens in each sequence.\n",
    "    - Before the summation, `input_mask_expanded` has the same dimensions as `last_hidden_state`, which is `[batch_size, seq_length, hidden_size]`. After summing across the sequence length (dimension 1), the size of `sum_mask` is reduced to `[batch_size, hidden_size]`. However, since all values across `hidden_size` are the same (because the mask was expanded by repeating the same values), this is effectively equivalent to having a tensor of shape `[batch_size, 1]` where each value represents the count of actual tokens in each sequence of the batch.\n",
    "    - **Clamping:** After the summation, the code ensures that sum_mask does not contain zeros, to avoid division by zero in the mean calculation. `torch.clamp(..., min=1e-9)` ensures that `sum_mask` does not have any zero values, **preventing division by zero in the next step.** The minimum value `(1e-9)` is arbitrary but small enough to not significantly affect the mean. `torch.clamp(..., min=1e-9`) sets a lower bound on the values in `sum_mask`. If any value is less than `1e-9`, it is set to `1e-9`. This is a safety measure to avoid division by zero. In practice, since `sum_mask` counts the number of tokens, it should not have any zero values unless a sequence is entirely made of padding tokens, which is unlikely in well-formed input data.\n",
    "    - So, the `sum_mask` tensor provides a count of actual tokens for each sequence, and its dimensions after summation are effectively `[batch_size, 1]`, representing the token count for each sequence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b661dc49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:16.817135Z",
     "iopub.status.busy": "2024-03-01T13:16:16.816565Z",
     "iopub.status.idle": "2024-03-01T13:16:16.821534Z",
     "shell.execute_reply": "2024-03-01T13:16:16.820734Z"
    },
    "papermill": {
     "duration": 0.055825,
     "end_time": "2024-03-01T13:16:16.823421",
     "exception": false,
     "start_time": "2024-03-01T13:16:16.767596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sum_mask = input_mask_expanded.sum(1)\n",
    "sum_mask = torch.clamp(sum_mask, min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0081349c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:16.921557Z",
     "iopub.status.busy": "2024-03-01T13:16:16.920892Z",
     "iopub.status.idle": "2024-03-01T13:16:16.930529Z",
     "shell.execute_reply": "2024-03-01T13:16:16.929488Z"
    },
    "papermill": {
     "duration": 0.060908,
     "end_time": "2024-03-01T13:16:16.932487",
     "exception": false,
     "start_time": "2024-03-01T13:16:16.871579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512, 768])\n",
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n",
      "torch.Size([4, 768])\n",
      "tensor([[512., 512., 512.,  ..., 512., 512., 512.],\n",
      "        [512., 512., 512.,  ..., 512., 512., 512.],\n",
      "        [487., 487., 487.,  ..., 487., 487., 487.],\n",
      "        [512., 512., 512.,  ..., 512., 512., 512.]])\n"
     ]
    }
   ],
   "source": [
    "print(input_mask_expanded.shape)\n",
    "print(input_mask_expanded)\n",
    "print(sum_mask.shape)\n",
    "print(sum_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758c9be0",
   "metadata": {
    "papermill": {
     "duration": 0.047366,
     "end_time": "2024-03-01T13:16:17.027899",
     "exception": false,
     "start_time": "2024-03-01T13:16:16.980533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- **Step 4- Computing Mean Embeddings:** Take Average. Dividing `sum_embeddings` by `sum_mask` gives the mean embeddings. Since `sum_embeddings` contains the sum of embeddings for actual tokens only, and `sum_mask` contains the count of actual tokens, the division computes the mean of the embeddings across each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "95623f52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:17.126489Z",
     "iopub.status.busy": "2024-03-01T13:16:17.125686Z",
     "iopub.status.idle": "2024-03-01T13:16:17.130091Z",
     "shell.execute_reply": "2024-03-01T13:16:17.129248Z"
    },
    "papermill": {
     "duration": 0.055576,
     "end_time": "2024-03-01T13:16:17.131934",
     "exception": false,
     "start_time": "2024-03-01T13:16:17.076358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_embeddings = sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5f557f32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:17.228255Z",
     "iopub.status.busy": "2024-03-01T13:16:17.227901Z",
     "iopub.status.idle": "2024-03-01T13:16:17.232973Z",
     "shell.execute_reply": "2024-03-01T13:16:17.232043Z"
    },
    "papermill": {
     "duration": 0.055413,
     "end_time": "2024-03-01T13:16:17.234860",
     "exception": false,
     "start_time": "2024-03-01T13:16:17.179447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 768])\n",
      "torch.Size([4, 768])\n",
      "torch.Size([4, 768])\n"
     ]
    }
   ],
   "source": [
    "print(sum_embeddings.shape)\n",
    "print(sum_mask.shape)\n",
    "print(mean_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bd67395d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:17.331594Z",
     "iopub.status.busy": "2024-03-01T13:16:17.330940Z",
     "iopub.status.idle": "2024-03-01T13:16:17.337429Z",
     "shell.execute_reply": "2024-03-01T13:16:17.336574Z"
    },
    "papermill": {
     "duration": 0.056782,
     "end_time": "2024-03-01T13:16:17.339399",
     "exception": false,
     "start_time": "2024-03-01T13:16:17.282617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model=\"microsoft/deberta-v3-base\"\n",
    "    gradient_checkpointing=True\n",
    "    epochs=2\n",
    "    batch_size=8\n",
    "    max_len=512\n",
    "    apex=True\n",
    "    scheduler='cosine' # ['linear', 'cosine']\n",
    "    gradient_accumulation_steps=1\n",
    "    num_warmup_steps=0\n",
    "    num_cycles=0.5\n",
    "    encoder_lr=2e-5\n",
    "    decoder_lr=2e-5\n",
    "    weight_decay=0.01\n",
    "    n_folds=4\n",
    "    train_folds=[0, 1, 2, 3]\n",
    "    batch_scheduler=True\n",
    "    print_freq=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f6c5e625",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:17.437268Z",
     "iopub.status.busy": "2024-03-01T13:16:17.436637Z",
     "iopub.status.idle": "2024-03-01T13:16:17.447683Z",
     "shell.execute_reply": "2024-03-01T13:16:17.446810Z"
    },
    "papermill": {
     "duration": 0.062184,
     "end_time": "2024-03-01T13:16:17.449495",
     "exception": false,
     "start_time": "2024-03-01T13:16:17.387311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "    \n",
    "class DAIGTModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "        if self.cfg.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        self.pool = MeanPooling()\n",
    "        self.classifier = nn.Linear(self.config.hidden_size, 1)\n",
    "        \n",
    "    def feature(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids, attention_mask)\n",
    "        last_hidden_states = outputs[0]\n",
    "        feature = self.pool(last_hidden_states, attention_mask)\n",
    "        return feature\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        embeddings = self.feature(input_ids, attention_mask)\n",
    "        logits = self.classifier(embeddings)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9a1daffd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:17.546827Z",
     "iopub.status.busy": "2024-03-01T13:16:17.546044Z",
     "iopub.status.idle": "2024-03-01T13:16:18.335234Z",
     "shell.execute_reply": "2024-03-01T13:16:18.334416Z"
    },
    "papermill": {
     "duration": 0.8403,
     "end_time": "2024-03-01T13:16:18.337691",
     "exception": false,
     "start_time": "2024-03-01T13:16:17.497391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = DAIGTModel(CFG, config_path=None, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "22320b59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:18.437861Z",
     "iopub.status.busy": "2024-03-01T13:16:18.437539Z",
     "iopub.status.idle": "2024-03-01T13:16:22.990074Z",
     "shell.execute_reply": "2024-03-01T13:16:22.988967Z"
    },
    "papermill": {
     "duration": 4.605314,
     "end_time": "2024-03-01T13:16:22.992979",
     "exception": false,
     "start_time": "2024-03-01T13:16:18.387665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for step, (input_ids, attention_mask, labels) in enumerate(sample_loader):\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6ce18d4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:23.092117Z",
     "iopub.status.busy": "2024-03-01T13:16:23.091723Z",
     "iopub.status.idle": "2024-03-01T13:16:23.097099Z",
     "shell.execute_reply": "2024-03-01T13:16:23.096244Z"
    },
    "papermill": {
     "duration": 0.056974,
     "end_time": "2024-03-01T13:16:23.099212",
     "exception": false,
     "start_time": "2024-03-01T13:16:23.042238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "45d2c0a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:23.197872Z",
     "iopub.status.busy": "2024-03-01T13:16:23.197526Z",
     "iopub.status.idle": "2024-03-01T13:16:23.204823Z",
     "shell.execute_reply": "2024-03-01T13:16:23.204015Z"
    },
    "papermill": {
     "duration": 0.058812,
     "end_time": "2024-03-01T13:16:23.206650",
     "exception": false,
     "start_time": "2024-03-01T13:16:23.147838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0344],\n",
       "        [ 0.2451],\n",
       "        [ 0.0803],\n",
       "        [-0.0541]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65293d1",
   "metadata": {
    "papermill": {
     "duration": 0.048672,
     "end_time": "2024-03-01T13:16:23.304487",
     "exception": false,
     "start_time": "2024-03-01T13:16:23.255815",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span style=\"color: #7b6b59;\">Step 4.6: Train the Model - Training Loop</span>\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">Key Concepts in Deep Learning for Effective Model Training</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3a3442",
   "metadata": {
    "papermill": {
     "duration": 0.047793,
     "end_time": "2024-03-01T13:16:23.401303",
     "exception": false,
     "start_time": "2024-03-01T13:16:23.353510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color: #7b6b59;\">Concept 1: Optimizers</span>\n",
    "\n",
    "#### <span style=\"color: #7b6b59;\">Introduction</span>\n",
    "\n",
    "\n",
    "Many people may be using optimizers while training the neural network without knowing that the method is known as optimization. \n",
    "\n",
    "- Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses. How you should change your weights or learning rates of your neural network to reduce the losses is defined by the optimizers you use. Optimization algorithms or strategies are responsible for reducing the losses and to provide the most accurate results possible.\n",
    "\n",
    "\n",
    "#### <span style=\"color: #7b6b59;\">Optimization Algorithms</span>\n",
    "\n",
    "We’ll learn about different types of optimizers and their advantages:\n",
    "\n",
    "\n",
    "1. **Gradient Descent:** Gradient Descent is the most basic but most used optimization algorithm. It’s used heavily in linear regression and classification algorithms. Backpropagation in neural networks also uses a gradient descent algorithm. Gradient descent is a first-order optimization algorithm which is dependent on the first order derivative of a loss function. It calculates that which way the weights should be altered so that the function can reach a minima. Through backpropagation, the loss is transferred from one layer to another and the model’s parameters also known as weights are modified depending on the losses so that the loss can be minimized. algorithm: θ=θ−α⋅∇J(θ). Gradient descent is an optimization algorithm based on a convex function and tweaks its parameters iteratively to minimize a given function to its local minimum. Gradient Descent iteratively reduces a loss function by moving in the direction opposite to that of steepest ascent. It is dependent on the derivatives of the loss function for finding minima. uses the data of the entire training set to calculate the gradient of the cost function to the parameters which requires large amount of memory and slows down the process. How big/small the steps are gradient descent takes into the direction of the local minimum are determined **by the learning rate**, which figures out how fast or slow we will move towards the optimal weights.\n",
    "\n",
    "    <img width=\"879\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/1ce01c91-6d49-45ab-aed5-05679edcbc01\">\n",
    "    \n",
    "    <img width=\"819\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/36cdb708-717a-47fe-83a4-dd63fe8c7af5\">\n",
    "\n",
    "    1. **Advantages:**\n",
    "    \n",
    "        - Easy computation.\n",
    "        - Easy to implement.\n",
    "        - Easy to understand.\n",
    "    1. **Disadvantages:**\n",
    "\n",
    "        - May trap at local minima.\n",
    "        - Weights are changed after calculating gradient on the whole dataset. So, if the dataset is too large than this may take years to converge to the minima. Because this method calculates the gradient for the entire data set in one update, the calculation is very slow.\n",
    "        - Requires large memory to calculate gradient on the whole dataset. It requires large memory and it is computationally expensive.\n",
    "\n",
    "1. **Stochastic Gradient Descent:** It’s a variant of Gradient Descent. It tries to update the model’s parameters more frequently. In this, the model parameters are altered after computation of loss on each training example. So, if the dataset contains 1000 rows SGD will update the model parameters 1000 times in one cycle of dataset instead of one time as in Gradient Descent. `θ=θ−α⋅∇J(θ;x(i);y(i)) , where {x(i) ,y(i)}` are the training examples. As the model parameters are frequently updated parameters have high variance and fluctuations in loss functions at different intensities. It is a variant of Gradient Descent. It update the model parameters one by one. If the model has 10K dataset SGD will update the model parameters 10k times.\n",
    "\n",
    "    <img width=\"812\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/4bbb77e3-5c50-4bbd-a467-b1e240970db2\">\n",
    "\n",
    "    1. **Advantages:**\n",
    "\n",
    "        - Frequent updates of model parameters hence, converges in less time.\n",
    "        - Requires less memory as no need to store values of loss functions.\n",
    "        - May get new minima’s.\n",
    "        - Allows the use of large data sets as it has to update only one example at a time.\n",
    "   \n",
    "    1. **Disadvantages:**\n",
    "\n",
    "        - High variance in model parameters.\n",
    "        - May shoot even after achieving global minima.\n",
    "        - To get the same convergence as gradient descent needs to slowly reduce the value of learning rate.\n",
    "        - The frequent can also result in noisy gradients which may cause the error to increase instead of decreasing it.\n",
    "        - Frequent updates are computationally expensive.\n",
    "\n",
    "1. **Mini-Batch Gradient Descent:** It’s best among all the variations of gradient descent algorithms. It is an improvement on both SGD and standard gradient descent. It updates the model parameters after every batch. So, the dataset is divided into various batches and after every batch, the parameters are updated. `θ=θ−α⋅∇J(θ; B(i)), where {B(i)}` are the batches of training examples. It is a combination of the concepts of SGD and batch gradient descent. It simply splits the training dataset into small batches and performs an update for each of those batches. This creates a balance between the robustness of stochastic gradient descent and the efficiency of batch gradient descent. it can reduce the variance when the parameters are updated, and the convergence is more stable. It splits the data set in batches in between 50 to 256 examples, chosen at random.\n",
    "\n",
    "    1. **Advantages:**\n",
    "\n",
    "        - Frequently updates the model parameters and also has less variance.\n",
    "        - Requires medium amount of memory.\n",
    "        - It leads to more stable convergence.\n",
    "        - more efficient gradient calculations.\n",
    "        \n",
    "    1. **All types of Gradient Descent have some challenges:**\n",
    "\n",
    "        - Choosing an optimum value of the learning rate. If the learning rate is too small than gradient descent may take ages to converge. If the learning rate is too small, the convergence rate will be slow. If it is too large, the loss function will oscillate or even deviate at the minimum value.\n",
    "\n",
    "        - Have a constant learning rate for all the parameters. There may be some parameters which we may not want to change at the same rate.\n",
    "        - May get trapped at local minima. Mini-batch gradient descent does not guarantee good convergence,\n",
    "\n",
    "1. **Momentum:** Momentum was invented for reducing high variance in SGD and softens the convergence. It accelerates the convergence towards the relevant direction and reduces the fluctuation to the irrelevant direction. One more hyperparameter is used in this method known as momentum symbolized by ‘γ’. `V(t)=γV(t−1)+α.∇J(θ)`. Now, the weights are updated by `θ=θ−V(t)`. The momentum term `γ` is usually set to 0.9 or a similar value.\n",
    "\n",
    "    1. **Advantages:**\n",
    "\n",
    "        - Reduces the oscillations and high variance of the parameters.\n",
    "        - Converges faster than gradient descent.\n",
    "\n",
    "    1. **Disadvantages:**\n",
    "\n",
    "        - One more hyper-parameter is added which needs to be selected manually and accurately.\n",
    "\n",
    "1. **Nesterov Accelerated Gradient:** Momentum may be a good method but if the momentum is too high the algorithm may miss the local minima and may continue to rise up. So, to resolve this issue the NAG algorithm was developed. It is a look ahead method. We know we’ll be using `γV(t−1)` for modifying the weights so, `θ−γV(t−1)` approximately tells us the future location. Now, we’ll calculate the cost based on this future parameter rather than the current one. `V(t)=γV(t−1)+α. ∇J( θ−γV(t−1) )` and then update the parameters using `θ=θ−V(t)`. \n",
    "     \n",
    "     <img width=\"434\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/c9351e81-bce5-4717-8654-27b300497341\">\n",
    " \n",
    "     1. **Advantages:**\n",
    "        \n",
    "        - Does not miss the local minima.\n",
    "        - Slows if minima’s are occurring.\n",
    "    \n",
    "     1. **Disadvantages:**\n",
    "\n",
    "        - Still, the hyperparameter needs to be selected manually.\n",
    "   \n",
    "1. **Adagrad**: One of the disadvantages of all the optimizers explained is that the learning rate is constant for all parameters and for each cycle. This optimizer changes the learning rate. It changes the learning rate `‘η’` for each parameter and at every time step `‘t’`. It’s a type second order optimization algorithm. It works on the derivative of an error function.\n",
    "    \n",
    "    <img width=\"564\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/6910d495-c191-4857-9360-e1402b7c6023\">\n",
    "    \n",
    "    `η` is a learning rate which is modified for given parameter `θ(i)` at a given time based on previous gradients calculated for given parameter `θ(i)`. We store the sum of the squares of the gradients w.r.t. `θ(i)` up to time step `t`, while `ϵ` is a smoothing term that avoids division by zero (usually on the order of 1e−8). Interestingly, without the square root operation, the algorithm performs much worse. **It makes big updates for less frequent parameters and a small step for frequent parameters.**\n",
    "\n",
    "    1. **Advantages:**\n",
    "\n",
    "        - Learning rate changes for each training parameter.\n",
    "        - Don’t need to manually tune the learning rate.\n",
    "        - Able to train on sparse data.\n",
    "\n",
    "    1. **Disadvantages:**\n",
    "\n",
    "        - Computationally expensive as a need to calculate the second order derivative.\n",
    "\n",
    "1. **AdaDelta:** It is an extension of AdaGrad which tends to remove the decaying learning Rate problem of it. Instead of accumulating all previously squared gradients, Adadelta limits the window of accumulated past gradients to some fixed size w. In this exponentially moving average is used rather than the sum of all the gradients.\n",
    "\n",
    "    1. **Advantages:**\n",
    "\n",
    "        - Now the learning rate does not decay and the training does not stop.\n",
    "        \n",
    "    1. **Disadvantages:**\n",
    "        - Computationally expensive.\n",
    "\n",
    "1. **Adam:**  Adam (Adaptive Moment Estimation) works with momentums of first and second order. The intuition behind the Adam is that we don’t want to roll so fast just because we can jump over the minimum, we want to decrease the velocity a little bit for a careful search. In addition to storing an exponentially decaying average of past squared gradients like AdaDelta, Adam also keeps an exponentially decaying average of past gradients.\n",
    "\n",
    "    1. **Advantages:**\n",
    "\n",
    "        - The method is too fast and converges rapidly.\n",
    "        - Rectifies vanishing learning rate, high variance.\n",
    "\n",
    "    1. **Disadvantages:**\n",
    "\n",
    "        - Computationally costly.\n",
    "\n",
    "#### <span style=\"color: #7b6b59;\">Conclusions</span>\n",
    "\n",
    "- **Adam is the best optimizers. If one wants to train the neural network in less time and more efficiently than Adam is the optimizer.**\n",
    "\n",
    "- **For sparse data use the optimizers with dynamic learning rate.**\n",
    "\n",
    "- **If, want to use gradient descent algorithm than min-batch gradient descent is the best option.The learning rate is always decreasing results in slow training.**\n",
    "\n",
    "**How to choose optimizers?**\n",
    "\n",
    "1. If the data is sparse, use the self-applicable methods, namely Adagrad, Adadelta, RMSprop, Adam.\n",
    "\n",
    "1. RMSprop, Adadelta, Adam have similar effects in many cases.\n",
    "\n",
    "1. Adam just added bias-correction and momentum on the basis of RMSprop,\n",
    "\n",
    "1. As the gradient becomes sparse, Adam will perform better than RMSprop.\n",
    "\n",
    "\n",
    "<img width=\"911\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/110e15de-29a7-4a7b-8850-211ba8d8c0ce\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca91432f",
   "metadata": {
    "papermill": {
     "duration": 0.047675,
     "end_time": "2024-03-01T13:16:23.496723",
     "exception": false,
     "start_time": "2024-03-01T13:16:23.449048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color: #7b6b59;\">Concept 2: Weight Decay</span>\n",
    "\n",
    "#### <span style=\"color: #7b6b59;\">Introduction</span>\n",
    "\n",
    "I mentioned before that data augmentation helps deep learning models generalize well. That was on the data side of things. **What about the model side of things? What can we do while training our models, that will help them generalize even better.**\n",
    "\n",
    "***We do weight decay.***\n",
    "\n",
    "<img width=\"917\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/73c04cdc-31f8-4818-97f6-ff70461c83ec\">\n",
    "\n",
    "We start by looking at the image above. We see that we have a bunch of data points and that we cannot fit them well with a straight line. Hence, we use a 2nd degree polynomial to do so. We also notice that if increase the degree of the polynomial beyond a certain point, then our model becomes too complex and starts to overfit.\n",
    "\n",
    "This means that in order to prevent overfitting, we shouldn’t allow our models to get too complex. **Unfortunately, this has led to a misconception in deep learning that we shouldn’t use a lot of parameters (in order to keep our models from getting overly complex).**\n",
    "\n",
    "\n",
    "- **Origin of weight decay:**  First of all, real world data is not going to be as simple as the one shown above. Real world data is complex and in order to solve complex problems, we need complex solutions. Having fewer parameters is only one way of preventing our model from getting overly complex. But it is actually a very limiting strategy. More parameters mean more interactions between various parts of our neural network. And more interactions mean more non-linearities. These non-linearities help us solve complex problems. **However, we don’t want these interactions to get out of hand. Hence, what if we penalize complexity. We will still use a lot of parameters, but we will prevent our model from getting too complex. This is how the idea of weight decay came up.**\n",
    "\n",
    "- **Weight decay:** One way to penalize complexity, would be to add all our parameters (weights) to our loss function. Well, that won’t quite work because some parameters are positive and some are negative. So what if we add the squares of all the parameters to our loss function. We can do that, however it might result in our loss getting so huge that the best model would be to set all the parameters to 0.\n",
    "\n",
    "- **weight decay rate:** To prevent that from happening, we multiply the sum of squares with another smaller number. This number is called weight decay or wd. Our loss function now looks as follows:\n",
    "\n",
    "<img width=\"829\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/056024ed-0084-486e-9dda-d6dfe99cbd1b\">\n",
    "\n",
    "- **Deciding the value of weight decay rate:** Generally a wd = 0.1 works pretty well. However, the folks at fastai have been a little conservative in this respect. Hence the default value of weight decay in fastai is actually 0.01 . The reason to choose this value is because if you have too much weight decay, then no matter how much you train, the model never quite fits well enough whereas if you have too little weight decay, you can still train well, you just have to stop a little bit early.\n",
    "\n",
    "To understand why using the sum of squares of weights leads to smaller weight updates in a machine learning model, let's delve into the role of weight decay and how it influences the learning process.\n",
    "\n",
    "**Regularization by Weight Decay**\n",
    "\n",
    "- **Weight Decay Mechanism:** In weight decay, we add a regularization term to the loss function. This term is typically the sum of the squares of all the model weights, multiplied by a regularization parameter (λ). The modified loss function looks something like this:\n",
    "    <img width=\"533\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/b62a55cb-c811-47bb-99ac-dd4d294c6ad1\">\n",
    "\n",
    "- **Purpose:** The purpose of this term is to penalize larger weights in the model. By doing so, it encourages the model to keep the weights as small as possible.\n",
    "\n",
    "**Impact on Weight Updates**\n",
    "\n",
    "- **Gradient Descent:** During training, models typically use gradient descent or its variants to minimize the loss. Gradient descent updates each weight by subtracting a fraction of the derivative (gradient) of the loss with respect to that weight.\n",
    "\n",
    "- **Incorporating Weight Decay:** When the sum of the squares of weights is added to the loss function, the gradient of this additional term with respect to each weight is proportional to the weight itself (since the derivative of w^2  with respect to w is 2 * w)\n",
    "\n",
    "- **The Update Rule:** So, the update rule for each weight not only involves the gradient derived from the original loss (which reflects how well the model is performing on the training data) but also includes a term from the weight decay. The update looks something like this:\n",
    "    <img width=\"653\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/09d407c2-ea1a-4c0f-a08a-2da4331ee1c1\">\n",
    "\n",
    "- **Effect of the Weight Decay Term:** Notice the last part of the update rule: it effectively reduces the weight by a factor proportional to its current value. Larger weights are reduced more than smaller weights.\n",
    "\n",
    "\n",
    "\n",
    "#### <span style=\"color: #7b6b59;\">What is weight decay?</span>\n",
    "\n",
    "- Weight decay is a **regularization technique** by adding a small penalty, usually the L2 norm of the weights (all the weights of the model), to the loss function.  **`loss = loss + weight decay parameter * L2 norm of the weights`**\n",
    "\n",
    "- Weight decay **is a form of regularization that penalizes large weights in the network.** It does this by adding a term to the loss function that is proportional to the sum of the squared weights. This term reduces the magnitude of the weights and prevents them from growing too large. Weight decay can also be seen as a way of introducing prior knowledge that the weights should be small and smooth.\n",
    "\n",
    "Weight decay essentially pulls the weights towards 0. **While this is beneficial for convolutional and linear layer weights, Batchnorm layer parameters are meant to scale (the gamma parameter) and shift (the beta parameter) the normalized input of the layer. As such, forcing these values to a lower value would affect the distribution and result in inferior results.** The weight decay is a regularization parameter that prevents the model weights from ‘exploding’. Zeroing the weight decay for these parameters is usually done by default in various projects and frameworks, but it’s still worth checking since it is still not the default behavior for Pytorch. \n",
    "\n",
    "Some people prefer to only apply weight decay to the weights and not the bias. **PyTorch applies weight decay to both weights and bias.**\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">How does weight decay work?</span>\n",
    "\n",
    "\n",
    "Weight decay works by updating the weights in the opposite direction of their current value, scaled by a factor called the weight decay rate. This factor determines how much the weights are shrunk at each step of the optimization. A higher weight decay rate means more regularization and less overfitting, but also less flexibility and more underfitting. A lower weight decay rate means less regularization and more overfitting, but also more flexibility and less underfitting. The optimal weight decay rate depends on the data, the model, and the learning rate. During each training step, the weight decay mechanism updates the weights by subtracting a fraction of the weight's value from itself. This update is proportional to the weight and in the opposite direction of its current value. For instance:\n",
    "\n",
    "- If a weight is positive and large, subtracting a fraction of it will make it less positive (i.e., smaller in magnitude).\n",
    "- If a weight is negative and large (in magnitude), subtracting a fraction (which will be negative) will make it less negative (i.e., closer to zero).\n",
    " \n",
    "### <span style=\"color: #7b6b59;\">Why do we use weight decay?</span>\n",
    "\n",
    "1. **To prevent overfitting.**\n",
    "\n",
    "1. **To keep the weights small and avoid exploding gradient.** Because the L2 norm of the weights are added to the loss, each iteration of your network will try to optimize/minimize the model weights in addition to the loss. This will help keep the weights as small as possible, preventing the weights to grow out of control, and thus avoid exploding gradient.\n",
    "\n",
    "**What are the benefits of weight decay?**\n",
    "\n",
    "Weight decay offers a variety of advantages for neural network training and performance. It can reduce the variance of the model, which leads to better generalization ability. Additionally, weight decay prevents the weights from becoming too large, which could cause numerical instability or gradient explosion. Furthermore, it simplifies the model and makes it more interpretable. Finally, it can also improve the convergence speed and stability of the optimization algorithm.\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">What are the drawbacks of weight decay??</span>\n",
    "\n",
    "\n",
    "Weight decay has some drawbacks that should be taken into account. For instance, it adds an extra hyperparameter to tune, making the model selection process more complex and costly. Additionally, it can reduce the capacity and expressiveness of a model, especially for deep and complex networks. Furthermore, weight decay can interfere with the learning of sparse or important features since it treats all weights equally. Finally, it can cause underfitting if the weight decay rate is too high or if the data is noisy or insufficient.\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">How do we use weight decay?</span>\n",
    "\n",
    "\n",
    "To use weight decay, we can simply define the weight decay parameter in the `torch.optim.SGD` optimizer or the `torch.optim.Adam optimizer`. \n",
    "\n",
    "<img width=\"941\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/94444aa1-11d7-40e9-a39f-a86e7f666d09\">\n",
    "\n",
    "Note that Adam uses a different equation for the loss. But the key concept is the same. Also, as I mentioned above that **PyTorch applies weight decay to both weights and bias.** If you would like to only use weights, you can use `model.named_parameters()` function. **`model.named_parameters()` also allows you to do more complex weight decay operations like using weight decay in different layers.**\n",
    "\n",
    "\n",
    "Weight decay is used to prevent overfitting, which happens when a model learns the training data so well that it performs poorly on new, unseen data. The idea is to make the model simpler and more generalizable by penalizing complexity.\n",
    "\n",
    "**The Problem with Complexity in Weights**\n",
    "\n",
    "1. **Why Penalize Weights:** In a neural network, weights (parameters) determine how the input data is transformed into predictions. If these weights are too large or complex, the model might start fitting the noise in the data, leading to overfitting.\n",
    "\n",
    "1. **Adding Weights to Loss Function:** One intuitive idea to prevent overfitting is to keep the weights small. Initially, one might think of adding the weights directly to the loss function (which measures how wrong the model's predictions are). But there's a problem – weights can be both positive and negative, and just adding them can cancel each other out.  To address the issue of positive and negative weights canceling each other out, the sum of the squares of the weights is used. Squaring each weight ensures that every term is positive, regardless of the original sign of the weight. By adding the squares of the weights to the loss function, the model is penalized for having large weights – the larger the weight, the larger its square, and consequently, the larger the penalty.\n",
    "\n",
    "\n",
    "**The Role of Weight Decay Coefficient**\n",
    "\n",
    "1. **Risk of Too Much Penalty:** Simply adding the squares of the weights to the loss function can lead to another problem. The loss might become so dominated by these squares that the model might find setting all weights to zero as the best solution. This is not desirable, as it would make the model useless.\n",
    "\n",
    "1. **Weight Decay Coefficient (wd):** To balance this, the sum of the squares of the weights is multiplied by a small number called the weight decay coefficient (wd). This coefficient controls how much we penalize the weights, preventing the loss from becoming excessively large due to the weight penalty.\n",
    "\n",
    "1. **Tuning the Weight Decay:** The weight decay coefficient is a hyperparameter that needs to be chosen carefully. If it's too large, the model is overly penalized for having weights far from zero, which might hamper its ability to learn from data. If it's too small, it might not sufficiently prevent overfitting.\n",
    "\n",
    "**Why This Leads to Smaller Weight Updates**\n",
    "\n",
    "1. **Regularization Effect:** The weight decay term ensures that during each update, weights are pushed towards smaller values. This is because, in addition to moving in the direction that reduces the original loss, weights are also \"shrinking\" due to the weight decay term.\n",
    "\n",
    "1. **Preventing Overfitting:** By penalizing large weights, weight decay helps in preventing overfitting. Overfitting often occurs when weights grow too large, fitting the training data too closely, including its noise.\n",
    "\n",
    "1. **Smaller and More General Weights:** Over the course of training, this consistent push towards smaller weights leads to a model that, while trying to minimize the original loss, also maintains a more generalized set of weights that are less likely to overfit.\n",
    "\n",
    "In summary, the sum of squares of weights in weight decay leads to smaller weight updates because it adds a term to the weight update rule that is proportional to the size of the weight itself, effectively shrinking larger weights more significantly. This results in a model that learns to balance fitting the training data and maintaining smaller, more general weights, which helps in preventing overfitting.\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">How do you choose the weight decay rate?</span>\n",
    "\n",
    "Choosing the weight decay rate is a trade-off between regularization and flexibility. There is no universal formula or rule for setting the weight decay rate, as it depends on many factors, such as the data, the model, the learning rate, and the optimization algorithm. However, some general guidelines are to start with a small weight decay rate and increase it gradually until an improvement in validation or test performance is seen. Validation sets or cross-validation can be used to evaluate the effect of different rates on model performance and select the one that minimizes validation error. Additionally, grid searches or random searches can be employed to explore a range of rates and find the optimal one for a given problem. ***Finally, learning rate schedulers or adaptive optimizers can adjust the weight decay rate dynamically based on learning progress.***\n",
    "\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">How do you compare weight decay with other regularization methods?</span>\n",
    "\n",
    "Weight decay is one of the most common and effective regularization methods for neural networks, but it is not the only one. Other regularization methods that can be used alone or in combination with weight decay include noise injection, dropout, batch normalization, early stopping, and data augmentation. Each of these methods has its own advantages and disadvantages, making the best choice dependent on the specific problem and data. As a general rule, use weight decay as a baseline regularization method as it is simple, effective, and widely applicable. For noisy, sparse, or imbalanced data or for very deep or complex networks, noise injection or dropout is recommended. Batch normalization or early stopping should be used when a network suffers from slow or unstable convergence or when the learning rate is too high or too low. Data augmentation should be used when data is limited, simple, or homogeneous or when the network is very flexible or expressive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5931a0e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:23.642880Z",
     "iopub.status.busy": "2024-03-01T13:16:23.641758Z",
     "iopub.status.idle": "2024-03-01T13:16:23.651166Z",
     "shell.execute_reply": "2024-03-01T13:16:23.650320Z"
    },
    "papermill": {
     "duration": 0.062596,
     "end_time": "2024-03-01T13:16:23.653118",
     "exception": false,
     "start_time": "2024-03-01T13:16:23.590522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "    \"\"\"\n",
    "    Prepares the optimizer parameters for different parts of the model with specific learning rates and weight decay settings.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model for which the optimizer parameters are to be prepared.\n",
    "        encoder_lr (float): The learning rate to be applied to the encoder part of the model.\n",
    "        decoder_lr (float): The learning rate to be applied to the decoder part of the model.\n",
    "        weight_decay (float, optional): The weight decay rate to be applied. Defaults to 0.0.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains parameters ('params'), a learning rate ('lr'), and a weight decay rate ('weight_decay'). The list has three elements:\n",
    "              1. Parameters of the model's encoder part with weight decay (excluding biases and LayerNorm parameters).\n",
    "              2. Parameters of the model's encoder part without weight decay (only biases and LayerNorm parameters).\n",
    "              3. Parameters outside the 'model' namespace, typically custom layers, with a separate learning rate and no weight decay.\n",
    "\n",
    "    Example:\n",
    "        optimizer_parameters = get_optimizer_params(model, encoder_lr=1e-5, decoder_lr=1e-4, weight_decay=0.01)\n",
    "    \"\"\"\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    \n",
    "    optimizer_parameters = [\n",
    "        # The condition if not any(nd in n for nd in no_decay) checks if the name of the parameter does\n",
    "        # not include any of the strings listed in no_decay (like \"bias\" or \"LayerNorm\").\n",
    "        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)], # n, p represent the name and parameter, respectively.\n",
    "         'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "        # This condition checks if the string \"model\" is not present in the name (n) of each parameter.\n",
    "        # Essentially, this filters out parameters that are specifically part of model.model.\n",
    "        # In other words, it's selecting parameters that belong to model but not to the nested model.model.\n",
    "        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "         'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "    ]\n",
    "    return optimizer_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1ac501",
   "metadata": {
    "papermill": {
     "duration": 0.048742,
     "end_time": "2024-03-01T13:16:23.751419",
     "exception": false,
     "start_time": "2024-03-01T13:16:23.702677",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**AdamW Optimizer:**\n",
    "\n",
    "AdamW introduces a decoupling of the weight decay from the optimization steps taken w.r.t. the loss function. In traditional L2 regularization (also known as weight decay), the regularization term is added directly to the loss function, and the optimization step considers this augmented loss. However, this approach can interfere with the adaptive learning rate property of Adam. AdamW, proposed by Loshchilov and Hutter in their paper \"Decoupled Weight Decay Regularization,\" separates the weight decay from the loss optimization, applying it directly to the weights themselves after the optimization step based on the loss function. This allows for better training stability and performance, especially in the context of complex deep learning models.\n",
    "\n",
    "***Differences Between Adam and AdamW***\n",
    "\n",
    "1. **Weight Decay Integration:** The key difference lies in how weight decay regularization is applied. Adam applies weight decay together with the loss gradient, which can lead to suboptimal updates due to the interaction with Adam's moment estimation. AdamW applies weight decay directly to the parameters after the gradient update, which is more in line with the original intention of L2 regularization.\n",
    "\n",
    "1. **Performance in Practice:** AdamW can often lead to better generalization than Adam, especially in tasks that are sensitive to the settings of weight decay and require careful regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "16ec203d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:23.849788Z",
     "iopub.status.busy": "2024-03-01T13:16:23.848915Z",
     "iopub.status.idle": "2024-03-01T13:16:23.857126Z",
     "shell.execute_reply": "2024-03-01T13:16:23.856237Z"
    },
    "papermill": {
     "duration": 0.059439,
     "end_time": "2024-03-01T13:16:23.859057",
     "exception": false,
     "start_time": "2024-03-01T13:16:23.799618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer_parameters = get_optimizer_params(\n",
    "    model,\n",
    "    encoder_lr=CFG.encoder_lr, \n",
    "    decoder_lr=CFG.decoder_lr,\n",
    "    weight_decay=CFG.weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6095a0e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:23.957760Z",
     "iopub.status.busy": "2024-03-01T13:16:23.957434Z",
     "iopub.status.idle": "2024-03-01T13:16:23.962234Z",
     "shell.execute_reply": "2024-03-01T13:16:23.961309Z"
    },
    "papermill": {
     "duration": 0.056779,
     "end_time": "2024-03-01T13:16:23.964146",
     "exception": false,
     "start_time": "2024-03-01T13:16:23.907367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65faab2",
   "metadata": {
    "papermill": {
     "duration": 0.048563,
     "end_time": "2024-03-01T13:16:24.061722",
     "exception": false,
     "start_time": "2024-03-01T13:16:24.013159",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color: #7b6b59;\">Concept 3: Learning Rate Schedulers</span>\n",
    "\n",
    "#### What is the Learning Rate in Deep Learning?\n",
    "\n",
    "Neural networks have many **hyperparameters** that affect the model’s performance. **One of the essential hyperparameters is the learning rate (LR), which determines how much the model weights change between training steps.** Learning rate is one of the most important hyperparameters in the training of neural networks, impacting the speed and effectiveness of the learning process. In the context of machine learning, the learning rate is a hyperparameter that determines the step size at which an optimization algorithm (like gradient descent) proceeds while attempting to minimize the loss function. In the simplest case, the LR value is a fixed value between 0 and 1. However, choosing the correct LR value can be challenging. \n",
    "\n",
    "- On the one hand, a large learning rate can help the algorithm to converge quickly. But it can also cause the algorithm to bounce around the minimum without reaching it or even jumping over it if it is too large. \n",
    "- On the other hand, a small learning rate can converge better to the minimum. However, the optimizer may take too long to converge or get stuck in a plateau if it is too small.\n",
    "\n",
    "A learning rate that is too high can cause the model to oscillate around the minimum, while a learning rate that is too low can cause the training process to be very slow or even stall. **This section provides a visual introduction to learning rate schedulers, which are techniques used to adapt the learning rate during training.**\n",
    "\n",
    "The importance of learning rate schedulers becomes evident when considering the dynamic nature of model training. As models traverse complex loss landscapes, a fixed learning rate may hinder convergence or cause overshooting. Learning rate schedulers address this challenge by adapting the learning rate based on the model’s performance during training. This adaptability is crucial for avoiding divergence, accelerating convergence, and facilitating the discovery of optimal model parameters.\n",
    "\n",
    "#### What is a Learning Rate Scheduler?\n",
    "\n",
    "- A learning rate scheduler is a method that adjusts the learning rate during the training process, often lowering it as the training progresses. This helps the model to make large updates at the beginning of training when the parameters are far from their optimal values, and smaller updates later when the parameters are closer to their optimal values, allowing for more fine-tuning.\n",
    "\n",
    "One solution to help the algorithm converge quickly to an optimum is to use a learning rate scheduler. **A learning rate scheduler adjusts the learning rate according to a pre-defined schedule during the training process.**\n",
    "\n",
    "***One solution to help the algorithm converge quickly to an optimum is to use a learning rate scheduler.***\n",
    "\n",
    "Usually, the learning rate is set to a higher value at the beginning of the training to allow faster convergence. As the training progresses, the learning rate is reduced to enable convergence to the optimum and thus leading to better performance. Reducing the learning rate over the training process is also known as **annealing** or **decay**.\n",
    "\n",
    "<img width=\"797\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/05db42fd-1bd2-4dba-87bb-44e045b74767\">\n",
    "\n",
    "An easy start is to use a constant learning rate in gradient descent algorithm. But you can do better with a learning rate schedule. A schedule is to make learning rate adaptive to the gradient descent optimization procedure, so you can increase performance and reduce training time.\n",
    "\n",
    "In the neural network training process, data is feed into the network in batches, with many batches in one epoch. Each batch triggers one training step, which the gradient descent algorithm updates the parameters once. However, usually the learning rate schedule is updated once for each training epoch only.\n",
    "\n",
    "You can update the learning rate as frequent as each step but usually it is updated once per epoch because you want to know how the network performs in order to determine how the learning rate should update. Regularly, a model is evaluated with validation dataset once per epoch.\n",
    "\n",
    "There are multiple ways of making learning rate adaptive. At the beginning of training, you may prefer a larger learning rate so you improve the network coarsely to speed up the progress. In a very complex neural network model, you may also prefer to gradually increasse the learning rate at the beginning because you need the network to explore on the different dimensions of prediction. At the end of training, however, you always want to have the learning rate smaller. Since at that time, you are about to get the best performance from the model and it is easy to overshoot if the learning rate is large.\n",
    "\n",
    "Therefore, the simplest and perhaps most used adaptation of the learning rate during training are techniques that reduce the learning rate over time. These have the benefit of making large changes at the beginning of the training procedure when larger learning rate values are used and decreasing the learning rate so that a smaller rate and, therefore, smaller training updates are made to weights later in the training procedure.\n",
    "\n",
    "This has the effect of quickly learning good weights early and fine-tuning them later.\n",
    "\n",
    "#### Different Learning Rate Schedulers\n",
    "\n",
    "The amount of different learning rate schedulers can be overwhelming. Thus, this section aims to give you an overview of how different pre-defined learning rate schedulers in PyTorch adjust the learning rate during training:\n",
    "\n",
    "1. **StepLR:** The StepLR reduces the learning rate by a multiplicative factor after every predefined number of training steps.\n",
    "    \n",
    "    <img width=\"842\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/b8cfa2bf-25f8-4591-b839-87ff77ae1eb2\">\n",
    "    \n",
    "1. **MultiStepLR:** The MultiStepLR — similarly to the StepLR — also reduces the learning rate by a multiplicative factor but after each pre-defined milestone.\n",
    "    \n",
    "    <img width=\"780\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/c7682b7c-6dbb-45ed-a9cb-a07b83b8d770\">\n",
    "\n",
    "1. **ConstantLR:** The ConstantLR reduces learning rate by a multiplicative factor until the number of training steps reaches a pre-defined milestone. As you might have already noticed, if your starting factor is smaller than 1, this learning rate scheduler increases the learning rate over the course of the training process instead of decreasing it.\n",
    "\n",
    "    <img width=\"992\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/ab72106f-b673-424f-abf9-b456c1fb7069\">\n",
    "\n",
    "1. **LinearLR:** The LinearLR — similarly to the ConstantLR— also reduces the learning rate by a multiplicative factor at the beginning of the training. But it linearly increases the learning rate over a defined number of training steps until it reaches its originally set learning rate. If your starting factor is smaller than 1, this learning rate scheduler also increases the learning rate over the course of the training process instead of decreasing it.\n",
    "    \n",
    "    <img width=\"948\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/f9404d66-7aa8-4a91-bdf3-8d4330c6f10e\">\n",
    " \n",
    "1. **ExponentialLR:** The ExponentialLR reduces learning rate by a multiplicative factor at every training step.\n",
    "\n",
    "    <img width=\"941\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/b649f3f2-9137-410a-8598-177427d30068\">\n",
    "\n",
    "1. **PolynomialLR:** The PolynomialLR reduces learning rate by using a polynomial function for a defined number of steps\n",
    "    \n",
    "    <img width=\"851\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/893c40c8-0d35-4fc9-a6e1-971d2000e33d\">\n",
    "\n",
    "1. **CosineAnnealingLR:** The CosineAnnealingLR reduces learning rate by a cosine function. While you could technically schedule the learning rate adjustments to follow multiple periods, the idea is to decay the learning rate over half a period for the maximum number of iterations. ***Philipp Singer and Yauhen Babakhin, two Kaggle Competition Grandmasters, recommend using cosine decay as a learning rate scheduler for deep transfer learning.***\n",
    "    \n",
    "    <img width=\"874\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/80f3e558-a7ee-4f4b-bee1-73d265fd979d\">\n",
    "\n",
    "1. **CosineAnnealingWarmRestartsLR:** The CosineAnnealingWarmRestarts is similar to the cosine annealing schedule. However, it allows you to restart the LR schedule with the initial LR at, e.g., each epoch. This is called a warm restart and was introduced in 2017. Increasing the LR causes the model to diverge. However, this intentional divergence enables the model to escape local minima and find an even better global minimum.\n",
    "    \n",
    "    <img width=\"974\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/229a407a-2813-416b-954f-c66b55976135\">\n",
    "    \n",
    "1. **CyclicLR:** The CyclicLR adjusted the learning rate according to a cyclical learning rate policy, which is based on the concept of warm restarts which we just discussed in the previous section. In PyTorch there are three built-in policies.\n",
    "    \n",
    "    <img width=\"975\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/7b07a4d0-b7cc-46dd-b7ae-aab6ec686603\">\n",
    "    <img width=\"1118\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/a40fd6f7-d3dd-4410-b34e-77ed8a2a1090\">\n",
    "\n",
    "1. **OneCycleLR:** The OneCycleLR reduces learning rate according to the 1cycle learning rate policy, which was introduced in a paper in 2017. In contrast to many other learning rate schedulers, the learning rate is not only decreased over the training process. Instead, the learning rate increases from an initial learning rate to some maximum learning rate and then decreases again.\n",
    "    \n",
    "    <img width=\"937\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/f94f5316-af75-43c7-89e3-edd25943c728\">\n",
    "    <img width=\"1028\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/01cf8cda-555f-45d3-a53c-74b000284f22\">\n",
    "\n",
    "1. **ReduceLROnPlateauLR:** The ReduceLROnPlateau reduces the learning rate by when the metric has stopped improving. As you can guess, this is difficult to visualize because the learning rate reduction timing depends on your model, data, and hyperparameters.\n",
    "\n",
    "1. **Custom Learning Rate Schedulers with Lambda Functions:** If the built-in learning rate schedulers don’t fit your needs, you have the possibility to define a scheduler with lambda functions. The lambda function is a function that returns a multiplicative factor based on the epoch value.\n",
    "\n",
    "#### Parameters and their Significance\n",
    "\n",
    "- **optimizer:** Establishes the connection between the PyTorch learning rate scheduler and the optimizer responsible for updating the model parameters.\n",
    "- **step_size:** Dictates the number of epochs between each adjustment of the learning rate, influencing how often the learning rate is updated during training.\n",
    "- **gamma:** Scales the learning rate after each step, controlling the rate at which the learning rate decays or grows.\n",
    "- **last_epoch:** A parameter that aids in resuming training from a specific epoch, providing flexibility in model development and training management.\n",
    "\n",
    "#### Conclusion\n",
    "Learning rate schedulers are an important tool in the machine learning practitioner’s toolkit, providing a mechanism to adjust the learning rate over time, which can help to improve the efficiency and effectiveness of the training process. The best learning rate scheduler to use can depend on the specific problem and dataset, and it is often helpful to experiment with different schedulers to see which one works best. \n",
    "\n",
    "**Tips for Using Learning Rate Schedules**\n",
    "\n",
    "1. **Increase the initial learning rate.** Because the learning rate will very likely decrease, start with a larger value to decrease from. A larger learning rate will result in a lot larger changes to the weights, at least in the beginning, allowing you to benefit from the fine-tuning later. To select a learning rate schedule, a common practice is to start with a value that’s not too small, e.g., 0.5, and then exponentially lower it to get the smaller values, such as 0.01, 0.001, 0.0001;\n",
    "\n",
    "1. **Use a large momentum.** Many optimizers can consider momentum. Using a larger momentum value will help the optimization algorithm continue to make updates in the right direction when your learning rate shrinks to small values. To build an effective model, we should also factor in other hyperparameters, such as momentum, regularization parameters (dropout, early stopping etc.).\n",
    "\n",
    "1. **Experiment with different schedules.** It will not be clear which learning rate schedule to use, so try a few with different configuration options and see what works best on your problem. Also, try schedules that change exponentially and even schedules that respond to the accuracy of your model on the training or test datasets.\n",
    "\n",
    "1. Although oftentimes being the default optimizer in deep learning applications, `Adam` under the hood does not necessarily outperforms all the time; it can cause model divergence;\n",
    "\n",
    "Now that you have seen a variety of different built-in PyTorch learning rate schedulers, you are probably curious about which learning rate scheduler you should choose for your Deep Learning project.\n",
    "\n",
    "Unfortunately, the answer is not that easy — as often in life. For a while, ReduceLROnPlateau was a popular learning rate scheduler. Today, other approaches like CosineAnnealingLR and OneCycleLR or approaches with warm restarts like CosineAnnealingWarmRestarts and CyclicLR have been increasing in popularity.\n",
    "\n",
    "Nonetheless, you might need to run a few experiments to determine which learning rate scheduler best suits your problem. **But, what we can say is that using any learning scheduler will most likely lead to better model performance.**\n",
    "\n",
    "Some more learning rates in one plot.\n",
    "\n",
    "<img width=\"845\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/15d72733-9844-49f5-8c06-d93cfdfd4885\">\n",
    "\n",
    "\n",
    "\n",
    "The function below, `get_scheduler`, is designed to select and configure a learning rate scheduler for an optimizer used in a machine learning model. The function takes three parameters: `cfg`, `optimizer`, and `num_train_steps`.\n",
    "\n",
    "Here's a breakdown of its functionality:\n",
    "\n",
    "- **Parameters:**\n",
    "\n",
    "    - **cfg**: A configuration object, presumably an instance of the CFG class. It contains settings for the scheduler.\n",
    "    - **optimizer:** The optimizer for which the scheduler will be applied. In machine learning, optimizers are algorithms or methods used to change the attributes of the neural network such as weights and learning rate to reduce the losses.\n",
    "    - **num_train_steps:** The total number of training steps (or iterations) that the model will undergo during training.\n",
    "    - **Scheduler Selection:** The function first checks the scheduler attribute of the cfg object. Based on this value, it selects the type of scheduler to be used:\n",
    "        - If cfg.scheduler is 'linear', it selects a linear scheduler.\n",
    "        - If cfg.scheduler is 'cosine', it selects a cosine scheduler.\n",
    "\n",
    "- **Scheduler Configuration:**\n",
    "\n",
    "    - **Linear Scheduler (get_linear_schedule_with_warmup):** This scheduler adjusts the learning rate linearly after an initial \"warmup\" period during which the learning rate increases linearly from 0 to the initial learning rate set in the optimizer.\n",
    "        - **num_warmup_steps:** The number of steps for the warmup phase.\n",
    "        - **num_training_steps:** Total number of training steps.\n",
    "\n",
    "    - **Cosine Scheduler (get_cosine_schedule_with_warmup):** This scheduler also starts with a warmup period like the linear scheduler. After the warmup, it adjusts the learning rate following a cosine curve, typically decreasing it gradually. This can be useful for converging to a better final solution.\n",
    "        - **num_warmup_steps:** The number of steps for the warmup phase.\n",
    "        - **num_training_steps:** Total number of training steps.\n",
    "        - **num_cycles:** The number of cycles in the cosine curve; 0.5 means it will use half a cosine curve, which is common in practice.\n",
    "\n",
    "- **Return Value:** The function returns the configured scheduler.\n",
    "\n",
    "In the provided `CFG class`, the scheduler is set to `'cosine'`, which means the `get_scheduler` function will configure and return a cosine learning rate scheduler with the specified num_warmup_steps (0 in this case) and num_cycles (0.5). The number of training steps (num_train_steps) must be provided when the function is called.  In the context of machine learning and particularly in training neural networks, a warmup period is a phase at the beginning of the training process where the learning rate is gradually increased from a lower value to its initially defined value. This concept is often used in conjunction with learning rate schedulers, such as the linear and cosine schedulers you have in your code. Here's a detailed explanation of the warmup period:\n",
    "\n",
    "1. **Purpose of Warmup:**\n",
    "\n",
    "    - **Stabilizing Training:** Starting with a high learning rate can cause the model's parameters to change too rapidly, leading to unstable training. A warmup period helps by starting from a lower learning rate, allowing the model to gradually adapt to the learning process.\n",
    "    - **Preventing Early Divergence:** A lower learning rate at the beginning helps in avoiding divergence of the model's loss, which can occur if the steps taken are too large.\n",
    "    - **Improving Convergence:** Gradually increasing the learning rate can lead to better overall convergence, as the model initially makes small, careful steps towards the minimum loss, before taking larger steps.\n",
    "\n",
    "1. **How It Works:** During the warmup period, the learning rate starts from a small value (often close to zero) and gradually increases to the pre-set learning rate of the optimizer. The length of the warmup period is determined by the num_warmup_steps. This is a hyperparameter that can be tuned based on the model and dataset.\n",
    "\n",
    "1. **Implementation:** In your code, during the warmup phase, the scheduler adjusts the learning rate linearly from a smaller value up to the optimizer's initial learning rate. After the warmup period is completed, the scheduler switches to its main schedule (linear or cosine) for the rest of the training process.\n",
    "\n",
    "1. **Application in Different Schedulers:**\n",
    "\n",
    "    - **Linear Scheduler with Warmup:** Post warmup, the learning rate decreases linearly from its initial value to zero.\n",
    "    - **Cosine Scheduler with Warmup:** After the warmup, the learning rate follows a cosine curve (or part of it, based on num_cycles), typically decreasing in a non-linear fashion.\n",
    "    \n",
    "<img width=\"1085\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/592f872b-bfba-4419-89a3-23d531a18225\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5e82b364",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:24.159820Z",
     "iopub.status.busy": "2024-03-01T13:16:24.159157Z",
     "iopub.status.idle": "2024-03-01T13:16:24.166178Z",
     "shell.execute_reply": "2024-03-01T13:16:24.165268Z"
    },
    "papermill": {
     "duration": 0.057278,
     "end_time": "2024-03-01T13:16:24.167991",
     "exception": false,
     "start_time": "2024-03-01T13:16:24.110713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "    \"\"\"\n",
    "    Creates a learning rate scheduler based on the configuration provided.\n",
    "\n",
    "    This function supports the creation of two types of schedulers: 'linear' and 'cosine'.\n",
    "    The 'linear' scheduler uses a linear schedule with warmup, whereas the 'cosine'\n",
    "    scheduler uses a cosine schedule with warmup.\n",
    "\n",
    "    Args:\n",
    "        cfg: A configuration object with attributes 'scheduler', 'num_warmup_steps', and optionally 'num_cycles'.\n",
    "             - cfg.scheduler (str): Type of scheduler to use ('linear' or 'cosine').\n",
    "             - cfg.num_warmup_steps (int): Number of warmup steps for the scheduler.\n",
    "             - cfg.num_cycles (float, optional): Number of cycles for the cosine scheduler. Required if cfg.scheduler is 'cosine'.\n",
    "        optimizer: The optimizer for which the scheduler is being created.\n",
    "        num_train_steps (int): Total number of training steps.\n",
    "\n",
    "    Returns:\n",
    "        A learning rate scheduler configured as per the provided configuration.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the scheduler type specified in cfg.scheduler is not supported.\n",
    "    \"\"\"\n",
    "    if cfg.scheduler == 'linear':\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "        )\n",
    "    elif cfg.scheduler == 'cosine':\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported scheduler type provided.\")\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f4ea20",
   "metadata": {
    "papermill": {
     "duration": 0.047562,
     "end_time": "2024-03-01T13:16:24.262920",
     "exception": false,
     "start_time": "2024-03-01T13:16:24.215358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color: #7b6b59;\">Concept 4: Automatic Mixed Precision (AMP)</span>\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "Mixed-precision training is one of the essential techniques that lets us significantly boost training speeds on modern GPUs. Sometimes, this can results in 2x to 3x speed-ups! Let’s see how this works.\n",
    "\n",
    "**Using 32-Bit Precision**\n",
    "\n",
    "When training deep neural networks on a GPU, we typically use a lower-than-maximum precision, namely, 32-bit floating point operations (in fact, PyTorch uses 32-bit floats by default).\n",
    "\n",
    "In contrast, in conventional scientific computing, we typically use 64-bit floats. In general, a larger number of bits corresponds to a higher precision, which lowers the chance of errors accumulating during computations. As a result, 64-bit floating point numbers (also known as double-precision) have long been the standard in scientific computing due to their ability to represent a wide range of numbers with higher accuracy.\n",
    "\n",
    "However, in deep learning, using 64-bit floating point operations is considered unnecessary and computationally expensive since 64-bit operations are generally more costly, and GPU hardware is also not optimized for 64-bit precision. So instead, 32-bit floating point operations (also known as single-precision) have become the standard for training deep neural networks on GPUs.\n",
    "\n",
    "In the context of floating-point numbers, “bits” refer to the binary digits used to represent a number in a computer’s memory. The more bits used to represent a number, the higher the precision and the greater the range of values that can be represented. In floating-point representation, numbers are stored in a combination of three parts: the sign, the exponent, and the significand (or mantissa).\n",
    "\n",
    "<img width=\"862\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/c6cbd706-c038-45e7-b103-75ce2130b4c3\">\n",
    "\n",
    "So, coming back to the motivation behind using a lower precision, there are essentially two main reasons why 32-bit floating point operations are preferred over 64-bit when training deep neural networks on a GPU:\n",
    "\n",
    "1. Reduced memory footprint. One of the primary advantages of using 32-bit floats is that they require half the memory compared to 64-bit floats. This allows for more efficient use of GPU memory, enabling the training of larger models (and larger batch sizes).\n",
    "\n",
    "1. Increased compute and speed. Since 32-bit floating point operations require less memory, GPUs can process them more quickly, leading to faster training times. This speedup is crucial in deep learning, where training complex models can take days or even weeks.\n",
    "\n",
    "**From 32-Bit to 16-Bit Precision**\n",
    "\n",
    "Now that discussed the benefits of 32-bit floats, can we go even further? Yes, we can! Recently, mixed-precision training has become a common training scheme where we temporarily use 16-bit precision for floating point computation, which often referred to as “half” precision.\n",
    "\n",
    "<img width=\"915\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/9caed63d-b116-497b-b381-4264b81b6c6e\">\n",
    "\n",
    "As shown in the figure above, float16 uses three fewer bits for the exponent and 13 fewer bits for the fractional value.\n",
    "\n",
    "Deep Neural Network training has traditionally relied on IEEE single-precision format, however with mixed precision, you can train with half precision while maintaining the network accuracy achieved with single precision. This technique of using both single- and half-precision representations is referred to as **mixed precision technique**. Mixed precision methods combine the use of different numerical formats in one computational workload. This document describes the application of mixed precision to deep neural network training.\n",
    "\n",
    "- **IEEE single-precision floating point computer numbering format**, is a binary computing format that occupies **4 bytes** (32 bits) in computer memory\n",
    "- In computing, **half precision** is a binary floating-point computer number format that occupies 16 bits in computer memory.\n",
    "\n",
    "There are numerous benefits to using numerical formats with lower precision than 32-bit floating point. First, they require less memory, enabling the training and deployment of larger neural networks. Second, they require less memory bandwidth which speeds up data transfer operations. Third, math operations run much faster in reduced precision, especially on GPUs with **Tensor Core** support for that precision. Mixed precision training achieves all these benefits while ensuring that no task-specific accuracy is lost compared to full precision training. ***It does so by identifying the steps that require full precision and using 32-bit floating point for only those steps while using 16-bit floating point everywhere else.***\n",
    "\n",
    "**Benefits of Mixed precision training**\n",
    "\n",
    "- Speeds up math-intensive operations, such as linear and convolution layers, by using Tensor Cores.\n",
    "- Speeds up memory-limited operations by accessing half the bytes compared to single-precision.\n",
    "- Reduces memory requirements for training models, enabling larger models or larger minibatches.\n",
    "\n",
    "*Nuance Research advances and applies conversational AI technologies to power solutions that redefine how humans and computers interact. The rate of our advances reflects the speed at which we train and assess deep learning models. With Automatic Mixed Precision, we’ve realized a 50% speedup in TensorFlow-based ASR model training without loss of accuracy via a minimal code change. We’re eager to achieve a similar impact in our other deep learning language processing applications.*, Wenxuan Teng, Senior Research Manager, Nuance Communications\n",
    "\n",
    "#### Tensor cores\n",
    "\n",
    "Tensor cores are specialized hardware units designed to accelerate deep learning computations, found in certain NVIDIA GPUs starting from the Volta architecture and beyond (including Turing, Ampere, and newer generations). These cores are specifically optimized for performing mixed-precision arithmetic operations, which are commonly used in machine learning and deep learning tasks.\n",
    "\n",
    "Key Features of Tensor Cores:\n",
    "\n",
    "1. **Mixed-Precision Computing:** Tensor cores are designed to perform operations in mixed precision, primarily using 16-bit floating-point (FP16) or 8-bit integer (INT8) formats for inputs and computations, while accumulating results in a higher precision format like 32-bit floating-point (FP32). This approach helps in speeding up computations without significantly impacting the model's accuracy.\n",
    "\n",
    "1. **Matrix Operations Acceleration:** They are particularly efficient at accelerating matrix multiplication operations, which are at the heart of deep learning computations, especially in neural network training and inference. For example, a common operation performed by tensor cores is the matrix multiply-and-accumulate operation (WMMA - Warp Matrix Multiply-Accumulate).\n",
    "\n",
    "1. **Increased Throughput:** By performing operations in lower precision and leveraging the specialized hardware, tensor cores can significantly increase the throughput of deep learning operations compared to using traditional CUDA cores alone. This results in faster training times and more efficient inference.\n",
    "\n",
    "1. **Energy Efficiency:** Mixed-precision computations are not only faster but also more energy-efficient, which is crucial for scaling up deep learning models and deploying them in power-constrained environments.\n",
    "\n",
    "#### Mixed-Precision Training Mechanics\n",
    "\n",
    "**The use of both FP16 and FP32 is the reason this technique is called mixed-precision training.**\n",
    "\n",
    "**Mixed precision** training offers significant computational speedup by performing operations in half-precision format, while storing minimal information in single-precision to retain as much information as possible in critical parts of the network. Since the introduction of Tensor Cores in the Volta and Turing architectures, significant training speedups are experienced by switching to mixed precision -- up to 3x overall speedup on the most arithmetically intense model architectures. Using mixed precision training requires two steps (Enabling mixed precision involves two steps):\n",
    "\n",
    "1. Porting the model to use the FP16 data type where appropriate.\n",
    "1. Adding loss scaling to preserve small gradient values.\n",
    "\n",
    "The ability to train deep learning networks with lower precision was introduced in the Pascal architecture and first supported in CUDA 8 in the NVIDIA Deep Learning SDK.\n",
    "\n",
    "Deep learning researchers and engineers can easily get started enabling this feature on **Ampere**, **Volta** and **Turing** GPUs. On Ampere GPUs, automatic mixed precision uses FP16 to deliver a performance boost of 3X versus TF32, the new format which is already ~6x faster than FP32. On Volta and Turing GPUs, automatic mixed precision delivers up to 3X higher performance vs FP32 with just a few lines of code.\n",
    "\n",
    "<img width=\"1121\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/ec6bbedf-82e8-43e2-bf0f-a3d0d3b2ee72\">\n",
    "\n",
    "\n",
    "Mixed precision is the combined use of different numerical precisions in a computational method.\n",
    "\n",
    "- **Half precision (also known as FP16)** data compared to higher precision FP32 vs FP64 reduces memory usage of the neural network, allowing training and deployment of larger networks, and FP16 data transfers take less time than FP32 or FP64 transfers.\n",
    "\n",
    "- **Single precision (also known as 32-bit)** is a common floating point format (float in C-derived programming languages), and 64-bit, known as double precision (double). \n",
    "\n",
    "Deep Neural Networks (DNNs) have led to breakthroughs in a number of areas, including:\n",
    "\n",
    "    - image processing and understanding\n",
    "    - language modeling\n",
    "    - language translation\n",
    "    - speech processing\n",
    "    - game playing, and many others.\n",
    "\n",
    "DNN complexity has been increasing to achieve these results, which in turn has increased the computational resources required to train these networks. One way to lower the required resources is to use lower-precision arithmetic, which has the following benefits.\n",
    "\n",
    "1. **Decrease the required amount of memory.** Half-precision floating point format (FP16) uses 16 bits, compared to 32 bits for single precision (FP32). Lowering the required memory enables training of larger models or training with larger mini-batches.\n",
    "\n",
    "1. **Shorten the training or inference time.** Execution time can be sensitive to memory or arithmetic bandwidth. Half-precision halves the number of bytes accessed, thus reducing the time spent in memory-limited layers. NVIDIA GPUs offer up to 8x more half precision arithmetic throughput when compared to single-precision, thus speeding up math-limited layers.\n",
    "\n",
    "*Figure 1. Training curves for the bigLSTM English language model shows the benefits of the mixed-precision training techniques. The Y-axis is training loss. Mixed precision without loss scaling (grey) diverges after a while, whereas mixed precision with loss scaling (green) matches the single precision model (black).*\n",
    "\n",
    "<img width=\"891\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/4ccef941-d4fb-4c3b-8f72-0e602c729743\">\n",
    "\n",
    "**Since DNN training has traditionally relied on IEEE single-precision format, this guide will focus on how to train with half precision while maintaining the network accuracy achieved with single precision (as Figure 1). This technique is called mixed-precision training since it uses both single and half-precision representations.**\n",
    "\n",
    "It’s called “mixed-” rather than “low-“precision training because we don’t transfer all parameters and operations to 16-bit floats. Instead, we switch between 32-bit and 16-bit operations during training, hence, the term “mixed” precision.\n",
    "\n",
    "As illustrated in the figure below, mixed-precision training involves converting weights to lower-precision (FP16) for faster computation, calculating gradients, converting gradients back to higher-precision (FP32) for numerical stability, and updating the original weights with the scaled gradients.\n",
    "\n",
    "This approach allows for efficient training while maintaining the accuracy and stability of the neural network.\n",
    "\n",
    "<img width=\"930\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/d979230c-a7fc-4ad4-aab0-3363d81e1c8b\">\n",
    "\n",
    "To combat this, a master copy of the weights is stored in FP32. This is converted into FP16 during part of each training iteration (one forward pass, back-propagation and weight update). At the end of the iteration, the weight gradients are used to update the master weights during the optimizer step.\n",
    "\n",
    "<img width=\"818\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/3eb8e5ce-2608-406c-b934-6fe6f33bcede\">\n",
    "\n",
    "\n",
    "In more detail, the steps are as follows.\n",
    "\n",
    "1. **Convert weights to FP16:** In this step, the weights (or parameters) of the neural network, which are initially in FP32 format, are converted to lower-precision FP16 format. This reduces the memory footprint and allows for faster computation, as FP16 operations require less memory and can be processed more quickly by the hardware.\n",
    "\n",
    "1. **Compute gradients:** The forward and backward passes of the neural network are performed using the lower-precision FP16 weights. This step calculates the gradients (partial derivatives) of the loss function with respect to the network’s weights, which are used to update the weights during the optimization process.\n",
    "\n",
    "1. **Gradient Scaling: `torch.cuda.amp.GradScaler`** If the forward pass for a particular op has float16 inputs, the backward pass for that op will produce float16 gradients. Gradient values with small magnitudes may not be representable in float16. These values will flush to zero (“underflow”), so the update for the corresponding parameters will be lost. To prevent underflow, “gradient scaling” multiplies the network’s loss(es) by a scale factor and invokes a backward pass on the scaled loss(es). Gradients flowing backward through the network are then scaled by the same factor. In other words, gradient values have a larger magnitude, so they don’t flush to zero. Each parameter’s gradient (.grad attribute) should be unscaled before the optimizer updates the parameters, so the scale factor does not interfere with the learning rate. An instance scaler of GradScaler. Helps perform the steps of gradient scaling conveniently.As was shown in the previous section, successfully training some networks requires gradient value scaling to keep them from becoming zeros in FP16. This can be achieved with a single multiplication. You can scale the loss values computed in the forward pass, before starting backpropagation. By the chain rule, backpropagation ensures that all the gradient values of the same amount are scaled. This requires no extra operations during backpropagation and keeps the relevant gradient values from becoming zeros and losing that gradient information. Weight gradients must be unscaled before weight update, to maintain the magnitude of updates the same as in FP32 training. It is simplest to perform this descaling right after the backward pass but before gradient clipping or any other gradient-related computations. This ensures that no hyperparameters (such as gradient clipping threshold, weight decay, etc.) have to be adjusted. While many networks match FP32 training results when all tensors are stored in FP16, some require updating an FP32 copy of weights. Furthermore, values computed by large reductions should be left in FP32. Examples of this include statistics (mean and variance) computed by batch-normalization, SoftMax. Batch-normalization can still take FP16 inputs and outputs, saving half the bandwidth compared to FP32, it’s just that the statistics and value adjustment should be done in FP32. This leads to the following high-level procedure for training:\n",
    "\n",
    "    - `scaler.scale(loss)` multiplies a given loss by scaler’s current scale factor. Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
    "    - `scaler.step(optimizer)` safely unscales gradients and calls `optimizer.step()`. `scaler.step()` first unscales gradients of the optimizer's params. If gradients don't contain infs/NaNs, `optimizer.step()` is then called, otherwise, `optimizer.step()` is skipped.\n",
    "    - Updates the scale for next iteration. `scaler.update()` scaler dynamically estimates the scale factor each iteration. To minimize gradient underflow, a large scale factor should be used. However, float16 values can “overflow” (become inf or NaN) if the scale factor is too large. Therefore, the optimal scale factor is the largest factor that can be used without incurring inf or NaN gradient values. scaler approximates the optimal scale factor over time by checking the gradients for infs and NaNs during every scaler.step(optimizer)\n",
    "\n",
    "1. **Convert gradients to FP32:** After computing the gradients in FP16, they are converted back to the higher-precision FP32 format. This conversion is essential for maintaining numerical stability and avoiding issues such as vanishing or exploding gradients that can occur when using lower-precision arithmetic.\n",
    "\n",
    "1. **Multiply by learning rate and update weights:** Now in FP32 format, the gradients are multiplied by a learning rate (a scalar value that determines the step size during optimization). Here, we can see the benefit of keeping the FP32 copy of the weights. As the learning rate is often small, when multiplied by the weight gradients they can often be tiny values. For FP16, any number with magnitude smaller than 2^(-24) will be equated to zero as it cannot be represented (this is the denormalized limit for FP16). Therefore, by completing the updates in FP32, these update values can be preserved.\n",
    "\n",
    "1. The product from step 4 is then used to update the original FP32 neural network weights. The learning rate helps control the convergence of the optimization process and is crucial for achieving good performance.\n",
    "\n",
    "Sum up:\n",
    "\n",
    "1. Maintain a primary copy of weights in FP32.\n",
    "1. For each iteration:\n",
    "    1. Make an FP16 copy of the weights.\n",
    "    1. Forward propagation (FP16 weights and activations).\n",
    "    1. Multiply the resulting loss with the scaling factor S.\n",
    "    1. Backward propagation (FP16 weights, activations, and their gradients).\n",
    "    1. Multiply the weight gradient with 1/S.\n",
    "    1. Complete the weight update (including gradient clipping, etc.).\n",
    "\n",
    "The above procedure sounds quite complicated, but in practice, it’s pretty simple to implement. In the next section, we will see how we can use mixed-precision training for finetuning an LLM by changing just one line of code.\n",
    "\n",
    "#### Automatic Mixed Precision\n",
    "Using mixed precision training requires three steps:\n",
    "\n",
    "1. Converting the model to use the float16 data type where possible.\n",
    "1. Keeping float32 master weights to accumulate per-iteration weight updates.\n",
    "1. Using loss scaling to preserve small gradient values.\n",
    "\n",
    "Frameworks that support fully automated mixed precision training also support:\n",
    "\n",
    "1. Automatic loss scaling and master weights integrated into optimizer classes\n",
    "1. Automatic casting between float16 and float32 to maximize speed while ensuring no loss in task-specific accuracy\n",
    "\n",
    "In those frameworks with automatic support, using mixed precision can be as simple as adding one line of code or enabling a single environment variable. Currently, the frameworks with support for automatic mixed precision are TensorFlow, PyTorch, and MXNet. Refer to NVIDIA Automatic Mixed Precision for Deep Learning for more information, along with the Frameworks section below.\n",
    "\n",
    "\n",
    "#### Automatic Mixed Precision (AMP) with PyTorch\n",
    "\n",
    "Provides the `torch.amp` module for seamless integration of mixed precision training. Automatic Mixed Precision feature is available in the Apex repository on GitHub. To enable, add these two lines of code into your existing training script:\n",
    "\n",
    "```python\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "with autocast():\n",
    "    output = model(input)\n",
    "    loss = loss_fn(output, target)\n",
    "\n",
    "scaler.scale(loss).backward()\n",
    "\n",
    "scaler.step(optimizer)\n",
    "\n",
    "scaler.update()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed2119d",
   "metadata": {
    "papermill": {
     "duration": 0.047579,
     "end_time": "2024-03-01T13:16:24.357729",
     "exception": false,
     "start_time": "2024-03-01T13:16:24.310150",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color: #7b6b59;\">Concept 5: Gradient Accumulation</span>\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "Learn how to use gradient accumulation to train models with large batch sizes in order to work around hardware limitations when GPU memory is a concern. Since we don’t have multiple GPUs available for tensor sharding, what can we do to train the model with larger batch sizes? **One workaround is gradient accumulation, where we modify the training loop.**\n",
    "\n",
    "Gradient accumulation is a technique used to effectively increase the batch size for training deep learning models without requiring additional memory. It's particularly useful when the desired batch size cannot fit into the GPU's memory. Instead of updating model weights after each small batch, gradients from several batches are accumulated before a single update is made. This simulates the effect of a larger batch size. This technique allows for training with larger effective batch sizes than what might fit in GPU memory at once. By accumulating gradients over multiple mini-batches and only updating model weights after a specified number of steps, it simulates the effect of a larger batch size.\n",
    "\n",
    "#### What is gradient accumulation?\n",
    "\n",
    "Gradient accumulation is a way to virtually increase the batch size during training, which is very useful when the available GPU memory is insufficient to accommodate the desired batch size. In gradient accumulation, gradients are computed for smaller batches and accumulated **(usually summed or averaged)** over multiple iterations instead of updating the model weights after every batch. Once the accumulated gradients reach the target “virtual” batch size, the model weights are updated with the accumulated gradients.\n",
    "\n",
    "If we set accumulation_steps to 2, then `zero_grad()` and `optimizer.step()` will only be called every second epoch. Consequently, running the modified training loop with `accumulation_steps=`2 will have the same effect as doubling the batch size.\n",
    "\n",
    "For example, if we want to use a batch size of 256 but can only fit a batch size of 64 into GPU memory, we can perform gradient accumulation over four batches of size 64. (After processing all four batches, we will have the accumulated gradients equivalent to a single batch of size 256.) This allows us to effectively emulate a larger batch size without requiring larger GPU memory or tensor sharding across different devices.\n",
    "\n",
    "While gradient accumulation can help us train models with larger batch sizes, it does not reduce the total computation required. In fact, it can sometimes lead to a slightly slower training process, as the weight updates are performed less frequently. Nevertheless, it allows us to work around limitations where we have very small batch sizes that lead to noisy updates.\n",
    "\n",
    "1. The optimizer step (updating model weights) and resetting the gradients (`optimizer.zero_grad()`) are performed only after a specified number of batches have been processed, as indicated by `CFG.gradient_accumulation_steps`. This is where the actual accumulation happens: gradients computed on each batch are added up across multiple batches. In this snippet:\n",
    "\n",
    "    - `scaler.step(optimizer)` is called to adjust the gradients and perform the optimizer step, but only after CFG.gradient_accumulation_steps batches have been processed. This is when the accumulated gradients are finally used to update the model parameters.\n",
    "    - `scaler.update()` prepares the scaler for the next accumulation cycle.\n",
    "    - `optimizer.zero_grad()` resets the gradients in the model parameters, ensuring that accumulation starts fresh for the next set of batches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed003d",
   "metadata": {
    "papermill": {
     "duration": 0.047819,
     "end_time": "2024-03-01T13:16:24.453831",
     "exception": false,
     "start_time": "2024-03-01T13:16:24.406012",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- `model.model.named_parameters()`: The use of `model.model.named_parameters()` suggests that the model (model) contains a nested model or a submodule (model.model). This is common in complex architectures where a model might encapsulate another model as one of its components. The first two groups are specifically targeting parameters of this nested model. By calling `named_parameters()` on `model.model`, the code is accessing parameters that are part of this inner model. Consider a scenario where model is a wrapper that contains a pre-trained model like BERT or ResNet as `model.model`. In this case, the first two groups are meant to configure parameters of that pre-trained component.\n",
    "    - ***First Group - Parameters of the Pretrained Model with Weight Decay***\n",
    "    - ***Second Group - Parameters of the Pretrained Model without Weight Decay***\n",
    "- `model.named_parameters()`: The third group uses `model.named_parameters()`. This targets parameters that are directly part of the model object, but not part of the nested `model.model`. This might be used in a situation where the outer model has its own layers or components in addition to the nested `model.model`. For example, if model includes some custom layers or a different decoder mechanism on top of the pre-trained `model.model`, this group would be configuring those additional components.\n",
    "    - ***Third Group - Custom Layers' Parameters, Typically Without Weight Decay***\n",
    "    \n",
    "- In summary, the code distinguishes between parameters of a nested model (handled in the first two groups) and parameters that are part of the outer model but not part of the nested model (handled in the third group). This distinction is crucial for applying different training configurations (like learning rates and weight decay settings) to different parts of a complex model architecture.\n",
    "\n",
    "- **Why Differentiate:** Differentiating these groups allows for more nuanced control over the training process. For instance, we might want to use different learning rates or apply weight decay differently to the pretrained model's parameters versus the custom layers added to CustomModel.\n",
    "\n",
    "- **Weight Decay Considerations:** Typically, weight decay is not applied to biases and normalization layers (like LayerNorm) because these parameters are not directly associated with the magnitude of the activations and thus don’t contribute as much to overfitting.\n",
    "\n",
    "\n",
    "The decision to not apply weight decay to custom layers' parameters in a deep learning model is typically a strategic choice, influenced by several factors rather than being just a random decision. Here are some key reasons and considerations:\n",
    "\n",
    "1. **Nature of the Custom Layers**\n",
    "    - **Simplicity and Scale:** Custom layers added to a pretrained model are often simpler and have fewer parameters compared to the complex structures within the pretrained model. Since weight decay primarily targets large weights that can contribute to overfitting, its impact might be less significant on smaller, simpler custom layers.\n",
    "    - **Specific Roles:** These layers often serve specific roles (like adaptation, transformation, or output formatting) that may not require aggressive regularization. Over-regularizing could hamper their ability to perform these roles effectively.\n",
    "2. **Pretrained Model Characteristics**\n",
    "\n",
    "    - **Already Regularized:** Pretrained models (like those from AutoModel) are usually trained on large datasets and have already undergone extensive regularization. Additional weight decay on these parts helps in fine-tuning without overfitting.\n",
    "    - **Complexity and Overfitting:** These models have a high capacity and are more prone to overfitting when adapted to a new task or dataset. Weight decay in these layers is crucial to maintain their generalization ability.\n",
    "3. **Fine-Tuning Dynamics**\n",
    "\n",
    "    - **Different Learning Rates:** Often, custom layers are trained with a higher learning rate because they start from random initialization, unlike the pretrained layers. Applying weight decay with a higher learning rate to these layers might lead to instability or hinder their rapid adaptation to the new task.\n",
    "    - **Balancing Adaptation and Stability:** The goal in fine-tuning is often to slightly adjust the pretrained layers while more significantly training the custom layers to adapt to the new task. Weight decay on custom layers might counteract this adaptation process.\n",
    "\n",
    "4. **Empirical Results and Task-Specific Factors:**\n",
    "    - **Empirical Observations:** In many cases, the decision is backed by empirical testing. Models might be trained with various configurations, and the setup that yields the best results on validation data is chosen.\n",
    "\n",
    "    - **Task Dependency:** The necessity of weight decay on custom layers can vary depending on the specific task and data. For some tasks, applying weight decay to all layers might yield better results.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "The choice to exclude weight decay from custom layers is often a considered decision based on the model's architecture, the nature of the task, and empirical results. It strikes a balance between allowing the custom layers to learn effectively from the new data and regulating the pretrained layers to leverage their learned representations without overfitting. This approach is not universal and can vary based on the specific requirements and observations of different models and tasks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f349aa8b",
   "metadata": {
    "papermill": {
     "duration": 0.047388,
     "end_time": "2024-03-01T13:16:24.548788",
     "exception": false,
     "start_time": "2024-03-01T13:16:24.501400",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color: #7b6b59;\">Concept 6: PyTorch Training Loop</span>\n",
    "\n",
    "**Writing the Custom Training Loop from Scratch**\n",
    "\n",
    "Unlike TensorFlow, PyTorch requires us to write a training loop in pure Python. After specifying the custom model architecture, we instantiated it and defined a loss function and an optimization algorithm to train our network. We have chosen Cross Entropy Loss as the loss function to backpropagate the errors (as we have a multi-class single-label problem), and Adam optimizer as the optimization algorithm. Taking all these components together, we will now write a custom training loop from scratch. Finally, we will write a simplistic and clean version of our training routine using PyTorch's API support for optimization called `torch.optim`.\n",
    "\n",
    "- **Optimizer:** Optimization is the process of adjusting model parameters to reduce model error in each training step. Optimization algorithms define how this process is performed (in this example we use Stochastic Gradient Descent). All optimization logic is encapsulated in the optimizer object. Here, we use the ADAM optimizer; additionally, there are many different optimizers available in PyTorch such as SGD and RMSProp, that work better for different kinds of models and data. We initialize the optimizer by registering the model’s parameters that need to be trained, and passing in the learning rate hyperparameter. `optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)`\n",
    "- **16-bit precision, Automatic Mixed Precision:** In a regular training loop, PyTorch stores all float variables in 32-bit precision. For people who are training their models with strict constraints, sometimes, this can cause their model to take up too much memory, forcing them to have a slower training process with a smaller model and a smaller batch size. However, storing all the variables/numbers in the model in 16-bit precision can improve upon and fix most of these problems, like dramatically decreasing the memory consumption of the model and speeding up the training loop while still maintaining the same performance/accuracy of the model. Converting all calculations to 16-bit precision in Pytorch is very simple to do and only requires a few lines of code. When you are doing backward propagation with loss and the optimizer, instead of doing loss.backward() and optimizer.step(), you need to do `scaler.scale(loss).backward` and `scaler.step(optimizer)`. This allows your scaler to convert all the gradients and do all the calculations in 16-bit precision. When you are doing everything with 16-bit precision, there may be some numerical instability that causes some functions that you may use to not work properly. Only certain operations work correctly in 16-bit precision. One common error in any large deep learning model is the problem of underflowing gradients (i.e., your gradients are too small to take into account). float16 tensors often don't take into account extremely small variations. To prevent this, we can scale our gradients by some factor, so they aren't flushed to zero. Not to be confused with vanishing gradients, these gradients might contribute to the learning process but are skipped because of computational limits.\n",
    "- **Gradient Accumulation:** If you run into a CUDA out of memory error, this means that you have exceeded your computational resources. To fix this, there are several things you can do, including converting everything to 16-bit precision as I mentioned above, reducing the batch size of your model, and reducing the num_workers parameter when creating your Dataloaders. However, sometimes, switching to 16-bit precision and reducing num_workers may not completely fix the problem. The most direct way to fix the problem is to reduce your batch size, but suppose that you don’t want to reduce your batch size. If you don’t want to reduce your batch size, you can use gradient accumulation to stimulate your desired batch size. Note that another solution to the CUDA out of memory issue is simply to use more than one GPU, but this is an option not accessible to many people. Suppose that your machine/model can only support a batch size of 16 and increasing it results in a CUDA out of memory error, and you want to have a batch size of 32. Gradient accumulation works by running the model with a batch size of 16 twice, accumulating the gradients computed for each batch, and finally doing an optimizer step after those 2 forward passes and accumulation of gradients. To understand gradient accumulation, it is important to understand what specific functions are done in training a neural network. \n",
    "    - `loss.backward()` creates and stores the gradients for the model. Calling loss.backward() twice before calling optimizer accumulates the gradients. \n",
    "    - but `optimizer.step()` actually updates the weights. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let’s see the procedure step by step:\n",
    "\n",
    "1. **Step 1:** We instantiate the model and the optimizer\n",
    "1. **Step 2:** We decide on a number of epochs\n",
    "1. **Step 3:** We create a for loop that iterates through the epochs\n",
    "1. **Step 4:** For each epoch, we set the model to training mode with `model.train()` and cycle through the train_loader. For each epoch, we open a for loop that iterates over the dataset, in batches. Gets a batch of training data from the DataLoader. \n",
    "1. **Step 5:** For each batch of the train_loader, we call the model on the input data to retrive the predictions, then we use them to compute a loss value. Performs an inference - that is, gets predictions from the model for an input batch. Calculates the loss for that set of predictions vs. the labels on the dataset.\n",
    "1. **Step 6:** Bring the calculation of the derivatives to 0 with `optimizer.zero_grad()`. Tensors, variables, optimizers are all interconnected to one another via hidden global state. Also, don't forget to call model.zero_grad() before loss.backward(), or you won't get the right gradients for your variables. Call `optimizer.zero_grad()` to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration. Zeros the optimizer’s gradients\n",
    "1. **Step 7:** Calculates the backward gradients over the learning weights. Calling `loss.backward()` on a loss tensor triggers backpropagation. PyTorch's automatic differentiation engine called **autograd** keeps track of every operation on the tensors that require gradients, creating a computation graph consisting of all the tensor operations tensors are subjected to. In neural networks, the weight tensors of the parameters are what we want to optimize. These tensors are, by default, the leaf tensors requiring gradient grad. As is explained above, we aim to optimize these model parameters (weights and biases) using some criterion which is called the model loss. The model loss is a function of the model output and the target tensor. The model output, eventually, is a function of all the model parameters. The target tensor is a fixed one and hence has nothing to do with the optimization process regarding adjusting and updating the neural network. Hence, the model parameters are the leaf tensors in the graph of the loss of the neural network, and they have their requires_grad attribute to True. As soon as we call `.backward()` on the loss tensor, a backward graph is constructed. The gradients of the loss (the tensor on which backward is called) concerning the leaf tensors (the model parameters ) are calculated, and their grad attribute of them is populated. **These gradients are what we are required to update our model parameters and what we eventually access using `weight.grad` or `bias.grad` to update the weights and biases, respectively.** Finally, mathematically, the gradients of the loss function concerning the model parameters are calculated, and an optimization algorithm uses these gradients to update the model parameters. Backpropagate the prediction loss with a call to `loss.backward()`. PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
    "1. **Step 8:** Once that's done, your optimizer is magically aware of the gradients for each variable and can update its variables, which is done via `optimizer.step()`. Outside the scope, we retrieve the gradients of the weights of the model with regard to the loss Finally, we use the optimizer to update the weights of the model based on the gradients. Once we have our gradients, we call `optimizer.step()` to adjust the parameters by the gradients collected in the backward pass. Tells the optimizer to perform one learning step - that is, adjust the model’s learning weights based on the observed gradients for this batch, according to the optimization algorithm we chose. \n",
    "\n",
    "\n",
    "At this point the training loop is complete, and if you want you can integrate the same logic on the validation dataloader as written in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1a5f4f54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:24.647695Z",
     "iopub.status.busy": "2024-03-01T13:16:24.647352Z",
     "iopub.status.idle": "2024-03-01T13:16:24.652831Z",
     "shell.execute_reply": "2024-03-01T13:16:24.652078Z"
    },
    "papermill": {
     "duration": 0.057823,
     "end_time": "2024-03-01T13:16:24.654665",
     "exception": false,
     "start_time": "2024-03-01T13:16:24.596842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fca99b8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:24.752663Z",
     "iopub.status.busy": "2024-03-01T13:16:24.751863Z",
     "iopub.status.idle": "2024-03-01T13:16:24.756818Z",
     "shell.execute_reply": "2024-03-01T13:16:24.755956Z"
    },
    "papermill": {
     "duration": 0.055628,
     "end_time": "2024-03-01T13:16:24.758624",
     "exception": false,
     "start_time": "2024-03-01T13:16:24.702996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_score(y_trues, y_preds):\n",
    "    \"\"\"\n",
    "    Calculate the AUC score from true labels and predicted probabilities.\n",
    "\n",
    "    Args:\n",
    "        y_trues (array-like): True binary class labels.\n",
    "        y_preds (array-like): Predicted probabilities for the positive class.\n",
    "\n",
    "    Returns:\n",
    "        float: The AUC score.\n",
    "    \"\"\"\n",
    "    # Calculate AUC score\n",
    "    auc_score = roc_auc_score(y_trues, y_preds)\n",
    "    return auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "10ae6eec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:24.855572Z",
     "iopub.status.busy": "2024-03-01T13:16:24.854825Z",
     "iopub.status.idle": "2024-03-01T13:16:24.861898Z",
     "shell.execute_reply": "2024-03-01T13:16:24.861044Z"
    },
    "papermill": {
     "duration": 0.057517,
     "end_time": "2024-03-01T13:16:24.863797",
     "exception": false,
     "start_time": "2024-03-01T13:16:24.806280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value.\n",
    "\n",
    "    Attributes:\n",
    "        val (float): The current value.\n",
    "        avg (float): The running average.\n",
    "        sum (float): The sum of all values encountered.\n",
    "        count (int): The number of values encountered.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the AverageMeter and resets all attributes.\"\"\"\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets all attributes to their initial state.\"\"\"\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        \"\"\"Updates the running average and current value.\n",
    "\n",
    "        Args:\n",
    "            val (float): The new value to update with.\n",
    "            n (int, optional): The weight of the new value, i.e., how many times to count `val`. Defaults to 1.\n",
    "        \"\"\"\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "58b1e81b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:24.961138Z",
     "iopub.status.busy": "2024-03-01T13:16:24.960827Z",
     "iopub.status.idle": "2024-03-01T13:16:24.967364Z",
     "shell.execute_reply": "2024-03-01T13:16:24.966521Z"
    },
    "papermill": {
     "duration": 0.057445,
     "end_time": "2024-03-01T13:16:24.969251",
     "exception": false,
     "start_time": "2024-03-01T13:16:24.911806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    \"\"\"Converts a time duration from seconds to a string in minute-second format.\n",
    "\n",
    "    Args:\n",
    "        s (float): The time duration in seconds.\n",
    "\n",
    "    Returns:\n",
    "        str: The time duration in 'minute m second s' format.\n",
    "    \"\"\"\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    \"\"\"Calculates elapsed time since a start point and estimates remaining time.\n",
    "\n",
    "    Args:\n",
    "        since (float): The start time (usually obtained from `time.time()`).\n",
    "        percent (float): The progress made as a fraction (between 0 and 1).\n",
    "\n",
    "    Returns:\n",
    "        str: A string indicating elapsed time and estimated remaining time in 'minute m second s' format.\n",
    "    \"\"\"\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / percent  # Estimated total time\n",
    "    rs = es - s  # Remaining time\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b157dfb2",
   "metadata": {
    "papermill": {
     "duration": 0.047515,
     "end_time": "2024-03-01T13:16:25.065658",
     "exception": false,
     "start_time": "2024-03-01T13:16:25.018143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "When you call `model.train()`, you're setting the entire model to training mode. This affects all layers of the model, not just a few. In training mode, certain behaviors are enabled that are suitable for training. Setting the model to training mode ensures that these and potentially other layer-specific behaviors that are relevant during training are correctly applied. If you want to train only certain layers of a model while keeping others frozen (i.e., their parameters are not updated), you would typically set `requires_grad` attribute of the parameters of the layers you want to freeze to `False`. This way, the optimizer won't compute gradients for those parameters, and they won't be updated during the training process. This technique is often used in transfer learning and fine-tuning scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6910bdc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:25.164520Z",
     "iopub.status.busy": "2024-03-01T13:16:25.163946Z",
     "iopub.status.idle": "2024-03-01T13:16:25.176915Z",
     "shell.execute_reply": "2024-03-01T13:16:25.176062Z"
    },
    "papermill": {
     "duration": 0.064604,
     "end_time": "2024-03-01T13:16:25.178753",
     "exception": false,
     "start_time": "2024-03-01T13:16:25.114149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fn(fold: int, \n",
    "             train_loader: DataLoader, \n",
    "             model: torch.nn.Module, \n",
    "             criterion: torch.nn.Module, \n",
    "             optimizer: torch.optim, \n",
    "             epoch: int, \n",
    "             scheduler, \n",
    "             device: torch.device\n",
    "            ) -> float:\n",
    "    \"\"\"Trains the model for one epoch through the entire dataset.\n",
    "\n",
    "    Args:\n",
    "        fold (int): The current fold in cross-validation.\n",
    "        train_loader (DataLoader): The DataLoader for the training data.\n",
    "        model (torch.nn.Module): The model to be trained.\n",
    "        criterion (function): The loss function used for training.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used for training.\n",
    "        epoch (int): The current epoch number.\n",
    "        scheduler (torch.optim.lr_scheduler): The learning rate scheduler.\n",
    "        device (torch.device): The device on which to train the model, 'cuda' or 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        float: The average loss for this training epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    model.train() # We set the model to training mode\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex) # Create a gradient scaler \n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "\n",
    "    for step, (input_ids, attention_mask, labels) in enumerate(train_loader): # We open a for loop that iterates over the dataset, in batches.\n",
    "        \n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            # Compute prediction and loss\n",
    "            # calls the forward, model.forward(inputs) - performs calculations of the network\n",
    "            logits = model(input_ids, attention_mask) # For each batch of the train_loader, we call the model on the input data to retrive the predictions, forward pass\n",
    "            loss = criterion(logits.view(-1), labels.float()) # Then we use them to compute a loss value\n",
    "                    \n",
    "            \n",
    "        \n",
    "        losses.update(loss.item(), batch_size)\n",
    "        \n",
    "        # computes the derivative of the loss tensor w.r.t. the parameters \n",
    "        # using backpropagation and thus populates the `grad` attribute of model parameters\n",
    "        scaler.scale(loss).backward() # backward pass, backpropagate, computes the derivative of the loss w.r.t. the parameters (or anything requiring gradients) using backpropagation.\n",
    "        \n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer) # causes the optimizer to take a step based on the gradients of the parameters\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad() # clears old gradients from the last step (otherwise you’d accumulate the gradients from all loss.backward() calls).\n",
    "           \n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch+1, step, len(train_loader), \n",
    "                          remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "\n",
    "    return losses.avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b67db8f",
   "metadata": {
    "papermill": {
     "duration": 0.047803,
     "end_time": "2024-03-01T13:16:25.275137",
     "exception": false,
     "start_time": "2024-03-01T13:16:25.227334",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. **Initial Setup:** \n",
    "    - The function starts by preparing for validation. It initializes a utility (likely an `AverageMeter`) to keep track of the average loss across all validation batches. \n",
    "    - The model is set to **evaluation mode**, which is crucial for certain layers that behave differently during training and evaluation (like dropout and batch normalization layers).\n",
    "    - An empty list is created to store the model's predictions on the validation data, and the start time of the validation process is noted for potentially measuring the duration of validation.\n",
    "\n",
    "1. **Iterating Over Batches:** The function enters a loop where it processes the validation data in batches. For each batch, it performs the following steps:\n",
    "\n",
    "    - **Data Preparation:** The input data (`input_ids` and `attention_mask`) and `labels` for the current batch are moved to the designated computing device (CPU or GPU).\n",
    "    - **Model Inference:** With **gradient calculation disabled** (to save memory and computation time), the model performs a forward pass on the input data to generate logits. These logits are then converted into probabilities using the sigmoid function, which is typical for binary classification tasks. In PyTorch, the context manager `torch.no_grad()` is used to disable gradient calculation, which effectively means that no training takes place and no backpropagation occurs within its scope. This is useful in situations where you are only performing forward passes, such as during model evaluation or inference, and you want to reduce memory usage and speed up computations by not keeping track of the operations for gradients. Context-manager that disables gradient calculation. Disabling gradient calculation is useful for inference, when you are sure that you will not call `Tensor.backward()`. It will reduce memory consumption for computations that would otherwise have `requires_grad=True`.\n",
    "    - **Loss Calculation:** The loss is calculated by comparing the model's logits against the true labels using a specified loss function (criterion). This loss represents how well the model's predictions match the expected outputs.\n",
    "    - **Tracking Loss and Predictions:** The calculated loss for the batch is recorded using the AverageMeter utility to compute an average over the entire validation set. The probabilities (model predictions) are detached from the computation graph (to prevent memory leaks), transferred to the CPU, and stored in a list.\n",
    "\n",
    "1. **Reporting Progress:** Periodically, or at least after processing the last batch, the function prints a status update. This update includes the current batch number, the total number of batches, the average loss up to that point, and the elapsed time since the start of validation.\n",
    "\n",
    "1. **Finalization:** After all batches have been processed, the function concatenates all batch predictions into a single array. It then returns the average loss across all validation batches and the array of concatenated predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "68eabc38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:25.372913Z",
     "iopub.status.busy": "2024-03-01T13:16:25.372100Z",
     "iopub.status.idle": "2024-03-01T13:16:25.382937Z",
     "shell.execute_reply": "2024-03-01T13:16:25.382197Z"
    },
    "papermill": {
     "duration": 0.061594,
     "end_time": "2024-03-01T13:16:25.384784",
     "exception": false,
     "start_time": "2024-03-01T13:16:25.323190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validation_loop(valid_loader: DataLoader, model: torch.nn.Module, criterion: torch.nn.Module, device: torch.device) -> Tuple[float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Perform the validation loop over the given DataLoader.\n",
    "\n",
    "    Args:\n",
    "        valid_loader (DataLoader): The DataLoader to iterate over for validation.\n",
    "        model (torch.nn.Module): The model to evaluate.\n",
    "        criterion (torch.nn.Module): The loss function to use for evaluation.\n",
    "        device (torch.device): The device to run the validation on, e.g., 'cuda' or 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, np.ndarray]: A tuple containing the average loss over the validation set and the concatenated predictions.\n",
    "    \"\"\"\n",
    "    losses = AverageMeter()  # Initialize an object to track the average loss.\n",
    "    model.eval()  # Set the model to evaluation mode.\n",
    "\n",
    "    preds = []  # List to store predictions.\n",
    "    start = time.time()  # Record the start time.\n",
    "\n",
    "    for step, (input_ids, attention_mask, labels) in enumerate(valid_loader):  # Iterate over batches.\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculation for validation.\n",
    "            logits = model(input_ids, attention_mask)  # Forward pass to get logits.\n",
    "            probabilities = torch.sigmoid(logits)  # Convert logits to probabilities using sigmoid.\n",
    "            loss = criterion(logits.view(-1), labels.float())  # Compute loss.\n",
    "\n",
    "        losses.update(loss.item(), batch_size)  # Update the average loss.\n",
    "        preds.append(probabilities.detach().cpu().numpy())  # Store predictions, detach from graph and move to CPU.\n",
    "        end = time.time()  # Record the end time.\n",
    "\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):  # Print progress.\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
    "    \n",
    "    predictions = np.concatenate(preds)  # Concatenate all batch predictions.\n",
    "    return losses.avg, predictions  # Return average loss and predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf59b81",
   "metadata": {
    "papermill": {
     "duration": 0.047911,
     "end_time": "2024-03-01T13:16:25.481516",
     "exception": false,
     "start_time": "2024-03-01T13:16:25.433605",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color: #7b6b59;\">Concept 7: Loss functions</span>\n",
    "\n",
    "Loss functions are an important component of a neural network. Interfacing between the forward and backward pass within a Deep Learning model, they effectively compute how poor a model performs (how big its loss) is. In this section, we're going to cover how to use a variety of PyTorch loss functions for classification.\n",
    "\n",
    "#### What is a loss function?\n",
    "\n",
    "Training a Deep Learning model involves what I call a high-level training process. This process is visualized below. It all starts with a training dataset, which - in the case of classification and regression - contains a set of descriptive variables (features) that jointly are capable of predicting some target variable.\n",
    "\n",
    "Training the Deep Learning model, which often is a neural network, involves sequentially performing a forward pass and a backward pass, followed by optimization. In the forward pass, the dataset is fed to the network (in a batched fashion). This leads to predictions for the targets, which can then be compared with the true labels. No prediction is perfect, and hence there will be an error value. Using this error value, the error can be computed backwards into the neural network using backpropagation. Subsequently, with an optimizer, the model can be changed slightly in the hope that it performs better next time. By repeating this process over and over again, the model can improve and learn to generate accurate predictions.\n",
    "\n",
    "Let's get back to this error value. As the name suggests, it is used to illustrate how poorly the model performs. In Deep Learning jargon, this value is also called a loss value. It is computed by means of a loss function. There are many functions that can be used for this purpose. Choosing one depends on:\n",
    "\n",
    "- the problem you are solving (i.e. classification or regression), \n",
    "- the characteristics of your dataset, \n",
    "- and quite frequently on trial and error. \n",
    "\n",
    "In the rest of this section, we're going to walk through a lot of loss functions available in PyTorch. Let's take a look!\n",
    "\n",
    "<img width=\"1420\" alt=\"image\" src=\"https://github.com/eraikakou/ml-theory/assets/28102493/877f6cdf-ea71-4ec0-a5b4-10109fe96b43\">\n",
    "\n",
    "\n",
    "#### PyTorch Classification loss function examples\n",
    "\n",
    "1. **Binary Cross-entropy loss, on Sigmoid (`nn.BCELoss`) example:** Binary cross-entropy loss is a good candidate for binary classification problems, where a classifier has two classes. Implementing binary cross-entropy loss with PyTorch is easy. It involves the following steps:\n",
    "    - Ensuring that the output of your neural network is a value between 0 and 1. **Recall that the Sigmoid activation function can be used for this purpose.** This is why we apply `nn.Sigmoid()` in our neural network below.\n",
    " - Ensuring that you use `nn.BCELoss()` as your loss function of choice during the training loop.\n",
    "\n",
    "1. **Binary Cross-entropy loss, on logits (`nn.BCEWithLogitsLoss`):** Simple binary cross-entropy loss (represented by `nn.BCELoss` in PyTorch) computes BCE loss on the predictions generated in the range [0, 1]. **However, it is possible to generate more numerically stable variant of binary cross-entropy loss by combining the Sigmoid and the BCE Loss into one loss function:** \"This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.\" In PyTorch, this is combined into the **`nn.BCEWithLogitsLoss`** function. The difference between `nn.BCEWithLogitsLoss` and `nn.BCELoss` is that BCE with Logits loss adds the Sigmoid function into the loss function. With simple BCE Loss, you will have to add Sigmoid to the neural network, whereas with BCE With Logits Loss you will not.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2405487e",
   "metadata": {
    "papermill": {
     "duration": 0.047869,
     "end_time": "2024-03-01T13:16:25.577429",
     "exception": false,
     "start_time": "2024-03-01T13:16:25.529560",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color: #7b6b59;\">Concept 8: Other Topics</span>\n",
    "\n",
    "\n",
    "#### Scheduler Step\n",
    "\n",
    "The `scheduler.step()` function is typically called to update the learning rate according to a specific policy defined by the learning rate scheduler you're using. Learning rate schedulers adjust the learning rate during training, which can lead to faster convergence and/or improved performance of the model.\n",
    "\n",
    "- **In epoch-wise learning rate scheduling,** `scheduler.step()` is often called at the end of each epoch. This is common with schedulers like `StepLR`, where the learning rate is decreased by a certain factor after a specified number of epochs, or `MultiStepLR`, where the learning rate is reduced at specific epochs.\n",
    "\n",
    "- **In batch-wise (or iteration-wise) learning rate scheduling,** `scheduler.step()` is called after each batch or iteration. This approach is used with schedulers like `OneCycleLR` or `CyclicLR`, which adjust the learning rate more frequently to allow for strategies like cyclic learning rates or learning rate warm-up followed by decay.\n",
    "\n",
    "**Batch Scheduler**\n",
    "\n",
    "The term \"batch scheduler\" as mentioned with if `CFG.batch_scheduler`: suggests a configuration option that, when enabled, indicates that the learning rate scheduler should be updated on a per-batch basis instead of the more common per-epoch basis. This means `scheduler.step()` is called after processing each batch instead of at the end of each epoch. This approach is used in certain training regimes where adjusting the learning rate more granularly can lead to better performance or faster convergence.\n",
    "\n",
    "When using a batch scheduler, it's important to ensure that the scheduler and the training loop are correctly configured to adjust the learning rate as intended after each batch. This often involves careful consideration of the total number of steps (batches) in the training process, especially when using learning rate schedulers designed with epoch-wise adjustments in mind.\n",
    "\n",
    "\n",
    "#### Gradient Checkpointing\n",
    "\n",
    "Even when we set the batch size to 1 and use gradient accumulation we can still run out of memory when working with large models. In order to compute the gradients during the backward pass all activations from the forward pass are normally saved. This can create a big memory overhead. Alternatively, one could forget all activations during the forward pass and recompute them on demand during the backward pass. This would however add a significant computational overhead and slow down training.\n",
    "\n",
    "Gradient checkpointing strikes a compromise between the two approaches and saves strategically selected activations throughout the computational graph so only a fraction of the activations need to be re-computed for the gradients. See [this great article](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9) explaining the ideas behind gradient checkpointing. One way to use significantly less GPU memory is to enabled “Gradient Checkpointing” (also known as “activation checkpointing”). When enabled, a lot of memory can be freed at the cost of small decrease in the training speed due to recomputing parts of the graph during back-propagation. The slowdown will depend on the model but quite often it is around 20-30%.\n",
    "\n",
    "\n",
    "Gradient checkpointing is a technique used to reduce the memory footprint of training deep neural networks at the cost of additional computation. It allows for the training of very deep networks that would otherwise not fit into the GPU memory by trading off computational overhead for memory efficiency.\n",
    "\n",
    "**How Gradient Checkpointing Works**\n",
    "\n",
    "The basic idea behind gradient checkpointing is to store only a subset of intermediate activations during the forward pass and then recompute the other activations during the backward pass as needed for gradient computation. This is opposed to the standard approach where all intermediate activations are stored during the forward pass to be used in the backward pass, which can consume a significant amount of memory for deep networks.\n",
    "\n",
    "When gradient checkpointing is enabled:\n",
    "\n",
    "- During the forward pass, only the activations at certain \"checkpoint\" layers are saved.\n",
    "- During the backward pass, when the gradient of a layer is needed, the network re-computes the forward pass from the nearest preceding checkpoint up to that layer. This requires additional computation because some of the forward pass computations are done multiple times.\n",
    "\n",
    "\n",
    "**Why Use Gradient Checkpointing**\n",
    "\n",
    "The primary reason to use gradient checkpointing is to enable the training of larger models or to use larger batch sizes within the memory constraints of the hardware. This can be particularly useful when working with very deep networks, such as those used in certain domains like deep learning for computer vision or large transformer models in natural language processing.\n",
    "\n",
    "**Considerations**\n",
    "\n",
    "- Trade-off: The main trade-off with gradient checkpointing is between memory usage and computational overhead. While it significantly reduces memory usage, it also increases the computational burden because of the need to recompute activations during the backward pass.\n",
    "\n",
    "- Implementation: Modern deep learning frameworks like PyTorch and TensorFlow offer built-in support or extensions for gradient checkpointing, making it easier to apply this technique without extensive modifications to the model code.\n",
    "\n",
    "- Use Cases: It's particularly useful in scenarios where memory is a limiting factor and where the additional computation time is acceptable. For example, when training very large models or when using particularly memory-intensive operations.\n",
    "\n",
    "In summary, gradient checkpointing is a valuable technique for training larger models within memory constraints by efficiently managing the trade-off between memory usage and computational cost.\n",
    "\n",
    "\n",
    "#### Logits\n",
    "Logits are the outputs of a neural network before the activation function is applied. They are the unnormalized probabilities of the item belonging to a certain class. Logits are often used in classification tasks, where the goal is to predict the class label of an input. **Converting the logits to probabilities makes understanding the neural network's final output easier.**\n",
    "\n",
    "Logits typically refer to the raw, unnormalized outputs of the last layer of a neural network, just before the application of a softmax function (or another type of normalization that converts them into probabilities). In the context of classification tasks, logits are the outputs of the final linear layer of a neural network, and they represent the input to the softmax function.\n",
    "\n",
    "**Understanding Logits in Detail:**\n",
    "\n",
    "1. **For Binary Classification:** In a binary classification problem, a single logit can be output by the model, which represents the log-odds of the positive class. The **sigmoid function** is then applied to this logit to obtain the probability of the positive class.\n",
    "\n",
    "1. **For Multi-Class Classification:** In multi-class classification problems, the model outputs a logit for each class. The **softmax function** is then applied to the vector of logits to obtain a probability distribution over all possible classes. The softmax function ensures that the output probabilities sum to 1 and are in the range [0, 1].\n",
    "\n",
    "#### Last but not Least\n",
    "\n",
    "\n",
    "We need terminologies like epochs, batch size, iterations only when the data is too big which happens all the time in machine learning and we can’t pass all the data to the computer at once. So, to overcome this problem we need to divide the data into smaller sizes and give it to our computer one by one and update the weights of the neural networks at the end of every step to fit it to the data given.\n",
    "\n",
    "1. **Epochs:** One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE. I know it doesn’t make sense in the starting that — passing the entire dataset through a neural network is not enough. And we need to pass the full dataset multiple times to the same neural network. But keep in mind that we are using a limited dataset and to optimise the learning and the graph we are using Gradient Descent which is an iterative process. So, updating the weights with single pass or one epoch is not enough.\n",
    "\n",
    "1. **Batch:**  Since one epoch is too big to feed to the computer at once we divide it in several smaller batches. You can’t pass the entire dataset into the neural net at once. So, you divide dataset into Number of Batches or sets or parts. `No. of batches = (Size of the entire dataset / batch size) + 1`\n",
    "\n",
    "1. **Batch Size:** Total number of training examples present in a single batch. Note: Batch size and number of batches are two different things.\n",
    "\n",
    "1. **Iteration / Step:** Iterations is the number of batches needed to complete one epoch. Note: The number of batches is equal to number of iterations for one epoch. More precisely, a training step (iteration) is one gradient update. `No. of training steps = No. of batches = No. of gradient updates` `No. of ALL gradient updates = No. of batches x No. of epochs\n",
    "`\n",
    "\n",
    "\n",
    "\n",
    "We can divide the dataset of 2000 examples into batches of 500 then it will take 4 iterations to complete 1 epoch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7df392d",
   "metadata": {
    "papermill": {
     "duration": 0.049317,
     "end_time": "2024-03-01T13:16:25.675761",
     "exception": false,
     "start_time": "2024-03-01T13:16:25.626444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color: #7b6b59;\">Put it all together for training</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "250eee08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:25.774800Z",
     "iopub.status.busy": "2024-03-01T13:16:25.774406Z",
     "iopub.status.idle": "2024-03-01T13:16:25.795270Z",
     "shell.execute_reply": "2024-03-01T13:16:25.794170Z"
    },
    "papermill": {
     "duration": 0.073327,
     "end_time": "2024-03-01T13:16:25.797803",
     "exception": false,
     "start_time": "2024-03-01T13:16:25.724476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def training_loop(train, fold, tokenizer):\n",
    "    \"\"\"Executes the training loop for a given fold of the data.\n",
    "\n",
    "    Args:\n",
    "        train (DataFrame): The training dataset containing features and labels.\n",
    "        fold (int): The current fold index to be used for validation within a cross-validation scheme.\n",
    "        tokenizer: The tokenizer instance to process the text data.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The predictions for the validation set of the current fold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Log the start of training for the current fold\n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    train_folds = train[train[\"fold\"] != fold].reset_index(drop=True)\n",
    "    valid_folds = train[train[\"fold\"] == fold].reset_index(drop=True)\n",
    "    valid_labels = valid_folds[\"label\"].values\n",
    "    \n",
    "    # Prepare the training dataset and loader\n",
    "    train_dataset = DAIGTDataset(train_folds, tokenizer, CFG.max_len)\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Prepare the validation dataset and loader\n",
    "    valid_dataset = DAIGTDataset(valid_folds, tokenizer, CFG.max_len)\n",
    "    valid_loader = DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=batch_size * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    # Initialize a custom model instance with some configuration and possibly load pretrained weights.\n",
    "    model = DAIGTModel(CFG, config_path=None, pretrained=True)\n",
    "\n",
    "    # Save the model's configuration to a file. 'model.config' is assumed to contain the configuration\n",
    "    # of the model which could include its architecture, hyperparameters, etc. This is saved to a file\n",
    "    # named 'config.pth' in the directory specified by 'OUTPUT_DIR'. 'OUTPUT_DIR' is assumed to be a\n",
    "    # predefined directory path where output files are stored.\n",
    "    torch.save(model.config, OUTPUT_DIR+'config.pth')\n",
    "\n",
    "    # Move the model to a specific device. 'device' is assumed to be a string or torch.device object\n",
    "    # that specifies whether the model should run on a CPU, a single GPU, or multiple GPUs.\n",
    "    # Commonly, 'device' would be set to something like 'cpu', 'cuda', or 'cuda:0'.\n",
    "    model.to(device)\n",
    "    \n",
    "    # Setup the optimizer and learning rate scheduler\n",
    "    optimizer_parameters = get_optimizer_params(\n",
    "        model,\n",
    "        encoder_lr=CFG.encoder_lr, \n",
    "        decoder_lr=CFG.decoder_lr,\n",
    "        weight_decay=CFG.weight_decay\n",
    "    )\n",
    "    \n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr)\n",
    "    \n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    \n",
    "    best_score = np.inf\n",
    "    # Initialize the BCEWithLogitsLoss\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Run one training epoch and return the average loss\n",
    "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "        \n",
    "        # Run the validation loop and return the average validation loss and predictions\n",
    "        avg_val_loss, predictions = validation_loop(valid_loader, model, criterion, device)\n",
    "        \n",
    "        # Compute the score based on validation predictions\n",
    "        score = get_score(valid_labels, predictions)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "        \n",
    "        # Check if the current model is the best so far and save if it is\n",
    "        if best_score > score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n",
    "        \n",
    "\n",
    "    # Load the best model's predictions for the validation set\n",
    "    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n",
    "                             map_location=torch.device('cpu'))['predictions']\n",
    "    \n",
    "    # Convert the numpy array to a pandas DataFrame\n",
    "    predictions = pd.DataFrame(predictions)\n",
    "\n",
    "    # Optionally, you can specify column names\n",
    "    valid_folds[\"preds\"] = predictions\n",
    "    \n",
    "    # Clear CUDA cache and collect garbage to free memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_folds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "42557dba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:16:25.917033Z",
     "iopub.status.busy": "2024-03-01T13:16:25.916651Z",
     "iopub.status.idle": "2024-03-01T13:20:40.116944Z",
     "shell.execute_reply": "2024-03-01T13:20:40.116117Z"
    },
    "papermill": {
     "duration": 254.258817,
     "end_time": "2024-03-01T13:20:40.119058",
     "exception": false,
     "start_time": "2024-03-01T13:16:25.860241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/130] Elapsed 0m 1s (remain 3m 54s) Loss: 0.9151(0.9151) LR: 0.00002000  \n",
      "Epoch: [1][20/130] Elapsed 0m 19s (remain 1m 42s) Loss: 0.0024(0.3265) LR: 0.00001967  \n",
      "Epoch: [1][40/130] Elapsed 0m 37s (remain 1m 21s) Loss: 0.0003(0.1677) LR: 0.00001878  \n",
      "Epoch: [1][60/130] Elapsed 0m 55s (remain 1m 2s) Loss: 0.0002(0.1127) LR: 0.00001737  \n",
      "Epoch: [1][80/130] Elapsed 1m 12s (remain 0m 43s) Loss: 0.0001(0.0849) LR: 0.00001552  \n",
      "Epoch: [1][100/130] Elapsed 1m 30s (remain 0m 25s) Loss: 0.0014(0.0794) LR: 0.00001334  \n",
      "Epoch: [1][120/130] Elapsed 1m 48s (remain 0m 8s) Loss: 0.0032(0.0738) LR: 0.00001097  \n",
      "Epoch: [1][129/130] Elapsed 1m 55s (remain 0m 0s) Loss: 0.0019(0.0693) LR: 0.00000988  \n",
      "EVAL: [0/44] Elapsed 0m 0s (remain 0m 21s) Loss: 0.0013(0.0013) \n",
      "EVAL: [20/44] Elapsed 0m 4s (remain 0m 5s) Loss: 0.0013(0.0013) \n",
      "EVAL: [40/44] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0013(0.0210) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.0693  avg_val_loss: 0.0200  time: 125s\n",
      "Epoch 1 - Score: 0.9913\n",
      "Epoch 1 - Save Best Score: 0.9913 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [43/44] Elapsed 0m 9s (remain 0m 0s) Loss: 0.0014(0.0200) \n",
      "Epoch: [2][0/130] Elapsed 0m 1s (remain 2m 34s) Loss: 0.0017(0.0017) LR: 0.00000976  \n",
      "Epoch: [2][20/130] Elapsed 0m 19s (remain 1m 38s) Loss: 0.0009(0.0012) LR: 0.00000735  \n",
      "Epoch: [2][40/130] Elapsed 0m 36s (remain 1m 19s) Loss: 0.0007(0.0010) LR: 0.00000511  \n",
      "Epoch: [2][60/130] Elapsed 0m 54s (remain 1m 1s) Loss: 0.0009(0.0305) LR: 0.00000315  \n",
      "Epoch: [2][80/130] Elapsed 1m 12s (remain 0m 43s) Loss: 0.0011(0.0232) LR: 0.00000159  \n",
      "Epoch: [2][100/130] Elapsed 1m 30s (remain 0m 25s) Loss: 0.0011(0.0188) LR: 0.00000054  \n",
      "Epoch: [2][120/130] Elapsed 1m 47s (remain 0m 8s) Loss: 0.0010(0.0159) LR: 0.00000004  \n",
      "Epoch: [2][129/130] Elapsed 1m 55s (remain 0m 0s) Loss: 0.0010(0.0150) LR: 0.00000000  \n",
      "EVAL: [0/44] Elapsed 0m 0s (remain 0m 22s) Loss: 0.0007(0.0007) \n",
      "EVAL: [20/44] Elapsed 0m 4s (remain 0m 5s) Loss: 0.0007(0.0008) \n",
      "EVAL: [40/44] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0007(0.0221) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0150  avg_val_loss: 0.0210  time: 125s\n",
      "Epoch 2 - Score: 0.9884\n",
      "Epoch 2 - Save Best Score: 0.9884 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [43/44] Elapsed 0m 9s (remain 0m 0s) Loss: 0.0008(0.0210) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 result ==========\n",
      "Score: 0.9884\n",
      "========== CV ==========\n",
      "Score: 0.9884\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    oof_df = pd.DataFrame()\n",
    "    for fold in range(CFG.n_folds):\n",
    "        _oof_df = training_loop(train, fold, tokenizer)\n",
    "        oof_df= pd.concat([oof_df, _oof_df])\n",
    "        LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "        score = get_score(oof_df[\"label\"], oof_df[\"preds\"])\n",
    "        LOGGER.info(f'Score: {score:<.4f}')\n",
    "        break # TO REMOVE IT FOR PROPER TRAINING\n",
    "    \n",
    "    oof_df = oof_df.reset_index(drop=True)\n",
    "    \n",
    "    LOGGER.info(f\"========== CV ==========\")\n",
    "    score = get_score(oof_df[\"label\"], oof_df[\"preds\"])\n",
    "    LOGGER.info(f'Score: {score:<.4f}')\n",
    "    \n",
    "    oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9e1edc",
   "metadata": {
    "papermill": {
     "duration": 0.050303,
     "end_time": "2024-03-01T13:20:40.223315",
     "exception": false,
     "start_time": "2024-03-01T13:20:40.173012",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***Attention: The primary objective of this notebook is to walk you through the concepts and detailed steps involved in fine-tuning a pretrained model. It is designed to be educational, focusing on each aspect of the process to enhance your understanding. Please note that we will not be training the actual model intended for submissions with the provided dataset. This is because the dataset in question is highly imbalanced, containing only 4 samples for the positive class, which would lead to results that are not representative of a well-trained model. You are encouraged to use this notebook as a template or skeleton code and apply the fine-tuning process to another, more balanced dataset to achieve meaningful results.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed16555",
   "metadata": {
    "papermill": {
     "duration": 0.050498,
     "end_time": "2024-03-01T13:20:40.324423",
     "exception": false,
     "start_time": "2024-03-01T13:20:40.273925",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#7b6b59;overflow:hidden\">Train your own Tokenizer</div>\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">Introduction</span>\n",
    "\n",
    "Large Language Generative AI models are developed mostly working with large amounts of text data. For this reason anyone working in this area should have specific skills in text processing. To enable AI models to learn from text data effectively we must first preprocess text into a format which is understandable to machines. Tokenization and Vectorization are two of the most important steps in this procedure. \n",
    "\n",
    "Before our data can be fed to a model, it needs to be transformed to a format the model can understand. Machine learning algorithms take numbers as inputs. This means that we will need to convert the texts into numerical vectors. There are two steps to this process:\n",
    "\n",
    "1. **Tokenization:** Divide the texts into words or smaller sub-texts, which will enable good generalization of relationship between the texts and the labels. This determines the “vocabulary” of the dataset (set of unique tokens present in the data). (Splitting text into smaller units such as words or phrases.)\n",
    "\n",
    "1. **Vectorization:** Define a good numerical measure to characterize these texts. Converting text into numerical representations for ML models.\n",
    "\n",
    "In summary, the typical order is tokenization first to break down the text into understandable units and then vectorization to turn those units into a numerical format suitable for machine learning models.\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">Tokenization</span>\n",
    "\n",
    "Text tokenization is the process of reformatting a piece of text into smaller units called “tokens.” It transforms unstructured text into structured data that models can understand. The goal of tokenization is to break down text into meaningful units like words, phrases, sentences, etc. which can then be inputted into machine learning models. It’s one of the first and most important steps in natural language preprocessing, and often goes hand-in-hand with text vectorization.\n",
    "\n",
    "Tokenization enables natural language processing tasks like part-of-speech tagging (identifying verbs vs nouns, etc.), named entity recognition (categories like person, organization, location), and relationship extraction (family relationships, professional relationships, etc.).\n",
    "\n",
    "There are a number of different tokenization methods; some of the simpler ones include splitting text on whitespace or punctuation. Advanced techniques use language rules to identify word boundaries and tokenize text into linguistic units; this can split words into sub-word tokens (such as prefixes, or based on syllables), or even combine certain tokens into larger units based on language semantics. The goal is to produce tokens that best represent the original text for ML purposes.\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">Vectorization</span>\n",
    "\n",
    "Now since most Large language models today are based on Transformers and Deep Learning architectures, they still work best with numbers, so to enable them to learn from text we should also convert the tokens to numbers, so each word will be represented with a number instead of sequence of letters. After tokenization, the tokens can then be converted into numerical format through vectorization, which is necessary because machine learning models don't understand text directly; they understand numbers. Vectorization represents the tokens in a way that the model can understand, often as vectors in a high-dimensional space. There are several methods of vectorization, including **Bag of Words**, **TF-IDF**, and **word embeddings** like **Word2Vec** or **GloVe**.\n",
    "\n",
    "Text vectorization is the process of converting text into numerical representations (or “vectors”) that can be understood by ML models. It transforms unstructured text into structured numeric data with the goal to represent the semantic meaning of text in a mathematical format.\n",
    "\n",
    "Text vectorization allows for a variety of NLP tasks like document classification (checking whether something is an email or an essay, etc.), sentiment analysis (opinions or attitudes of the text, etc.), enhancing search engines, and so on.\n",
    "\n",
    "Common text vectorization methods include **one-hot encoding** (assigning a unique integer value to each word), **bag-of-words** (counting the occurrence of words within each document), and **word embeddings** (mapping words to vectors so as to capturing meaning). ***The vector space allows words with similar meanings to have similar representations.***\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">Tokenizers</span>\n",
    "\n",
    "Before you can use your data in a model, the data needs to be processed into an acceptable format for the model. A model does not understand raw text, images or audio. These inputs need to be converted into numbers and assembled into tensors.\n",
    "\n",
    "**The main tool for processing textual data is a tokenizer. A tokenizer starts by splitting text into tokens according to a set of rules. The tokens are converted into numbers, which are used to build tensors as input to a model. Any additional inputs required by a model are also added by the tokenizer.**\n",
    "\n",
    "\n",
    "On this section, we will have a closer look at tokenization. As we saw, tokenizing a text is splitting it into **words** or **subwords**, which then are converted to ids through a look-up table. Converting words or subwords to ids is straightforward, so in this summary, we will focus on splitting a text into words or subwords (i.e. tokenizing a text). \n",
    "More specifically, we will look at the three main types of tokenizers used in 🤗 Transformers: \n",
    "\n",
    "1. **Byte-Pair Encoding (BPE)**, \n",
    "1. **WordPiece**, \n",
    "1. and **SentencePiece**, and show examples of which tokenizer type is used by which model.\n",
    "\n",
    "Note that on each model page, you can look at the documentation of the associated tokenizer to know which tokenizer type was used by the pretrained model. For instance, if we look at [BertTokenizer](https://huggingface.co/docs/transformers/v4.36.1/en/model_doc/bert#transformers.BertTokenizer), we can see that the model uses WordPiece.\n",
    "\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Introduction</span>\n",
    "\n",
    "***What is a tokenizer?***\n",
    "\n",
    "The definition of tokenization, as given by Stanford NLP group is:\n",
    "\n",
    "“Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation”\n",
    "\n",
    "Tokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data.\n",
    "\n",
    "The goal is to find the most meaningful representation — that is, the one that makes the most sense to the model — and, if possible, the smallest representation.\n",
    "\n",
    "There are different solutions available: **word-based**, **character-based** but the one used by the state-of-the-art transformer models are **sub-word tokenizers**: Byte-level BPE(GPT-2), WordPiece(BERT) etc.\n",
    "\n",
    "#### <span style=\"color: #7b6b59;\">Space & Punctuation Tokenization</span>\n",
    "\n",
    "Splitting a text into smaller chunks is a task that is harder than it looks, and there are multiple ways of doing so. For instance, let’s look at the sentence `\"Don't you love 🤗 Transformers? We sure do.\"`\n",
    "\n",
    "A simple way of tokenizing this text is to split it by spaces, which would give:\n",
    "\n",
    "`[\"Don't\", \"you\", \"love\", \"🤗\", \"Transformers?\", \"We\", \"sure\", \"do.\"]`\n",
    "\n",
    "This is a sensible first step, but if we look at the tokens \"Transformers?\" and \"do.\", we notice that the punctuation is attached to the words \"Transformer\" and \"do\", which is suboptimal. We should take the punctuation into account so that a model does not have to learn a different representation of a word and every possible punctuation symbol that could follow it, which would explode the number of representations the model has to learn. Taking punctuation into account, tokenizing our exemplary text would give:\n",
    "\n",
    "`[\"Don\", \"'\", \"t\", \"you\", \"love\", \"🤗\", \"Transformers\", \"?\", \"We\", \"sure\", \"do\", \".\"]`\n",
    "\n",
    "Better. However, it is disadvantageous, how the tokenization dealt with the word \"Don't\". \"Don't\" stands for \"do not\", so it would be better tokenized as `[\"Do\", \"n't\"]`. This is where things start getting complicated, and part of the reason each model has its own tokenizer type. Depending on the rules we apply for tokenizing a text, a different tokenized output is generated for the same text. A pretrained model only performs properly if you feed it an input that was tokenized with the same rules that were used to tokenize its training data.\n",
    "\n",
    "**spaCy** and **Moses** are two popular **rule-based tokenizers**. Applying them on our example, spaCy and Moses would output something like:\n",
    "\n",
    "`[\"Do\", \"n't\", \"you\", \"love\", \"🤗\", \"Transformers\", \"?\", \"We\", \"sure\", \"do\", \".\"]`\n",
    "\n",
    "As can be seen space and punctuation tokenization, as well as rule-based tokenization, is used here. Space and punctuation tokenization and rule-based tokenization are both examples of word tokenization, which is loosely defined as splitting sentences into words. While it’s the most intuitive way to split texts into smaller chunks, this tokenization method can lead to problems for massive text corpora. In this case, space and punctuation tokenization usually generates a very big vocabulary (the set of all unique words and tokens used). E.g., **Transformer XL uses space and punctuation tokenization**, resulting in a vocabulary size of 267,735!\n",
    "\n",
    "Such a big vocabulary size forces the model to have an enormous embedding matrix as the input and output layer, which causes both an increased memory and time complexity. In general, **transformers models rarely have a vocabulary size greater than 50,000, especially if they are pretrained only on a single language.**\n",
    "\n",
    "So if simple space and punctuation tokenization is unsatisfactory, why not simply tokenize on characters?\n",
    "\n",
    "#### <span style=\"color: #7b6b59;\">Character Tokenization</span>\n",
    "\n",
    "While character tokenization is very simple and would greatly reduce memory and time complexity it makes it much harder for the model to learn meaningful input representations. E.g. learning a meaningful context-independent representation for the letter \"t\" is much harder than learning a context-independent representation for the word \"today\". Therefore, character tokenization is often accompanied by a loss of performance. **So to get the best of both worlds, transformers models use a hybrid between word-level and character-level tokenization called subword tokenization.**\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Subword Tokenization</span>\n",
    "Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords. For instance \"annoyingly\" might be considered a rare word and could be decomposed into \"annoying\" and \"ly\". Both \"annoying\" and \"ly\" as stand-alone subwords would appear more frequently while at the same time the meaning of \"annoyingly\" is kept by the composite meaning of \"annoying\" and \"ly\". This is especially useful in agglutinative languages such as Turkish, where you can form (almost) arbitrarily long complex words by stringing together subwords.\n",
    "\n",
    "Subword tokenization allows the model to have a reasonable vocabulary size while being able to learn meaningful context-independent representations. In addition, subword tokenization enables the model to process words it has never seen before, by decomposing them into known subwords. For instance, the BertTokenizer tokenizes \"I have a new GPU!\" as follows:\n",
    "\n",
    "`[\"i\", \"have\", \"a\", \"new\", \"gp\", \"##u\", \"!\"]`\n",
    "\n",
    "Because we are considering the uncased model, the sentence was lowercased first. We can see that the words `[\"i\", \"have\", \"a\", \"new\"]` are present in the tokenizer’s vocabulary, but the word \"gpu\" is not. Consequently, the tokenizer splits \"gpu\" into known subwords: `[\"gp\" and \"##u\"]`. \"##\" means that the rest of the token should be attached to the previous one, without space (for decoding or reversal of the tokenization).\n",
    "As another example, XLNetTokenizer tokenizes our previously exemplary text as follows:\n",
    "\n",
    "`[\"▁Don\", \"'\", \"t\", \"▁you\", \"▁love\", \"▁\", \"🤗\", \"▁\", \"Transform\", \"ers\", \"?\", \"▁We\", \"▁sure\", \"▁do\", \".\"]`\n",
    "\n",
    "We’ll get back to the meaning of those \"▁\" when we look at SentencePiece. As one can see, the rare word \"Transformers\" has been split into the more frequent subwords \"Transform\" and \"ers\".\n",
    "\n",
    "Let’s now look at how the different subword tokenization algorithms work. Note that all of those tokenization algorithms rely on some form of training which is usually done on the corpus the corresponding model will be trained on.\n",
    "\n",
    "Concepts related to BPE:\n",
    "\n",
    "\n",
    "1. **Vocabulary:** A set of subword units that can be used to represent a text corpus.\n",
    "1. **Byte:** A unit of digital information that typically consists of eight bits.\n",
    "1. **Character:** A symbol that represents a written or printed letter or numeral.\n",
    "1. **Frequency:** The number of times a byte or character occurs in a text corpus.\n",
    "1. **Merge:** The process of combining two consecutive bytes or characters to create a new subword unit.\n",
    "\n",
    "#### <span style=\"color: #7b6b59;\">Byte-Pair Encoding (BPE)</span>\n",
    "\n",
    "Byte-Pair Encoding (BPE) was introduced in Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015). BPE relies on a pre-tokenizer that splits the training data into words. Pretokenization can be as simple as space tokenization, e.g. GPT-2, RoBERTa. More advanced pre-tokenization include rule-based tokenization, e.g. XLM, FlauBERT which uses Moses for most languages, or GPT which uses Spacy and ftfy, to count the frequency of each word in the training corpus.\n",
    "\n",
    "After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.\n",
    "\n",
    "As an example, let’s assume that after pre-tokenization, the following set of words including their frequency has been determined:\n",
    "\n",
    "`(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)`\n",
    "\n",
    "Consequently, the base vocabulary is `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"]`. Splitting all words into symbols of the base vocabulary, we obtain:\n",
    "\n",
    "`(\"h\" \"u\" \"g\", 10), (\"p\" \"u\" \"g\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"u\" \"g\" \"s\", 5)`\n",
    "\n",
    "BPE then counts the frequency of each possible symbol pair and picks the symbol pair that occurs most frequently. In the example above \"h\" followed by \"u\" is present 10 + 5 = 15 times (10 times in the 10 occurrences of \"hug\", 5 times in the 5 occurrences of \"hugs\"). However, the most frequent symbol pair is \"u\" followed by \"g\", occurring 10 + 5 + 5 = 20 times in total. Thus, the first merge rule the tokenizer learns is to group all \"u\" symbols followed by a \"g\" symbol together. Next, \"ug\" is added to the vocabulary. The set of words then becomes\n",
    "\n",
    "`(\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"ug\" \"s\", 5)`\n",
    "\n",
    "BPE then identifies the next most common symbol pair. It’s \"u\" followed by \"n\", which occurs 16 times. \"u\", \"n\" is merged to \"un\" and added to the vocabulary. The next most frequent symbol pair is \"h\" followed by \"ug\", occurring 15 times. Again the pair is merged and \"hug\" can be added to the vocabulary.\n",
    "\n",
    "At this stage, the vocabulary is `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"]` and our set of unique words is represented as\n",
    "\n",
    "`(\"hug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"hug\" \"s\", 5)`\n",
    "\n",
    "Assuming, that the Byte-Pair Encoding training would stop at this point, the learned merge rules would then be applied to new words (as long as those new words do not include symbols that were not in the base vocabulary). For instance, the word \"bug\" would be tokenized to `[\"b\", \"ug\"]` but \"mug\" would be tokenized as `[\"<unk>\", \"ug\"]` since the symbol `\"m\"` is not in the base vocabulary. In general, single letters such as `\"m\"` are not replaced by the `\"<unk>\"` symbol because the training data usually includes at least one occurrence of each letter, but it is likely to happen for very special characters like emojis.\n",
    "\n",
    "As mentioned earlier, the vocabulary size, i.e. the base vocabulary size + the number of merges, is a hyperparameter to choose. For instance GPT has a vocabulary size of 40,478 since they have 478 base characters and chose to stop training after 40,000 merges.\n",
    "\n",
    "**Recap:** Steps involved in BPE:\n",
    "\n",
    "1. Initialize the vocabulary with all the bytes or characters in the text corpus\n",
    "1. Calculate the frequency of each byte or character in the text corpus.\n",
    "1. Repeat the following steps until the desired vocabulary size is reached:\n",
    "    - Find the most frequent pair of consecutive bytes or characters in the text corpus\n",
    "    - Merge the pair to create a new subword unit.\n",
    "    - Update the frequency counts of all the bytes or characters that contain the merged pair.\n",
    "    - Add the new subword unit to the vocabulary.\n",
    "\n",
    "1. Represent the text corpus using the subword units in the vocabulary.\n",
    "\n",
    "#### <span style=\"color: #7b6b59;\">Byte-level BPE</span>\n",
    "\n",
    "A base vocabulary that includes all possible base characters can be quite large if e.g. all unicode characters are considered as base characters. To have a better base vocabulary, GPT-2 uses bytes as the base vocabulary, which is a clever trick to force the base vocabulary to be of size 256 while ensuring that every base character is included in the vocabulary. With some additional rules to deal with punctuation, the GPT2’s tokenizer can tokenize every text without the need for the `<unk>` symbol. GPT-2 has a vocabulary size of 50,257, which corresponds to the 256 bytes base tokens, a special end-of-text token and the symbols learned with 50,000 merges.\n",
    "\n",
    "**Understanding Characters and Bytes:**\n",
    "\n",
    "1. **Characters:** These are the basic units of text (like 'A', '7', '!', 'é', '中'). In human language, we see these as individual symbols or letters.\n",
    "1. **Bytes:** A byte is a unit of digital information that commonly consists of eight bits. It's a fundamental concept in computer science and is used to represent data.\n",
    "\n",
    "**Character Encoding:**\n",
    "\n",
    "Characters are represented in computers using various encoding systems, which map characters to specific byte sequences. Two common encodings are ASCII and UTF-8:\n",
    "\n",
    "1. **ASCII (American Standard Code for Information Interchange):** This is one of the oldest character encoding standards. It uses one byte (8 bits) per character and can represent up to 256 different symbols (0-255). ASCII is limited to English characters and some control characters and symbols. The maximum of 256 different symbols in ASCII is due to its use of one byte per character, and a byte consists of 8 bits. Here's a breakdown of why this limits it to 256 symbols. When you have a single bit, you have two possible values (0 or 1). With two bits, you can have 4 possible combinations (00, 01, 10, 11). For 8 bits (1 byte), the number of possible combinations is  2^8 = 256. This range is from 0 to 255, which gives 256 total possible values.\n",
    "\n",
    "1. **UTF-8 (8-bit Unicode Transformation Format):** This is a more modern and versatile encoding standard capable of representing a vast array of characters from virtually all written languages. UTF-8 is backward compatible with ASCII but can use one to four bytes per character, allowing it to cover much more than the basic ASCII set.\n",
    "\n",
    "\n",
    "For the GPT models, OpenAI uses a method known as byte-level byte pair encoding, instead of alphabets or ASCII, the base vocabulary is defined in bytes. Since every character in any encoding on a computer is created from bytes, the base vocabulary contains every possible combination of byte, and the tokenizer never runs into an unknown token.\n",
    "\n",
    "**Byte-Level:** \n",
    "\n",
    "Instead of starting with a vocabulary of words or characters (like alphabets or ASCII characters), byte-level BPE operates on bytes, which are essentially the smallest addressable group of bits in a computer (usually 8 bits). This means the base vocabulary consists of all 256 possible byte values (from 0 to 255).\n",
    "In traditional BPE or other tokenization methods that start with characters, the process involves looking at the text's character-level representation. For example, the word \"hello\" would be considered as 'h', 'e', 'l', 'l', 'o' – five separate characters.\n",
    "\n",
    "In Byte-Level BPE, instead of looking at characters, we consider the byte representation of the text. This approach doesn't start with an understanding of \"characters\" per se but with the bytes that encode these characters. Here's why it's significant:\n",
    "\n",
    "\n",
    "**Why Byte-Level?:**\n",
    "\n",
    "1. **All-Inclusive:** Since every character (no matter the language or symbol) can be broken down into bytes, starting with bytes ensures that the vocabulary can represent any text without missing symbols or needing placeholders for unknowns.\n",
    "\n",
    "1. **Simplifies Vocabulary:** Instead of potentially needing thousands of character tokens to cover various languages and symbols, Byte-Level BPE only needs 256 base tokens, corresponding to all possible values of a byte (0-255). This drastically simplifies the model's vocabulary.\n",
    "\n",
    "1. **Handles Varied Text:** By using bytes, the tokenizer can handle texts in ASCII (like English text) and texts in more complex encodings like UTF-8 (which can represent virtually all human languages) without needing separate mechanisms or special handling for different languages or symbol sets.\n",
    "\n",
    "1. **Universality:** Bytes are the fundamental building blocks of digital data. By using bytes, the model can represent any character in any language or even other forms of data like emojis or special symbols without being restricted to a specific character set. This universality means that it can process text in virtually any language or symbol system.\n",
    "\n",
    "1. **No Unknown Tokens:** Traditional tokenizers might encounter characters or words they have never seen before (out-of-vocabulary words), leading to the use of a special \"unknown\" token. Byte-level BPE virtually eliminates this problem because every piece of text can be broken down into bytes, which are always within the model's vocabulary. Thus, the tokenizer is capable of handling any text input without encountering unknown tokens.\n",
    "\n",
    "\n",
    "So, when we say a \"character is a byte\" in the context of Byte-Level BPE, it's a bit of a simplification. A more accurate statement would be: \"All characters can be represented as sequences of bytes, and Byte-Level BPE uses these byte sequences as the foundational elements of its vocabulary.\" This means each character in text is represented by one or more bytes, depending on its encoding, and these bytes are the building blocks for the tokenizer's vocabulary and subsequent text processing. In summary, byte-level BPE is a way of preparing text for machine learning models like GPT that is both highly versatile and capable of handling a wide variety of languages and symbols without running into the issue of unknown tokens. It's a foundational aspect of how these models process and understand the text data they're trained on and generate.\n",
    "\n",
    "\n",
    "Byte-Level Byte Pair Encoding (BPE) is a tokenization method that builds upon the standard BPE algorithm by using bytes as the fundamental unit for its vocabulary. This approach, as used in models like GPT-2, is particularly effective and efficient for several reasons. Here's a more detailed explanation of how it works and its benefits:\n",
    "\n",
    "**Base Vocabulary:**\n",
    "\n",
    "1. **Standard BPE:** Traditional Byte Pair Encoding starts with a base vocabulary of all unique characters (or tokens) in the training corpus and iteratively combines the most frequent pair of tokens to create new, longer tokens. This process continues for a number of merges, determined beforehand.\n",
    "\n",
    "1. **Byte-Level BPE:** Instead of starting with characters, Byte-Level BPE considers each byte (256 possible values in total, representing all possible single-byte characters) as the base vocabulary. This approach automatically includes all possible characters in ASCII and extends to any byte value that might represent a part of a character in more extensive encoding systems like UTF-8.\n",
    "\n",
    "***Advantages:***\n",
    "\n",
    "1. **Compact and Comprehensive Base Vocabulary:** By using bytes, the base vocabulary is limited to 256 tokens (since there are 256 possible byte values), which is more compact compared to potentially thousands of Unicode characters. Yet, it's comprehensive enough to represent any text because all text can be broken down into bytes.\n",
    "\n",
    "1. **Eliminating `<unk>` Tokens:** Traditional tokenizers might encounter unknown characters or words not present in the vocabulary, often represented by an `<unk>` (unknown) token. Since Byte-Level BPE can tokenize any text into bytes (and subsequentially into byte-level tokens), it theoretically doesn't need an `<unk>` symbol, as every possible byte can be represented in its vocabulary.\n",
    "\n",
    "1. **Handling Diverse Scripts and Symbols:** With the ability to represent any character as a series of bytes, Byte-Level BPE is naturally equipped to handle text in multiple languages, including those with large character sets or special symbols, without needing separate models or token sets for different languages.\n",
    "\n",
    "**GPT-2's Vocabulary:**\n",
    "In the case of GPT-2:\n",
    "\n",
    "- **256 Base Tokens:** Corresponding to all possible byte values.\n",
    "- **Special End-of-Text Token:** Used to signify the end of a text.\n",
    "- **50,000 Merges:** The tokenizer iteratively combines frequent pairs of these byte-level tokens to form higher-level tokens, up to 50,000 merges. These merges are learned from the training corpus and represent common words, subwords, or sequences of characters that appear frequently together.\n",
    "\n",
    "The resulting vocabulary size is 50,257 (256 base tokens + 1 special token + 50,000 merged tokens), which provides a good balance between granularity and coverage. This means GPT-2's tokenizer is capable of handling a wide variety of texts, from different languages and domains, without a substantial increase in vocabulary size, making it efficient and powerful for language understanding and generation tasks.\n",
    "    \n",
    "#### <span style=\"color: #7b6b59;\">WordPiece</span>\n",
    "\n",
    "WordPiece is the subword tokenization algorithm used for BERT, DistilBERT, and Electra. The algorithm was outlined in Japanese and Korean Voice Search (Schuster et al., 2012) and is very similar to BPE. WordPiece first initializes the vocabulary to include every character present in the training data and progressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent symbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary.\n",
    "\n",
    "So what does this mean exactly? Referring to the previous example, maximizing the likelihood of the training data is equivalent to finding the symbol pair, whose probability divided by the probabilities of its first symbol followed by its second symbol is the greatest among all symbol pairs. E.g. \"u\", followed by \"g\" would have only been merged if the probability of \"ug\" divided by \"u\", \"g\" would have been greater than for any other symbol pair. Intuitively, WordPiece is slightly different to BPE in that it evaluates what it loses by merging two symbols to ensure it’s worth it.\n",
    "\n",
    "\n",
    "#### <span style=\"color: #7b6b59;\">SentencePiece</span>\n",
    "\n",
    "All tokenization algorithms described so far have the same problem: It is assumed that the input text uses spaces to separate words. However, not all languages use spaces to separate words. One possible solution is to use language specific pre-tokenizers, e.g. XLM uses a specific Chinese, Japanese, and Thai pre-tokenizer). To solve this problem more generally, SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo et al., 2018) treats the input as a raw input stream, thus including the space in the set of characters to use. It then uses the BPE or unigram algorithm to construct the appropriate vocabulary.\n",
    "\n",
    "The XLNetTokenizer uses SentencePiece for example, which is also why in the example earlier the \"▁\" character was included in the vocabulary. Decoding with SentencePiece is very easy since all tokens can just be concatenated and \"▁\" is replaced by a space.\n",
    "\n",
    "All transformers models in the library that use SentencePiece use it in combination with unigram. Examples of models using SentencePiece are ALBERT, XLNet, Marian, and T5.\n",
    "\n",
    "\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">HuggingFace Tokenizers: `tokenizers` Library</span>\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Introduction</span>\n",
    "\n",
    "Fast State-of-the-art [tokenizers](https://huggingface.co/docs/tokenizers/index), optimized for both research and production\n",
    "\n",
    "[🤗 Tokenizers](https://github.com/huggingface/tokenizers) provides an implementation of today’s most used tokenizers, with a focus on performance and versatility. These tokenizers are also used in [🤗 Transformers](https://github.com/huggingface/transformers).\n",
    "\n",
    "**Main features:**\n",
    "\n",
    "- Train new vocabularies and tokenize, using today’s most used tokenizers.\n",
    "- Extremely fast (both training and tokenization), thanks to the Rust implementation. Takes less than 20 seconds to tokenize a GB of text on a server’s CPU.\n",
    "- Easy to use, but also extremely versatile.\n",
    "- Designed for both research and production.\n",
    "- Full alignment tracking. Even with destructive normalization, it’s always possible to get the part of the original sentence that corresponds to any token.\n",
    "- Does all the pre-processing: Truncation, Padding, add the special tokens your model needs.\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">The tokenization pipeline</span>\n",
    "\n",
    "In this section, we will try to understand the HuggingFace tokenizers in depth and will go through all the parameters and also the outputs returned by a tokenizer. We’ll dive into the AutoTokenizer class and see how to use a pre-trained tokenizer for our data.\n",
    "\n",
    "So, let’s get started!\n",
    "\n",
    "Hugging Face is a New York based company that has swiftly developed language processing expertise. The company’s aim is to advance NLP and democratize it for use by practitioners and researchers around the world.\n",
    "\n",
    "In an effort to offer access to fast, state-of-the-art, and easy-to-use tokenization that plays well with modern NLP pipelines, Hugging Face contributors have developed and open-sourced Tokenizers. Tokenizers is, as the name implies, an implementation of today’s most widely used tokenizers with emphasis on performance and versatility.\n",
    "\n",
    "An implementation of a tokenizer consists of the following pipeline of processes, each applying different transformations to the textual information. When calling Tokenizer.encode or Tokenizer.encode_batch, the input text(s) go through the following pipeline:\n",
    "\n",
    "- normalization\n",
    "- pre-tokenization\n",
    "- model\n",
    "- post-processing\n",
    "\n",
    "We’ll see in details what happens during each of those steps in detail, as well as when you want to decode `<decoding>` some token ids, and how the 🤗 Tokenizers library allows you to customize each of those steps to your needs. \n",
    "\n",
    "Let’s go through these steps:\n",
    "\n",
    "<img width=\"935\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/57dbec9a-de4a-4bed-b4a0-491639298f65\">\n",
    "\n",
    "1. **Normalization:** The [normalization step](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.normalizers) involves some general cleanup, such as removing needless whitespace, lowercasing, and/or removing accents. If you’re familiar with Unicode normalization (such as NFC or NFKC), this is also something the tokenizer may apply. `\"Héllò hôw are yoü?\"` Given the input above, the normalization step would transform it into: `\"hello, how are you?\"`. Normalization is, in a nutshell, a set of operations you apply to a raw string to make it less random or “cleaner”. Common operations include stripping whitespace, removing accented characters or lowercasing all text. If you’re familiar with Unicode normalization, it is also a very common normalization operation applied in most tokenizers. Each normalization operation is represented in the 🤗 Tokenizers library by a `Normalizer`, and you can combine several of those by using a `normalizers.Sequence.` Here is a normalizer applying NFD Unicode normalization and removing accents as an example:\n",
    "\n",
    "    ```python\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, StripAccents\n",
    "normalizer = normalizers.Sequence([NFD(), StripAccents()])\n",
    "# You can manually test that normalizer by applying it to any string:\n",
    "normalizer.normalize_str(\"Héllò hôw are ü?\")\n",
    "``` \n",
    "    When building a Tokenizer, you can customize its normalizer by just changing the corresponding attribute: `tokenizer.normalizer = normalizer`. Of course, if you change the way a tokenizer applies normalization, you should probably retrain it from scratch afterward.\n",
    "\n",
    "1. **Pre-tokenization:** A tokenizer cannot be trained on raw text alone. Instead, we first need to split the texts into small entities, like words. That’s where the pre-tokenization step comes in. A word-based tokenizer can simply split a raw text into words on whitespace and punctuation. Those words will be the boundaries of the subtokens the tokenizer can learn during its training. `\"hello, how are you?\"`. Given this string, the pre-tokenizer’s output will be something like: `[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]`. As we can see, the tokenizer also keeps track of the offsets. Also, the rules for [pre-tokenization](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.pre_tokenizers) can vary with the tokenizer being used. For instance, BERT will have different set of rules for this step than GPT-2. Pre-tokenization is the act of splitting a text into smaller objects that give an upper bound to what your tokens will be at the end of training. A good way to think of this is that the pre-tokenizer will split your text into “words” and then, your final tokens will be parts of those words. An easy way to pre-tokenize inputs is to split on spaces and punctuations, which is done by the `pre_tokenizers.Whitespace pre-tokenizer`. Of course, if you change the way the pre-tokenizer, you should probably retrain your tokenizer from scratch afterward. The output is a list of tuples, with each tuple containing one word and its span in the original sentence (which is used to determine the final offsets of our Encoding). Note that splitting on punctuation will split contractions like \"I'm\" in this example. You can combine together any PreTokenizer together. For instance, here is a pre-tokenizer that will split on space, punctuation and digits, separating numbers in their individual digits:\n",
    "`pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=True)])\n",
    "`\n",
    "\n",
    "1. **Modeling:** After normalization and pre-processing steps, we apply [a training algorithm](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.models) to the text data. This output of this step is dependent on the type of training strategy we are going to use. The state-of-the-art models use subword tokenization algorithms, for example BERT uses WordPiece tokenization, GPT, GPT-2 use BPE, AIBERT uses unigram etc. Using a BERT tokenizer, will tokenize the sentence like this: `[\"hello\"; \",\"; \"how\"; \"are\"; \"you\"; \"?\"]`. Once the input texts are normalized and pre-tokenized, the Tokenizer applies the model on the pre-tokens. ***This is the part of the pipeline that needs training on your corpus (or that has been trained if you are using a pretrained tokenizer).*** ***The role of the model is to split your “words” into tokens, using the rules it has learned.*** It’s also responsible for mapping those tokens to their corresponding IDs in the vocabulary of the model. This model is passed along when intializing the Tokenizer so you already know how to customize this part. Currently, the 🤗 Tokenizers library supports:\n",
    "    - models.BPE\n",
    "    - models.Unigram\n",
    "    - models.WordLevel\n",
    "    - models.WordPiece\n",
    "\n",
    "1. **Post-processing:** Similar to the modeling part, a number of post-processors are available depending on the training strategy used. They’re responsible for adding the special tokens to the input sequence as needed by the model. Using a BERT post-processor to our sequence will result in: `[\"CLS\"; \"hello\"; \",\"; \"how\"; \"are\"; \"you\"; \"?\"; \"SEP\"]`. Here, `[CLS]` denotes the classification token, which tells the model that this is a classification task and `[SEP]` denotes the end of sentence and is also used between two sentences. Post-processing is the last step of the tokenization pipeline, to perform any additional transformation to the Encoding before it’s returned, like adding potential special tokens.\n",
    "\n",
    "Subword tokenization methods, such as Byte Pair Encoding (BPE), WordPiece, or SentencePiece, need to be trained on a specific corpus to learn an efficient and effective way of breaking down words into smaller units (subwords). The training process allows the tokenizer to adapt to the particularities of the text it will be processing.\n",
    "\n",
    "**The Training Process:**\n",
    "\n",
    "During training, a subword tokenizer typically starts with a large corpus of text and performs the following:\n",
    "\n",
    "1. **Initial Vocabulary Creation:** It creates an initial vocabulary, often at the character level or using a simple character or word frequency threshold.\n",
    "\n",
    "1. **Merging Rules Learning:** It iteratively finds the most frequent pairs of characters or subwords and merges them to form a new, longer subword. This process repeats until a set number of merges is reached or the desired vocabulary size is achieved.\n",
    "\n",
    "1. **Final Vocabulary Compilation:** The final vocabulary consists of the original characters plus all the merged subwords, ensuring that any word can be tokenized using this set.\n",
    "\n",
    "In essence, the training of a subword tokenizer is about learning the most efficient and effective way to break down and represent the text it will encounter, taking into account frequency, morphology, and the specific needs of the task or language. This process results in a tokenizer that can handle a wide variety of text inputs, generalize well to new text, and efficiently interface with downstream language models or other NLP tools.\n",
    "\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Build a tokenizer from scratch</span>\n",
    "\n",
    "To illustrate how fast the 🤗 Tokenizers library is, let’s train a new tokenizer on wikitext-103 (516M of text) in just a few seconds. In this tour, we will build and train a Byte-Pair Encoding (BPE) tokenizer. Here, training the tokenizer means it will learn merge rules by:\n",
    "\n",
    "1. Start with all the characters present in the training corpus as tokens.\n",
    "1. Identify the most common pair of tokens and merge it into one token.\n",
    "1. Repeat until the vocabulary (e.g., the number of tokens) has reached the size we want.\n",
    "\n",
    "The main API of the library is the class `Tokenizer`, here is how we instantiate one with a BPE model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d5dcf766",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:20:40.427251Z",
     "iopub.status.busy": "2024-03-01T13:20:40.426386Z",
     "iopub.status.idle": "2024-03-01T13:20:40.431885Z",
     "shell.execute_reply": "2024-03-01T13:20:40.431024Z"
    },
    "papermill": {
     "duration": 0.059112,
     "end_time": "2024-03-01T13:20:40.433800",
     "exception": false,
     "start_time": "2024-03-01T13:20:40.374688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, normalizers\n",
    "from tokenizers.normalizers import NFD, StripAccents, NFC, Lowercase\n",
    "from tokenizers.pre_tokenizers import Whitespace, ByteLevel\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "LOWERCASE = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6e0b89e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:20:40.538174Z",
     "iopub.status.busy": "2024-03-01T13:20:40.537821Z",
     "iopub.status.idle": "2024-03-01T13:20:40.542536Z",
     "shell.execute_reply": "2024-03-01T13:20:40.541579Z"
    },
    "papermill": {
     "duration": 0.060104,
     "end_time": "2024-03-01T13:20:40.544389",
     "exception": false,
     "start_time": "2024-03-01T13:20:40.484285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The main API of the library is the class Tokenizer, here is how we instantiate one with a BPE model:\n",
    "# Creating Byte-Pair Encoding tokenizer\n",
    "# we instantiate a new Tokenizer with this model - BPE\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f35b4b55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:20:40.648048Z",
     "iopub.status.busy": "2024-03-01T13:20:40.647721Z",
     "iopub.status.idle": "2024-03-01T13:20:40.655396Z",
     "shell.execute_reply": "2024-03-01T13:20:40.654420Z"
    },
    "papermill": {
     "duration": 0.061863,
     "end_time": "2024-03-01T13:20:40.657468",
     "exception": false,
     "start_time": "2024-03-01T13:20:40.595605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello how are u?\n",
      "Héllò hôw are ü?\n"
     ]
    }
   ],
   "source": [
    "normalizer = normalizers.Sequence([NFD(), StripAccents()])\n",
    "# You can manually test that normalizer by applying it to any string:\n",
    "print(normalizer.normalize_str(\"Héllò hôw are ü?\"))\n",
    "\n",
    "normalizer = normalizers.Sequence([NFC()] + [normalizers.Lowercase()] if LOWERCASE else [])\n",
    "print(normalizer.normalize_str(\"Héllò hôw are ü?\"))\n",
    "tokenizer.normalizer = normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bbd77b46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:20:40.762439Z",
     "iopub.status.busy": "2024-03-01T13:20:40.762119Z",
     "iopub.status.idle": "2024-03-01T13:20:40.773396Z",
     "shell.execute_reply": "2024-03-01T13:20:40.772526Z"
    },
    "papermill": {
     "duration": 0.065691,
     "end_time": "2024-03-01T13:20:40.775280",
     "exception": false,
     "start_time": "2024-03-01T13:20:40.709589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', (0, 5)), ('!', (5, 6)), ('How', (7, 10)), ('are', (11, 14)), ('you', (15, 18)), ('?', (18, 19)), ('I', (20, 21)), (\"'\", (21, 22)), ('m', (22, 23)), ('fine', (24, 28)), (',', (28, 29)), ('thank', (30, 35)), ('you', (36, 39)), ('.', (39, 40))]\n",
      "[('ĠHello', (0, 5)), ('!', (5, 6)), ('ĠHow', (6, 10)), ('Ġare', (10, 14)), ('Ġyou', (14, 18)), ('?', (18, 19)), ('ĠI', (19, 21)), (\"'m\", (21, 23)), ('Ġfine', (23, 28)), (',', (28, 29)), ('Ġthank', (29, 35)), ('Ġyou', (35, 39)), ('.', (39, 40))]\n"
     ]
    }
   ],
   "source": [
    "pre_tokenizer = Whitespace()\n",
    "print(pre_tokenizer.pre_tokenize_str(\"Hello! How are you? I'm fine, thank you.\"))\n",
    "pre_tokenizer = ByteLevel()\n",
    "print(pre_tokenizer.pre_tokenize_str(\"Hello! How are you? I'm fine, thank you.\"))\n",
    "\n",
    "# We could train our tokenizer right now, but it wouldn’t be optimal.\n",
    "# Without a pre-tokenizer that will split our inputs into words, we might get tokens that overlap several words:\n",
    "# for instance we could get an \"it is\" token since those two words often appear next to each other. \n",
    "# Using a pre-tokenizer will ensure no token is bigger than a word returned by the pre-tokenizer.\n",
    "# Here we want to train a subword BPE tokenizer, and we will use the easiest pre-tokenizer possible by splitting on whitespace.\n",
    "# As we saw in the quicktour, you can customize the pre-tokenizer of a Tokenizer by just changing the corresponding attribute:\n",
    "tokenizer.pre_tokenizer = pre_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "22becd2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T13:20:40.879378Z",
     "iopub.status.busy": "2024-03-01T13:20:40.878749Z",
     "iopub.status.idle": "2024-03-01T13:20:40.992917Z",
     "shell.execute_reply": "2024-03-01T13:20:40.991927Z"
    },
    "papermill": {
     "duration": 0.168528,
     "end_time": "2024-03-01T13:20:40.994949",
     "exception": false,
     "start_time": "2024-03-01T13:20:40.826421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# To train our tokenizer on the wikitext files, we will need to instantiate a [trainer]{.title-ref}, in this case a BpeTrainer\n",
    "# We can set the training arguments like vocab_size or min_frequency \n",
    "# but the most important part is to give the special_tokens we plan to use later on (they are not used at all during training) so that they get inserted in the vocabulary.\n",
    "\n",
    "# Adding special tokens and creating trainer instance\n",
    "# The order in which you write the special tokens list matters: here \"[UNK]\" will get the ID 0, \"[PAD]\" will get the ID 1 and so forth.\n",
    "\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "VOCAB_SIZE = 30522\n",
    "trainer = BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)\n",
    "\n",
    "# Now, we can just call the Tokenizer.train method with any list of files we want to use:\n",
    "\n",
    "#files = [f\"data/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "#tokenizer.train(files, trainer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b66f7f8",
   "metadata": {
    "papermill": {
     "duration": 0.052913,
     "end_time": "2024-03-01T13:20:41.100625",
     "exception": false,
     "start_time": "2024-03-01T13:20:41.047712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#7b6b59;overflow:hidden\">References</div>\n",
    "\n",
    "\n",
    "1. [Text Tokenization and Vectorization in NLP](https://medium.com/@WojtekFulmyk/text-tokenization-and-vectorization-in-nlp-ac5e3eb35b85)\n",
    "1. [Developing LLMs for Generative AI Tokenization and Vectorization\n",
    "](https://www.linkedin.com/pulse/developing-llms-generative-ai-tokenization-darko-medin/)\n",
    "1. [Google Machine Learning Guide](https://developers.google.com/machine-learning/guides/text-classification/step-3#:~:text=Tokenization%3A%20Divide%20the%20texts%20into,measure%20to%20characterize%20these%20texts.)\n",
    "1. [Hugging Face: Understanding tokenizers\n",
    "](https://medium.com/@awaldeep/hugging-face-understanding-tokenizers-1b7e4afdb154)\n",
    "1. [How to use [HuggingFace’s] Transformers Pre-Trained tokenizers? - To READ](https://nlpiation.medium.com/how-to-use-huggingfaces-transformers-pre-trained-tokenizers-e029e8d6d1fa)\n",
    "1. [Byte-Pair Encoding (BPE) in NLP](https://www.geeksforgeeks.org/byte-pair-encoding-bpe-in-nlp/)\n",
    "1. https://neptune.ai/blog/vectorization-techniques-in-nlp-guide\n",
    "1. [Summary of the tokenizers](https://huggingface.co/docs/transformers/tokenizer_summary)\n",
    "1. [Build a tokenizer from scratch](https://huggingface.co/docs/tokenizers/quicktour)\n",
    "1. https://huggingface.co/blog/how-to-train\n",
    "1. [HuggingFace Tokenizers](https://huggingface.co/docs/tokenizers/index)\n",
    "1. [Adding Custom Layers on Top of a Hugging Face Model](https://towardsdatascience.com/adding-custom-layers-on-top-of-a-hugging-face-model-f1ccdfc257bd)\n",
    "1. [Add dense layer on top of Huggingface BERT model](https://stackoverflow.com/questions/64156202/add-dense-layer-on-top-of-huggingface-bert-model)\n",
    "1. [FINE-TUNING PRE-TRAINED MODELS FOR GENERATIVE AI APPLICATIONS](https://www.leewayhertz.com/fine-tuning-pre-trained-models/)\n",
    "1. [Fine-Tuning the Model: What, Why, and How\n",
    "](https://medium.com/@amanatulla1606/fine-tuning-the-model-what-why-and-how-e7fa52bc8ddf)\n",
    "1. https://rumn.medium.com/part-1-ultimate-guide-to-fine-tuning-in-pytorch-pre-trained-model-and-its-configuration-8990194b71e\n",
    "1. https://towardsdatascience.com/fine-tuning-pretrained-nlp-models-with-huggingfaces-trainer-6326a4456e7b\n",
    "1. https://medium.com/@alexmriggio/bert-for-sequence-classification-from-scratch-code-and-theory-fb88053800fa\n",
    "1. https://mccormickml.com/2019/07/22/BERT-fine-tuning/#4-train-our-classification-model\n",
    "1. https://huggingface.co/transformers/v2.2.0/model_doc/bert.html\n",
    "1. https://towardsdatascience.com/fine-tuning-pretrained-nlp-models-with-huggingfaces-trainer-6326a4456e7b\n",
    "1. [Fine-tune a pretrained model](https://huggingface.co/docs/transformers/training#train-with-pytorch-trainer)\n",
    "1. [What’s in the Dataset object](https://huggingface.co/docs/datasets/v1.2.1/exploring.html)\n",
    "1. [Loading a Dataset](https://huggingface.co/docs/datasets/v1.1.1/loading_datasets.html)\n",
    "1. [The Dataset object](https://huggingface.co/docs/datasets/v2.2.1/en/access)\n",
    "1. [Create a dataset](https://huggingface.co/docs/datasets/create_dataset)\n",
    "1. [BertForSequenceClassification source code](https://huggingface.co/transformers/v3.0.2/_modules/transformers/modeling_bert.html#BertForSequenceClassification)\n",
    "1. [7 Text Classification Techniques for Any Scenario](https://blog.dataiku.com/7-text-classification-techniques-for-any-scenario#:~:text=A%20simple%20approach%20for%20text,regression%20or%20tree%2Dbased%20models.)\n",
    "1. [TF-IDF Simplified](https://towardsdatascience.com/tf-idf-simplified-aba19d5f5530)\n",
    "1. [Understanding TF-IDF for Machine Learning](https://www.capitalone.com/tech/machine-learning/understanding-tf-idf/)\n",
    "1. [Understanding TF-IDF in NLP: A Comprehensive Guide\n",
    "](https://medium.com/@er.iit.pradeep09/understanding-tf-idf-in-nlp-a-comprehensive-guide-26707db0cec5)\n",
    "1. [TF-IDF Guide: Using scikit-learn for TF-IDF implementation](https://www.capitalone.com/tech/machine-learning/scikit-tfidf-implementation/)\n",
    "1. [Creating BERT Embeddings with Hugging Face Transformers](https://www.analyticsvidhya.com/blog/2023/08/bert-embeddings/)\n",
    "1. [How to use embeddings for feature extraction?](https://medium.com/mlearning-ai/how-to-use-embeddings-for-feature-extraction-4956db52b5f5)\n",
    "1. [Feedback Prize - English Language Learning](https://www.kaggle.com/competitions/feedback-prize-english-language-learning/code?competitionId=38321&sortBy=voteCount)\n",
    "1. [Tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer)\n",
    "1. [Summary of the tokenizers](https://huggingface.co/docs/transformers/tokenizer_summary)\n",
    "1. [The tokenization pipeline](https://huggingface.co/docs/tokenizers/pipeline)\n",
    "1. [Preprocess](https://huggingface.co/docs/transformers/preprocessing)\n",
    "1. [How to use BERT from the Hugging Face transformer library\n",
    "](https://towardsdatascience.com/how-to-use-bert-from-the-hugging-face-transformer-library-d373a22b0209)\n",
    "1. [Neural Networks: Pooling Layers](https://www.baeldung.com/cs/neural-networks-pooling-layers)\n",
    "1. [Understanding Pooling in Transformer Architecture, Aggregating Outputs for Downstream Tasks](https://www.datasciencebyexample.com/2023/04/30/what-is-pooling-in-transformer-model/)\n",
    "1. https://huggingface.co/docs/transformers/main_classes/output\n",
    "1. https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/output#transformers.utils.ModelOutput\n",
    "1. [Deep learning basics — weight decay](https://medium.com/analytics-vidhya/deep-learning-basics-weight-decay-3c68eb4344e9)\n",
    "1. [How do you compare weight decay with other regularization methods for neural networks?\n",
    "](https://www.linkedin.com/advice/3/how-do-you-compare-weight-decay-other#:~:text=Weight%20decay%20is%20a%20form,them%20from%20growing%20too%20large.)\n",
    "1. [Zero-Weight Decay on BatchNorm and Bias\n",
    "](https://deci.ai/deep-learning-glossary/zero-weight-decay-on-batchnorm-and-bias/)\n",
    "1. [Various Optimization Algorithms For Training Neural Network\n",
    "](https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6)\n",
    "1. [Optimizers in Deep Learning\n",
    "](https://medium.com/mlearning-ai/optimizers-in-deep-learning-7bf81fed78a0)\n",
    "1. [DATASETS & DATALOADERS\n",
    "](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "1. [An Introduction to Datasets and DataLoader in PyTorch\n",
    "](https://wandb.ai/sauravmaheshkar/Dataset-DataLoader/reports/An-Introduction-to-Datasets-and-DataLoader-in-PyTorch--VmlldzoxMDI5MTY2)\n",
    "1. [PyTorch DataLoader: Features, Benefits, and How to Use it\n",
    "](https://saturncloud.io/blog/pytorch-dataloader-features-benefits-and-how-to-use-it/#:~:text=The%20basic%20architecture%20of%20PyTorch%20DataLoader&text=The%20DataLoader%20class%20takes%20in,of%20data%20loading%20and%20preprocessing.)\n",
    "1. [A detailed example of how to generate your data in parallel with PyTorch\n",
    "](https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel)\n",
    "1. [PyTorch DataLoader: A Complete Guide](https://datagy.io/pytorch-dataloader/)\n",
    "1. [How does DataLoader work in PyTorch?](https://medium.com/noumena/how-does-dataloader-work-in-pytorch-8c363a8ee6c1)\n",
    "1. [How to use Datasets and DataLoader in PyTorch for custom text data\n",
    "](https://towardsdatascience.com/how-to-use-datasets-and-dataloader-in-pytorch-for-custom-text-data-270eed7f7c00)\n",
    "1. [Playing with PyTorch and Datasets\n",
    "](https://fede-bianchi.medium.com/playing-with-pytorch-and-datasets-fe64f5590f2)\n",
    "1. [Effective Data Handling with Custom PyTorch Dataset Classes\n",
    "](https://dantokeefe.medium.com/effective-data-handling-with-custom-pytorch-dataset-classes-b141bcb87b41)\n",
    "1. [TRAINING WITH PYTORCH\n",
    "](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html#:~:text=The%20Dataset%20and%20DataLoader%20classes,processing%20single%20instances%20of%20data.)\n",
    "1. [Training a PyTorch Model with DataLoader and Dataset\n",
    "](https://machinelearningmastery.com/training-a-pytorch-model-with-dataloader-and-dataset/)\n",
    "1. [Cross-Validation in Machine Learning\n",
    "](https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f)\n",
    "1. [Understanding 8 types of Cross-Validation\n",
    "](https://towardsdatascience.com/understanding-8-types-of-cross-validation-80c935a4976d)\n",
    "1. [7 Types of Cross-Validation (CV) Techniques You Should Know as a Data Scientist in 2023](https://rukshanpramoditha.medium.com/7-types-of-cross-validation-cv-techniques-you-should-know-as-a-data-scientist-in-2023-516bd17b9189)\n",
    "1. [Cross-Validation in Machine Learning: How to Do It Right\n",
    "](https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right)\n",
    "1. [Automatic Mixed Precision for Deep Learning](https://developer.nvidia.com/automatic-mixed-precision)\n",
    "1. [Introduction to PyTorch: from training loop to prediction\n",
    "](https://towardsdatascience.com/introduction-to-pytorch-from-training-loop-to-prediction-a70372764432)\n",
    "1. [Writing a training loop from scratch in PyTorch\n",
    "](https://keras.io/guides/writing_a_custom_training_loop_in_torch/)\n",
    "1. [Writing a Custom Training loop](https://www.scaler.com/topics/pytorch/writing-a-custom-training-loop-with-pytorch/)\n",
    "1. [OPTIMIZING MODEL PARAMETERS](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html)\n",
    "1. [TRAINING WITH PYTORCH](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html)\n",
    "1. [CUDA AUTOMATIC MIXED PRECISION EXAMPLES](https://pytorch.org/docs/stable/notes/amp_examples.html)\n",
    "1. [Pytorch Training Tricks and Tips](https://towardsdatascience.com/pytorch-training-tricks-and-tips-a8808ebf746c)\n",
    "1. [A (Very Short) Visual Introduction to Learning Rate Schedulers (With Code)\n",
    "](https://medium.com/@theom/a-very-short-visual-introduction-to-learning-rate-schedulers-with-code-189eddffdb00)\n",
    "1. [Using Learning Rate Schedule in PyTorch Training\n",
    "](https://machinelearningmastery.com/using-learning-rate-schedule-in-pytorch-training/)\n",
    "1. [How to Choose a Learning Rate Scheduler for Neural Networks](https://neptune.ai/blog/how-to-choose-a-learning-rate-scheduler#:~:text=A%20Learning%20rate%20schedule%20is,iterations%20as%20the%20training%20progresses.)\n",
    "1. [A Visual Guide to Learning Rate Schedulers in PyTorch\n",
    "](https://towardsdatascience.com/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863)\n",
    "1. [This thing called Weight Decay\n",
    "](https://towardsdatascience.com/this-thing-called-weight-decay-a7cd4bcfccab)\n",
    "1. [7 Text Classification Techniques for Any Scenario\n",
    "](https://blog.dataiku.com/7-text-classification-techniques-for-any-scenario)\n",
    "1. [Accelerating Large Language Models with Mixed-Precision Techniques\n",
    "](https://lightning.ai/pages/community/tutorial/accelerating-large-language-models-with-mixed-precision-techniques/)\n",
    "1. [Understanding Mixed Precision Training\n",
    "](https://towardsdatascience.com/understanding-mixed-precision-training-4b246679c7c4)\n",
    "1. [AUTOMATIC MIXED PRECISION PACKAGE - TORCH.AMP\n",
    "](https://pytorch.org/docs/stable/amp.html#gradient-scaling)\n",
    "1. [How to use pytorch loss functions](https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-pytorch-loss-functions.md#binary-cross-entropy-loss-on-sigmoid-nnbceloss-example)\n",
    "\n",
    "# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#7b6b59;overflow:hidden\">QA</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aafbc37",
   "metadata": {
    "papermill": {
     "duration": 0.050749,
     "end_time": "2024-03-01T13:20:41.203618",
     "exception": false,
     "start_time": "2024-03-01T13:20:41.152869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7516023,
     "sourceId": 61542,
     "sourceType": "competition"
    },
    {
     "datasetId": 4005256,
     "sourceId": 6977472,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 465.218649,
   "end_time": "2024-03-01T13:20:44.079192",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-01T13:12:58.860543",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "002dc168dbf042b2b0cd6f2eecbe4338": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_544fae6a07a54ead861b96f619bd37fb",
       "placeholder": "​",
       "style": "IPY_MODEL_b6d2012d4fae4c43b4eb0ec584ba337a",
       "value": " 232k/232k [00:00&lt;00:00, 1.93MB/s]"
      }
     },
     "0047ec3700cf47799b265b41dbc09830": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2eb0ac6e5d0a4862a13c91e3efed0d4e",
        "IPY_MODEL_9195e38528464ee9a1db7243d8615d0f",
        "IPY_MODEL_78f4f8191b8e4ad2bb780f273c22b4a7"
       ],
       "layout": "IPY_MODEL_0f75522702074244a09566cfa4788c6a"
      }
     },
     "0396e08de9634191b92f4751b3cf56fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "03e1828fb5854e2f9a07c0c2e8159c47": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c2e58848748b4d148b9396d7f42f4df6",
       "placeholder": "​",
       "style": "IPY_MODEL_becd87a2d4ac4dbeacee2f34a6783c97",
       "value": " 28.0/28.0 [00:00&lt;00:00, 2.19kB/s]"
      }
     },
     "05d3650d3fa64491937de327f934a65b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "081c0cce82c749b78a1c30a26c2621b7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0c361d03ebf94a8cad966dd933cfb5c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_cce0d19332a54670bdff1769dd25aab2",
        "IPY_MODEL_65ce6eae33c143beac2ffb54958b9f3e",
        "IPY_MODEL_30ca68a5d98c47d392ee9555a6ea5b13"
       ],
       "layout": "IPY_MODEL_a4a28b1548ca4caca97f5525a33f4eab"
      }
     },
     "0c5a10bc57f145d2ab981f610807ac31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "0f75522702074244a09566cfa4788c6a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "100b6851c4124ba58e3102d14a3b5f0e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1092bda5ea954bb2871716080e278e80": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "10e2d1534db34a158415ef19f75cd581": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "114174702b944bd38c644747df0af822": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "120cb87e15754aeaa5eec1cf8784e210": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "121ba623149340258c654b862d9bfb50": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "146ff4d91c8e47e79b4455f3cf9057d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2f1e393ae07d4f2e8819f1e7edc30de7",
       "placeholder": "​",
       "style": "IPY_MODEL_100b6851c4124ba58e3102d14a3b5f0e",
       "value": "spm.model: 100%"
      }
     },
     "1633f5bdac6941548b4435b6a7fc3a01": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1766eb7a138f4e61a0c62d3aafa79ead": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2d44c8e6164c45a3838badcff7849352",
       "placeholder": "​",
       "style": "IPY_MODEL_d926e5b45a2c4a3ca179f0c7682696a8",
       "value": " 5/5 [00:04&lt;00:00,  1.17ba/s]"
      }
     },
     "19582326546e4e058794801c0e47a3f0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1a90bb2c1e0e4990b873ea018bae8c9e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_82261749fa4c4053858e28228f5d8c7c",
       "placeholder": "​",
       "style": "IPY_MODEL_f6d6732fb411462aa33cb10479b928f5",
       "value": "model.safetensors: 100%"
      }
     },
     "1c1b19e194d445caaef409dc3cae5492": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "21e29c3cd7c147618078b382a1c727b7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "23f0b712f244466fae4036eef8ac1a27": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "26c3ac1acf664400b0fbc41dcf0612a1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2730443770af446f97f9a7e4c8b7f4e9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "27e4042d93634052813553954e3293c9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "285d01f161c74759b6e1031a92f0abb3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2b8a2cb7267046d1a2c1972075297377": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2d44c8e6164c45a3838badcff7849352": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2eb0ac6e5d0a4862a13c91e3efed0d4e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_428981798f884019ac67b9c903314a88",
       "placeholder": "​",
       "style": "IPY_MODEL_754f525ac17d48dcb9312ba200b68dee",
       "value": "pytorch_model.bin: 100%"
      }
     },
     "2f1e393ae07d4f2e8819f1e7edc30de7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3026256f0b9642398bf23fc4bb64170e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9c0eee5ff0574782a41789377e62cd8b",
       "placeholder": "​",
       "style": "IPY_MODEL_76ca526a7d6b4684a08deea229c24c80",
       "value": "tokenizer.json: 100%"
      }
     },
     "30b36ed72936410b9243e49f67d8acc8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "30ca68a5d98c47d392ee9555a6ea5b13": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_348b549fdf3149fa98728c6ea85c878f",
       "placeholder": "​",
       "style": "IPY_MODEL_6bb01dfc69c6416e900e4af2c3e7ffbb",
       "value": " 579/579 [00:00&lt;00:00, 39.0kB/s]"
      }
     },
     "348b549fdf3149fa98728c6ea85c878f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3a2f10dea84c43ce829f851228e77018": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_983984032034402aa97e9fc3183116cc",
        "IPY_MODEL_dcbe34965b64471f88feb3d1ef562708",
        "IPY_MODEL_a020f2ed8b6c4d938aeebdcb097e9db3"
       ],
       "layout": "IPY_MODEL_2b8a2cb7267046d1a2c1972075297377"
      }
     },
     "3b1a695bcbde4dcfbf05337f46bb6efe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3c4b080b8a6a4fc2988acb34a57e043e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6ef472a80fba46a1b452bd7c428c82a4",
       "placeholder": "​",
       "style": "IPY_MODEL_47ec5f1c070149b482aaf2deb9e2e750",
       "value": " 52.0/52.0 [00:00&lt;00:00, 3.90kB/s]"
      }
     },
     "3d31117b72794f64a01aa12933c5de0f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3fc75d599c8b4ca491ae01c1dd817d20": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3fe592b615f842369971363c3781fa87": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3b1a695bcbde4dcfbf05337f46bb6efe",
       "placeholder": "​",
       "style": "IPY_MODEL_f87311473ad245b69e0f27cd6842572e",
       "value": " 41/41 [00:33&lt;00:00,  1.60ba/s]"
      }
     },
     "4016b39de3894c5cb0652c7ba83935ad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "40b66fcdb2f743af85668f0e55d782e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "428981798f884019ac67b9c903314a88": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4345cf1c0a434306880ccbb43b02b80b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1633f5bdac6941548b4435b6a7fc3a01",
       "placeholder": "​",
       "style": "IPY_MODEL_aad4f467854f4968bf8c5ecdaf421011",
       "value": "vocab.txt: 100%"
      }
     },
     "45232c08d27e45d5a7682afbfe38ad2d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "46a6040557074421a7379a57a13813a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "47ec5f1c070149b482aaf2deb9e2e750": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "482dd2f16de5459c86dc4f795bba6e9f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ce7f300d7a5a425db9615455c3f167ad",
       "max": 28.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1c1b19e194d445caaef409dc3cae5492",
       "value": 28.0
      }
     },
     "4839153fc13e4cbd96aceb3bf95fce09": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "48f8c3eaaefa47418911699cc83ee60f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "49946bb54c234cbcad126a43c8474071": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "49dab54f6a89436b915c3854a69aba9f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4a37220d6de34d24964a7d3c373f3707": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4b7d1fa8ba8f416da77341351dfbeeee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ec67f7882c7f40449f6fbf78884d0218",
       "max": 371146213.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_538f262b28e44922bc42c5b9e175aed4",
       "value": 371146213.0
      }
     },
     "50b48eb76e2a49e5a177575fb46a9b52": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "538f262b28e44922bc42c5b9e175aed4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "544fae6a07a54ead861b96f619bd37fb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "55285107f14a4bee9555a264e8f3dcac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6f1e670e2c104a069751c4cc668e8099",
       "max": 267954768.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_40b66fcdb2f743af85668f0e55d782e8",
       "value": 267954768.0
      }
     },
     "559feeac87a249abb05e4a0a48423476": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5640e14a90184b7687bafb8bc30c8ba2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "57e9fa7a06cf42f7921f91fff5ab101a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ecfff20a7166461e861130b907cd3c73",
       "max": 52.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_121ba623149340258c654b862d9bfb50",
       "value": 52.0
      }
     },
     "5a212c705b63426eba72c99402e22c74": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5a91f978931a48599f957a4541d00a16": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f968856038ae475a90edc4d580c1bc35",
        "IPY_MODEL_d3fe46721ad54ec5ac28100f41a66609",
        "IPY_MODEL_bd43553f7b72412bb9441d64ea2b94ae"
       ],
       "layout": "IPY_MODEL_64458041d0354c6d9be8b87cf255decc"
      }
     },
     "5aff645bfc7546b6bdd58a5eacc617bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5fd387ad0176481a9f95dea2e2063c16": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a1a821330f414622ab66c9e2763fa9c9",
        "IPY_MODEL_af60d39ddc0d42329e42a48395a5e430",
        "IPY_MODEL_3fe592b615f842369971363c3781fa87"
       ],
       "layout": "IPY_MODEL_7d43278e1aa443e0930f9b5ef2105683"
      }
     },
     "64458041d0354c6d9be8b87cf255decc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "65ce6eae33c143beac2ffb54958b9f3e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_72fd8f74672444989c418aad070ee61a",
       "max": 579.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_50b48eb76e2a49e5a177575fb46a9b52",
       "value": 579.0
      }
     },
     "661b1af133224109b96be26d25cc7945": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "67350f5638784207917a7f1d1a67ea11": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6bb01dfc69c6416e900e4af2c3e7ffbb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6ef472a80fba46a1b452bd7c428c82a4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6f1e670e2c104a069751c4cc668e8099": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "72fd8f74672444989c418aad070ee61a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "74aceb496f4c47519729df548a880873": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "74b83952124142bdafac40518681633d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_48f8c3eaaefa47418911699cc83ee60f",
       "max": 5.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_99b1bf2506ab469391dbfd36284f2938",
       "value": 5.0
      }
     },
     "754f525ac17d48dcb9312ba200b68dee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "76ca526a7d6b4684a08deea229c24c80": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "77c279e4de894c01852aa79fe7d5fedb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "78f4f8191b8e4ad2bb780f273c22b4a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5640e14a90184b7687bafb8bc30c8ba2",
       "placeholder": "​",
       "style": "IPY_MODEL_dde8f6f3467c48fdb1e820eff1e4d521",
       "value": " 241M/241M [00:00&lt;00:00, 330MB/s]"
      }
     },
     "7b1a9ac5a90144739e2d4d4959f4bce5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7d166173869945bf9f321f51664a8990": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7d43278e1aa443e0930f9b5ef2105683": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7ec332c3b3874ababf3bded239be4e11": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_49dab54f6a89436b915c3854a69aba9f",
       "placeholder": "​",
       "style": "IPY_MODEL_46a6040557074421a7379a57a13813a2",
       "value": " 466k/466k [00:00&lt;00:00, 18.6MB/s]"
      }
     },
     "8186befe249b412b96928a63a9f871c3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "82261749fa4c4053858e28228f5d8c7c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "829861587e264c9ea5abfc65475ef089": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1092bda5ea954bb2871716080e278e80",
       "max": 578.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_285d01f161c74759b6e1031a92f0abb3",
       "value": 578.0
      }
     },
     "82ef6291777a4a459a4acda3054d9fcb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "861184be6eb84c4a856c25cdb1eff21c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2730443770af446f97f9a7e4c8b7f4e9",
       "placeholder": "​",
       "style": "IPY_MODEL_9a3982fd67ec4c96864efb46836c70f6",
       "value": " 2.46M/2.46M [00:00&lt;00:00, 138MB/s]"
      }
     },
     "8c1af9d100fc4d7eaac6f49fb77da683": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_559feeac87a249abb05e4a0a48423476",
       "placeholder": "​",
       "style": "IPY_MODEL_d16deaf74d244bc99a04644b53fc75fe",
       "value": " 371M/371M [00:01&lt;00:00, 345MB/s]"
      }
     },
     "8f73f74520af45e3a1f1e85eaed7e349": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "90b6446f81814b03b4ea559a9d725f0d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c1fb821b726044e19f256b5bc7ff4fad",
       "placeholder": "​",
       "style": "IPY_MODEL_f24c9d6fb8044e41b02963fee254301f",
       "value": " 268M/268M [00:01&lt;00:00, 328MB/s]"
      }
     },
     "9195e38528464ee9a1db7243d8615d0f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_21e29c3cd7c147618078b382a1c727b7",
       "max": 241453931.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a4116f4a72ab443a9edba3ebd5b77c7f",
       "value": 241453931.0
      }
     },
     "91b1bff403874166b2128978f24c676c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "92d69fb74cac4421a5b11092fa4cb96d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d7627af45c1b4032b05482761f9ce31d",
       "placeholder": "​",
       "style": "IPY_MODEL_23f0b712f244466fae4036eef8ac1a27",
       "value": " 52.0/52.0 [00:00&lt;00:00, 4.21kB/s]"
      }
     },
     "983984032034402aa97e9fc3183116cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_77c279e4de894c01852aa79fe7d5fedb",
       "placeholder": "​",
       "style": "IPY_MODEL_120cb87e15754aeaa5eec1cf8784e210",
       "value": "spm.model: 100%"
      }
     },
     "99b1bf2506ab469391dbfd36284f2938": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "9a3982fd67ec4c96864efb46836c70f6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9b911f6d026646769be548191cb59b34": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9c0eee5ff0574782a41789377e62cd8b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9ea3c4eb9d67451e8c8efff754e5198a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_eca29008868f4368b432be89ffebf28d",
        "IPY_MODEL_df60ca804252402ca6ba48228836e578",
        "IPY_MODEL_92d69fb74cac4421a5b11092fa4cb96d"
       ],
       "layout": "IPY_MODEL_114174702b944bd38c644747df0af822"
      }
     },
     "a020f2ed8b6c4d938aeebdcb097e9db3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_10e2d1534db34a158415ef19f75cd581",
       "placeholder": "​",
       "style": "IPY_MODEL_5aff645bfc7546b6bdd58a5eacc617bb",
       "value": " 2.46M/2.46M [00:00&lt;00:00, 24.5MB/s]"
      }
     },
     "a1a821330f414622ab66c9e2763fa9c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ebbd8cd8be5b4563bb22fb8cbf4e423f",
       "placeholder": "​",
       "style": "IPY_MODEL_0396e08de9634191b92f4751b3cf56fd",
       "value": "100%"
      }
     },
     "a40ed3be320f47eca4fefa96a41b44ac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a48e69038a4d4e749770926c90eb4af8",
       "max": 2464616.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3d31117b72794f64a01aa12933c5de0f",
       "value": 2464616.0
      }
     },
     "a4116f4a72ab443a9edba3ebd5b77c7f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a48e69038a4d4e749770926c90eb4af8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a4a28b1548ca4caca97f5525a33f4eab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aad4f467854f4968bf8c5ecdaf421011": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ad3c7867a3bd4b9abd3446337f58e8f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1a90bb2c1e0e4990b873ea018bae8c9e",
        "IPY_MODEL_55285107f14a4bee9555a264e8f3dcac",
        "IPY_MODEL_90b6446f81814b03b4ea559a9d725f0d"
       ],
       "layout": "IPY_MODEL_b915305a4ab548eb957bc1ef1fe0bbcf"
      }
     },
     "aee8f76b4eb649a2af699e4f8f84663d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "af3dd866eafd4d77a89622dd2b90b5bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_081c0cce82c749b78a1c30a26c2621b7",
       "max": 466062.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3fc75d599c8b4ca491ae01c1dd817d20",
       "value": 466062.0
      }
     },
     "af60d39ddc0d42329e42a48395a5e430": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7b1a9ac5a90144739e2d4d4959f4bce5",
       "max": 41.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_aee8f76b4eb649a2af699e4f8f84663d",
       "value": 41.0
      }
     },
     "afb2a8f071154d72b54afa0effb95b80": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e72c11fbb3ab45d4947efa047c59ed75",
       "placeholder": "​",
       "style": "IPY_MODEL_ed29c70b1ccd429390aa1a80d5a7cc84",
       "value": "100%"
      }
     },
     "b2be137a38a8408dabb9029a56365aaf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_26c3ac1acf664400b0fbc41dcf0612a1",
       "placeholder": "​",
       "style": "IPY_MODEL_d02260bcf5b24c08a2ac6e3fcfde271d",
       "value": " 578/578 [00:00&lt;00:00, 43.7kB/s]"
      }
     },
     "b6d2012d4fae4c43b4eb0ec584ba337a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b75674d6fab845f8a79af1062b5bdae8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e8ab3d08df86449ba9b867bba52647f6",
       "placeholder": "​",
       "style": "IPY_MODEL_67350f5638784207917a7f1d1a67ea11",
       "value": "config.json: 100%"
      }
     },
     "b7f839c26d6f40ebba832dafb9e68a03": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3026256f0b9642398bf23fc4bb64170e",
        "IPY_MODEL_af3dd866eafd4d77a89622dd2b90b5bd",
        "IPY_MODEL_7ec332c3b3874ababf3bded239be4e11"
       ],
       "layout": "IPY_MODEL_4016b39de3894c5cb0652c7ba83935ad"
      }
     },
     "b915305a4ab548eb957bc1ef1fe0bbcf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bd43553f7b72412bb9441d64ea2b94ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_91b1bff403874166b2128978f24c676c",
       "placeholder": "​",
       "style": "IPY_MODEL_f12adfceae6d476a9f050cbdfc5d9849",
       "value": " 483/483 [00:00&lt;00:00, 40.5kB/s]"
      }
     },
     "becd87a2d4ac4dbeacee2f34a6783c97": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "bfee8c6903f541d2aea06372c577cd06": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c1fb821b726044e19f256b5bc7ff4fad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c27c0ec41c294a64aefea10f47e4724f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c2e58848748b4d148b9396d7f42f4df6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ca17a09e44294c1aaccbecfc827374d3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4345cf1c0a434306880ccbb43b02b80b",
        "IPY_MODEL_d100b2accbf64f9fa68d96808208b116",
        "IPY_MODEL_002dc168dbf042b2b0cd6f2eecbe4338"
       ],
       "layout": "IPY_MODEL_c27c0ec41c294a64aefea10f47e4724f"
      }
     },
     "cbcea344114846aeb556c74e2821ed28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "cce0d19332a54670bdff1769dd25aab2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_49946bb54c234cbcad126a43c8474071",
       "placeholder": "​",
       "style": "IPY_MODEL_d1998dcf5fb44d00b9d228f94dc660bb",
       "value": "config.json: 100%"
      }
     },
     "ce16dd793b724c3f9287e65eb719cc08": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ce7f300d7a5a425db9615455c3f167ad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d02260bcf5b24c08a2ac6e3fcfde271d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d100b2accbf64f9fa68d96808208b116": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ce16dd793b724c3f9287e65eb719cc08",
       "max": 231508.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_74aceb496f4c47519729df548a880873",
       "value": 231508.0
      }
     },
     "d16deaf74d244bc99a04644b53fc75fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d1998dcf5fb44d00b9d228f94dc660bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d1ea379d6c8841aa8c80799179edf969": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8f73f74520af45e3a1f1e85eaed7e349",
       "placeholder": "​",
       "style": "IPY_MODEL_30b36ed72936410b9243e49f67d8acc8",
       "value": "tokenizer_config.json: 100%"
      }
     },
     "d3fe46721ad54ec5ac28100f41a66609": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_82ef6291777a4a459a4acda3054d9fcb",
       "max": 483.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e0f2b72f96b0430c861b2521ad617828",
       "value": 483.0
      }
     },
     "d42259d58d8d479d9416e4120ee82306": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b75674d6fab845f8a79af1062b5bdae8",
        "IPY_MODEL_829861587e264c9ea5abfc65475ef089",
        "IPY_MODEL_b2be137a38a8408dabb9029a56365aaf"
       ],
       "layout": "IPY_MODEL_05d3650d3fa64491937de327f934a65b"
      }
     },
     "d7627af45c1b4032b05482761f9ce31d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d926e5b45a2c4a3ca179f0c7682696a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "dcbe34965b64471f88feb3d1ef562708": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_19582326546e4e058794801c0e47a3f0",
       "max": 2464616.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_cbcea344114846aeb556c74e2821ed28",
       "value": 2464616.0
      }
     },
     "dde8f6f3467c48fdb1e820eff1e4d521": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "debf3378d4424f1ca1b9a7c9f3ed8b24": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9b911f6d026646769be548191cb59b34",
       "placeholder": "​",
       "style": "IPY_MODEL_5a212c705b63426eba72c99402e22c74",
       "value": "pytorch_model.bin: 100%"
      }
     },
     "df60ca804252402ca6ba48228836e578": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4839153fc13e4cbd96aceb3bf95fce09",
       "max": 52.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e6c887a8f9144333bd8f6d6c254f8d8a",
       "value": 52.0
      }
     },
     "e0f2b72f96b0430c861b2521ad617828": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e1b9f8df07b74fccbcbb70bb648a782e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_debf3378d4424f1ca1b9a7c9f3ed8b24",
        "IPY_MODEL_4b7d1fa8ba8f416da77341351dfbeeee",
        "IPY_MODEL_8c1af9d100fc4d7eaac6f49fb77da683"
       ],
       "layout": "IPY_MODEL_e8d2089df2234092b3c000bde1cf2fb7"
      }
     },
     "e1bad7ce67ab43a6a5690eb1c2399a79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ee69f5427b2d488ebd2915bd0e3f5964",
        "IPY_MODEL_482dd2f16de5459c86dc4f795bba6e9f",
        "IPY_MODEL_03e1828fb5854e2f9a07c0c2e8159c47"
       ],
       "layout": "IPY_MODEL_27e4042d93634052813553954e3293c9"
      }
     },
     "e6455a400a0147c79025730b7f054d48": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d1ea379d6c8841aa8c80799179edf969",
        "IPY_MODEL_57e9fa7a06cf42f7921f91fff5ab101a",
        "IPY_MODEL_3c4b080b8a6a4fc2988acb34a57e043e"
       ],
       "layout": "IPY_MODEL_7d166173869945bf9f321f51664a8990"
      }
     },
     "e6c887a8f9144333bd8f6d6c254f8d8a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e72c11fbb3ab45d4947efa047c59ed75": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e844d6a0d41b4ceeb47badf5c307ef35": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e8ab3d08df86449ba9b867bba52647f6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e8d2089df2234092b3c000bde1cf2fb7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ebbd8cd8be5b4563bb22fb8cbf4e423f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ec67f7882c7f40449f6fbf78884d0218": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eca29008868f4368b432be89ffebf28d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f501ccd297654407a298d6074b8218e4",
       "placeholder": "​",
       "style": "IPY_MODEL_45232c08d27e45d5a7682afbfe38ad2d",
       "value": "tokenizer_config.json: 100%"
      }
     },
     "ecfff20a7166461e861130b907cd3c73": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ed29c70b1ccd429390aa1a80d5a7cc84": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ee69f5427b2d488ebd2915bd0e3f5964": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e844d6a0d41b4ceeb47badf5c307ef35",
       "placeholder": "​",
       "style": "IPY_MODEL_0c5a10bc57f145d2ab981f610807ac31",
       "value": "tokenizer_config.json: 100%"
      }
     },
     "f12adfceae6d476a9f050cbdfc5d9849": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f24c9d6fb8044e41b02963fee254301f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f30951d3e06f4dec97724765e98c2ef4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_146ff4d91c8e47e79b4455f3cf9057d9",
        "IPY_MODEL_a40ed3be320f47eca4fefa96a41b44ac",
        "IPY_MODEL_861184be6eb84c4a856c25cdb1eff21c"
       ],
       "layout": "IPY_MODEL_8186befe249b412b96928a63a9f871c3"
      }
     },
     "f501ccd297654407a298d6074b8218e4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f6d6732fb411462aa33cb10479b928f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f87311473ad245b69e0f27cd6842572e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f968856038ae475a90edc4d580c1bc35": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4a37220d6de34d24964a7d3c373f3707",
       "placeholder": "​",
       "style": "IPY_MODEL_661b1af133224109b96be26d25cc7945",
       "value": "config.json: 100%"
      }
     },
     "fee26d3c63f04371be2d54e446624229": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_afb2a8f071154d72b54afa0effb95b80",
        "IPY_MODEL_74b83952124142bdafac40518681633d",
        "IPY_MODEL_1766eb7a138f4e61a0c62d3aafa79ead"
       ],
       "layout": "IPY_MODEL_bfee8c6903f541d2aea06372c577cd06"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
